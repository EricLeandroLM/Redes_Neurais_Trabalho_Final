{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center; align-items: center;\">\n",
    "<p align=\"center\">\n",
    "<img src=\"../imagens/Capa_c.jpeg\" style=\"width:250px;height:250px\" alt='imagem do daniel'>\n",
    "</p>\n",
    "\n",
    "<div style=\"margin-left: 20px;\">\n",
    "<h1 style=\"font-family: times new roman;\">DreamCoders</h1>\n",
    "<p style=\"margin-bottom: 5px;\">Trabalho Final de Redes Neurais</p>\n",
    "<p style=\"margin-bottom: 5px;\">INTEGRANTES: Eric Leandro e Samuel Araujo </p>\n",
    "<p style=\"margin-bottom: 5px;\">DISCIPLINA: Redes Neurais e Algoritmos Genéticos</p>\n",
    "<p style=\"margin-bottom: 5px;\">MESTRE: Daniel Cassar</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextualização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Ciência de Materiais é uma área vasta que abrange uma grande quantidade de áreas da ciência, desde físico-química até as biociências. O desenvolvimento da Ciência de Materiais é de suma importância para o descobrimento e aperfeiçoamento de materiais para as mais diversas aplicações nos setores da indústria e, dessa forma, permitndo o avanço de diversas tecnologias que imapactam o dia a dia de toda a humanidade seja diretamente ou indiretamente. Tendo como uma parte muito importante para as pesquisas em materiais o entendimento das características/propriedades intrísecas de cada material, através delas é possível prever e entender como aplicar o material ou aperfeiçoa-lo.\n",
    "\n",
    "Com o advento da ciência de dados, foi possível avanços mais rápidos na descoberta e caracterização dos materiais, tornando-se uma parte fundamental no avanço da ciência de materiais. Dentro da área de dados, existem diversas técnicas de previsão de características de materiais, sendo comum a aplicação de redes neurais para previsão de características específicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As redes neurais são uma técnica de aprendizado de máquina inspirada no funcionamento do cérebro humano. Tendo como ideia fundamental o conceito de camadas de neurônios, onde cada neurônio aplica uma função de ativação após calcular uma combinação linear das entradas com pesos passadas pelos neurônios das camadas anteriores, isso só é possível devido a conexão entre os neurôneos de diferentes camadas.\n",
    "\n",
    "Existem três divisões principais de camadas:\n",
    "- Camada de Entrada -> Recebe os dados originais que serão a base para o treinamento da rede, e transmitem para os neurônios da primeira camada oculta.Cada neurônio corresponde a uma característica de entrada, no nosso caso, a uma feature\n",
    "- Camadas Ocultas -> As camadas ocultas são compostas por neurônios que recebem entradas dos neurôneos da camada anterior, aplicam uma combinação linear, em seguida aplicam a função de ativação e, por fim, tranmitem uma saída para os neurôneos da próxima camada. Cada camada oculta pode ter um número de neurôneos diferentes, mudando assim, os resultados da combinação linear e, por conseguinte, da saída de cada neurônio.\n",
    "- Camada de Saída -> Recebe as saídas dos neurônios da última camada oculta e mostram um resultado. Sendo que cada neurônio da camada de saída corresponde a uma características específica. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O projeto consiste na aplicação e na otimização de hiperparâmetros de uma rede neural do tipo MLP para prever a dureza, na escala de Mohs, de minerais a partir de 11 features inerentes a cada material. A partir dos resultados da predição, analisar a eficácia de redes neurais MLP para predição de dureza de minerais com estruturas cristalinas distintas e explorar as variações para cada tipo de estrutura cristalina, considerando as amostragens utilizadas.\n",
    "Vale ressaltar que não faz parte do escopo do projeto se aprofundar nos aspectos técnicos, sendo necessário, para isso, a leitura das referências bibliográficas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados extraídos consiste em dois databases de minerais, uma para treino com 622 minerais e outro para teste com 51 minerais. Ambos os dataset possuem um total de 11 descrições atômicas de cada mineral, ou seja, 11 features e a dureza como target.\n",
    "*Features*\n",
    "- Número de elétrons\n",
    "- Número de elétrons de valência\n",
    "- Número atômico\n",
    "- Eletronegativa de Pauling\n",
    "- Eletronegatividade do estado de oxidação mais comum\n",
    "- Raios atômicos covalentes\n",
    "- Raio de Vander Waals\n",
    "- Energia de ionização do neutro\n",
    "- Média de todos os elétrons\n",
    "- Densidade média\n",
    "- Peso atômico\n",
    "\n",
    "Entretanto, o dataset de teste possui duas colunas a mais. A primeira corresponde a fórmula molecular do mineral e a segunda ao tipo de estrutura ceistalina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset de Treino\n",
    "O Dataset de treino é composto por 622 minerais com composições únicas obtidos por permutações composicionais de uma base de dados com 369 minerais únicos retirados do *Physical and Optical Properties of Minerals CRC Handbook of Chemestry and Physics* e da *American Mineralogist Crystal Structure Database* . Os minerais presentes do dataset possuem estruturas critalinas diversas como já foi mencionado, sendo: \n",
    "\n",
    "- 210 de estrutura monoclínica;\n",
    "- 96 de estrutura romboédrica;\n",
    "- 89 de estrutura hexagonal;\n",
    "- 80 de estrutura tetragonal;\n",
    "- 73 de estrutura cúbica;\n",
    "- 50 de estrutura ortorrômbica;\n",
    "- 22 de estrutura triclínica;\n",
    "- 1 de estrutura trigonal;\n",
    "- 1 de estrutura amorfa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset de Teste\n",
    "O dataset de teste é composto por 51 cristais sintéticos singulares retirados da literatura, sendo a distribuição das estruturas cristalinas:\n",
    "\n",
    "- 15 de estrutura monoclínica;\n",
    "- 7 de estrutura tetragonal;\n",
    "- 7 de estrutura hexagonal;\n",
    "- 6 de estrutura ortorrômbicas;\n",
    "- 4 de estrutura cúbica;\n",
    "- 3 de estrutura romboédrica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metodologia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para alcançar o objetivo do trabalho de forma plena, foi realizado uma série de etapas: \n",
    "\n",
    "- Primeiro, a manipulação dos dados partindo da transformação dos dados de csv para o formato dataframe de tratamento para os datasets junto ao tratamento dos datasets a partir da exploração dos mesmos, permitindo, assim, filtrar as features necessária e relevantes para previsão da dureza, depois, foi feita a segmentação dos dois datasets em treino e teste junto a normalização de dados. \n",
    "- Após a etapa de manipulação, foi construída uma rede neural MLP do tipo regressora, logo depois, fez-se a otimização dos hiperparâmetros usando SGD do próprio PyTorch, ademais, foi feito o treinamento aprimorando a perda através da aplicação de 100 arquiteturas de rede distintas. \n",
    "- Por fim, aplicou-se a rede neural treinada no dataset de teste e computou-se diversas métricas de erros para análise da precisão da rede neural para os diferentes tipos de estrutura cristalina.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definições"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro é necessário a importação das bibliotecas do python que serão utilizadas.\n",
    "\n",
    "**Manipulação de dados**\n",
    "- Numpy;\n",
    "- Pandas.\n",
    "\n",
    "**Rede Neural**\n",
    "- Pytorch -> Incluindo \"torch.nn\" para criação da rede neural e \"torch.optim\" para optimização da rede.\n",
    "\n",
    "**Métricas e Dimensionamento**\n",
    "- Scikitlearn  -> Sendo \"sklearn.metrics\" para as métricas dos erros das previsões e \"sklearn.preprocessing\", em específico, \"MinMaxScaler\" para redimenssionamento.\n",
    "\n",
    "**Visualização**\n",
    "- Matplotlib -> \"matplotlib.pyplot\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir daqui, será dado início a manipulação dos datasets e o desenvolvimento da rede neural junto com a sua otimização e aplicação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante o desenvolvimento do presente trebalho, fez-se necessário o uso de bibliotecas específicas para cada etapa.\n",
    "\n",
    "**Manipulação e Tratamento dos Datasets**\n",
    "- Numpy\n",
    "- Pandas\n",
    "- sklearn.preprocessing\n",
    "\n",
    "**Rede Neural**\n",
    "- torch.nn\n",
    "- torch.optim\n",
    "- optuna\n",
    "- tqdm\n",
    "\n",
    "**Cálculo das Métricas**\n",
    "- sklearn.metrics\n",
    "\n",
    "**Visualização**\n",
    "- matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''IMPORTS'''\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import optuna\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, há a definição de algumas funções que serão utilizadas ao decorrer do notebook para o cálculo das métricas de erro(RMSE e R2) junto com a função para o plot do gráfico dos resíduos de erros, sendo que todas as funções recebem como parâmetros a lista dos valores verdadeiros do target e a lista dos valores preditos para os targets.\n",
    "\n",
    "- root_mean_squared_error -> Cálculo da raiz do erro quadrático médio (RMSE)\n",
    "- r_squared -> Cálculo do coeficiente de determinação (R<sup>2</sup>)\n",
    "- plot_residuals -> Plota o gráfico dos resíduos dos erros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''FUNÇÕES'''\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula a raiz do erro quadrático médio (RMSE).\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): Os valores verdadeiros.\n",
    "        y_pred (array-like): Os valores previstos.\n",
    "\n",
    "    Returns:\n",
    "        float: A raiz do erro quadrático médio.\n",
    "    \"\"\"\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula o coeficiente de determinação (R²).\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): Os valores verdadeiros.\n",
    "        y_pred (array-like): Os valores previstos.\n",
    "\n",
    "    Returns:\n",
    "        float: O coeficiente de determinação.\n",
    "    \"\"\"\n",
    "    return r2_score(y_true, y_pred)\n",
    "\n",
    "def plot_residuals(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Plota o gráfico de resíduos.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): Os valores verdadeiros.\n",
    "        y_pred (array-like): Os valores previstos.\n",
    "    \"\"\"\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "    plt.title('Gráfico de Resíduos')\n",
    "    plt.xlabel('Valores Previstos')\n",
    "    plt.ylabel('Resíduos')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulação dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, é feita a leitura dos arquivos dos datasets no formato .csv e a transformação para o formato DataFrame do pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Pegando os dados'''\n",
    "\n",
    "# Carregando o arquivo CSV para o teste\n",
    "minerais_treino = pd.read_csv(\"../Datasets/Mineral_Dataset_Supplementary_Info.csv\")\n",
    "\n",
    "# Transformando o dataset em um Dataframe pandas\n",
    "df_treino = pd.DataFrame(minerais_treino) \n",
    "\n",
    "# Carregando o arquivo CSV para o teste\n",
    "minerais_teste = pd.read_csv('../Datasets/Artificial_Crystals_Dataset.csv')\n",
    "\n",
    "# Transformando o dataset em um Dataframe pandas\n",
    "df_teste = pd.DataFrame(minerais_teste) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, a mostragem do dataframe referente ao dataset de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de treino:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>allelectrons_Total</th>\n",
       "      <th>density_Total</th>\n",
       "      <th>allelectrons_Average</th>\n",
       "      <th>val_e_Average</th>\n",
       "      <th>atomicweight_Average</th>\n",
       "      <th>ionenergy_Average</th>\n",
       "      <th>el_neg_chi_Average</th>\n",
       "      <th>R_vdw_element_Average</th>\n",
       "      <th>R_cov_element_Average</th>\n",
       "      <th>zaratio_Average</th>\n",
       "      <th>density_Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>110.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>36.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>82.598467</td>\n",
       "      <td>8.504133</td>\n",
       "      <td>2.146667</td>\n",
       "      <td>2.006667</td>\n",
       "      <td>1.253333</td>\n",
       "      <td>0.456803</td>\n",
       "      <td>7.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>406.0</td>\n",
       "      <td>30.472136</td>\n",
       "      <td>9.902439</td>\n",
       "      <td>4.682927</td>\n",
       "      <td>19.813180</td>\n",
       "      <td>11.456151</td>\n",
       "      <td>2.700244</td>\n",
       "      <td>1.676829</td>\n",
       "      <td>0.868293</td>\n",
       "      <td>0.522909</td>\n",
       "      <td>0.743223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>406.0</td>\n",
       "      <td>30.472464</td>\n",
       "      <td>10.410256</td>\n",
       "      <td>4.923077</td>\n",
       "      <td>20.931371</td>\n",
       "      <td>11.541405</td>\n",
       "      <td>2.753590</td>\n",
       "      <td>1.703846</td>\n",
       "      <td>0.894359</td>\n",
       "      <td>0.497498</td>\n",
       "      <td>0.781345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5.5</td>\n",
       "      <td>476.0</td>\n",
       "      <td>61.142136</td>\n",
       "      <td>11.609756</td>\n",
       "      <td>4.682927</td>\n",
       "      <td>23.659644</td>\n",
       "      <td>11.487395</td>\n",
       "      <td>2.763659</td>\n",
       "      <td>1.714634</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.519474</td>\n",
       "      <td>1.491272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.5</td>\n",
       "      <td>476.0</td>\n",
       "      <td>61.142464</td>\n",
       "      <td>12.205128</td>\n",
       "      <td>4.923077</td>\n",
       "      <td>24.975089</td>\n",
       "      <td>11.574251</td>\n",
       "      <td>2.820256</td>\n",
       "      <td>1.743590</td>\n",
       "      <td>0.873846</td>\n",
       "      <td>0.493887</td>\n",
       "      <td>1.567755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>625</td>\n",
       "      <td>3.8</td>\n",
       "      <td>46.0</td>\n",
       "      <td>9.133000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>48.719500</td>\n",
       "      <td>9.877100</td>\n",
       "      <td>2.115000</td>\n",
       "      <td>1.905000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>0.478880</td>\n",
       "      <td>4.566500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>626</td>\n",
       "      <td>4.5</td>\n",
       "      <td>86.0</td>\n",
       "      <td>6.674328</td>\n",
       "      <td>14.333333</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>30.645954</td>\n",
       "      <td>11.862733</td>\n",
       "      <td>2.861667</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.901667</td>\n",
       "      <td>0.487172</td>\n",
       "      <td>1.112388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>628</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>7.134332</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>40.689515</td>\n",
       "      <td>11.506150</td>\n",
       "      <td>2.545000</td>\n",
       "      <td>1.765000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.479405</td>\n",
       "      <td>3.567166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>629</td>\n",
       "      <td>7.5</td>\n",
       "      <td>86.0</td>\n",
       "      <td>8.841328</td>\n",
       "      <td>14.333333</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>30.550687</td>\n",
       "      <td>11.543000</td>\n",
       "      <td>2.831667</td>\n",
       "      <td>1.735000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.489507</td>\n",
       "      <td>1.473555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>630</td>\n",
       "      <td>6.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>18.204400</td>\n",
       "      <td>10.272727</td>\n",
       "      <td>4.727273</td>\n",
       "      <td>20.652175</td>\n",
       "      <td>11.148755</td>\n",
       "      <td>2.702273</td>\n",
       "      <td>1.695455</td>\n",
       "      <td>0.875455</td>\n",
       "      <td>0.519605</td>\n",
       "      <td>0.827473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>622 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Hardness  allelectrons_Total  density_Total  \\\n",
       "0             0       2.3               110.0      23.000000   \n",
       "1             1       5.5               406.0      30.472136   \n",
       "2             2       5.5               406.0      30.472464   \n",
       "3             3       5.5               476.0      61.142136   \n",
       "4             4       5.5               476.0      61.142464   \n",
       "..          ...       ...                 ...            ...   \n",
       "617         625       3.8                46.0       9.133000   \n",
       "618         626       4.5                86.0       6.674328   \n",
       "619         628       4.0                38.0       7.134332   \n",
       "620         629       7.5                86.0       8.841328   \n",
       "621         630       6.0               226.0      18.204400   \n",
       "\n",
       "     allelectrons_Average  val_e_Average  atomicweight_Average  \\\n",
       "0               36.666667       2.666667             82.598467   \n",
       "1                9.902439       4.682927             19.813180   \n",
       "2               10.410256       4.923077             20.931371   \n",
       "3               11.609756       4.682927             23.659644   \n",
       "4               12.205128       4.923077             24.975089   \n",
       "..                    ...            ...                   ...   \n",
       "617             23.000000       4.000000             48.719500   \n",
       "618             14.333333       5.166667             30.645954   \n",
       "619             19.000000       4.000000             40.689515   \n",
       "620             14.333333       5.000000             30.550687   \n",
       "621             10.272727       4.727273             20.652175   \n",
       "\n",
       "     ionenergy_Average  el_neg_chi_Average  R_vdw_element_Average  \\\n",
       "0             8.504133            2.146667               2.006667   \n",
       "1            11.456151            2.700244               1.676829   \n",
       "2            11.541405            2.753590               1.703846   \n",
       "3            11.487395            2.763659               1.714634   \n",
       "4            11.574251            2.820256               1.743590   \n",
       "..                 ...                 ...                    ...   \n",
       "617           9.877100            2.115000               1.905000   \n",
       "618          11.862733            2.861667               1.700000   \n",
       "619          11.506150            2.545000               1.765000   \n",
       "620          11.543000            2.831667               1.735000   \n",
       "621          11.148755            2.702273               1.695455   \n",
       "\n",
       "     R_cov_element_Average  zaratio_Average  density_Average  \n",
       "0                 1.253333         0.456803         7.666667  \n",
       "1                 0.868293         0.522909         0.743223  \n",
       "2                 0.894359         0.497498         0.781345  \n",
       "3                 0.848780         0.519474         1.491272  \n",
       "4                 0.873846         0.493887         1.567755  \n",
       "..                     ...              ...              ...  \n",
       "617               1.120000         0.478880         4.566500  \n",
       "618               0.901667         0.487172         1.112388  \n",
       "619               0.920000         0.479405         3.567166  \n",
       "620               0.890000         0.489507         1.473555  \n",
       "621               0.875455         0.519605         0.827473  \n",
       "\n",
       "[622 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mostrando o Dataframe\n",
    "print(\"Dataset de treino:\")\n",
    "display(df_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, o Dataframe do dataset de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de teste:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Formula</th>\n",
       "      <th>Crystal structure</th>\n",
       "      <th>Hardness (Mohs)</th>\n",
       "      <th>allelectrons_Total</th>\n",
       "      <th>density_Total</th>\n",
       "      <th>allelectrons_Average</th>\n",
       "      <th>val_e_Average</th>\n",
       "      <th>atomicweight_Average</th>\n",
       "      <th>ionenergy_Average</th>\n",
       "      <th>el_neg_chi_Average</th>\n",
       "      <th>R_vdw_element_Average</th>\n",
       "      <th>R_cov_element_Average</th>\n",
       "      <th>zaratio_Average</th>\n",
       "      <th>density_Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MnTeMoO6</td>\n",
       "      <td>tetragonal</td>\n",
       "      <td>4.50</td>\n",
       "      <td>167.000</td>\n",
       "      <td>23.907992</td>\n",
       "      <td>18.555556</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>41.609136</td>\n",
       "      <td>11.693844</td>\n",
       "      <td>2.938889</td>\n",
       "      <td>1.711111</td>\n",
       "      <td>0.884444</td>\n",
       "      <td>0.477830</td>\n",
       "      <td>2.656444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>MgH2</td>\n",
       "      <td>tetragonal</td>\n",
       "      <td>4.00</td>\n",
       "      <td>14.000</td>\n",
       "      <td>1.740168</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>8.773227</td>\n",
       "      <td>11.614333</td>\n",
       "      <td>1.903333</td>\n",
       "      <td>1.310000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.825990</td>\n",
       "      <td>0.580056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CO(NH2)2C4H6O5</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>2.50</td>\n",
       "      <td>102.000</td>\n",
       "      <td>8.511159</td>\n",
       "      <td>4.434783</td>\n",
       "      <td>3.304348</td>\n",
       "      <td>8.440584</td>\n",
       "      <td>13.176622</td>\n",
       "      <td>2.672609</td>\n",
       "      <td>1.379130</td>\n",
       "      <td>0.530870</td>\n",
       "      <td>0.713850</td>\n",
       "      <td>0.370050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>GaPO4</td>\n",
       "      <td>trigonal</td>\n",
       "      <td>5.50</td>\n",
       "      <td>78.000</td>\n",
       "      <td>8.109328</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>27.448814</td>\n",
       "      <td>11.826400</td>\n",
       "      <td>2.960000</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.488163</td>\n",
       "      <td>1.351555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Ga3PO7</td>\n",
       "      <td>trigonal</td>\n",
       "      <td>6.50</td>\n",
       "      <td>164.000</td>\n",
       "      <td>19.921324</td>\n",
       "      <td>14.909091</td>\n",
       "      <td>5.090909</td>\n",
       "      <td>32.012361</td>\n",
       "      <td>11.255573</td>\n",
       "      <td>2.881818</td>\n",
       "      <td>1.640909</td>\n",
       "      <td>0.841818</td>\n",
       "      <td>0.483480</td>\n",
       "      <td>1.811029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>LiB3O5</td>\n",
       "      <td>orthorhombic</td>\n",
       "      <td>6.50</td>\n",
       "      <td>58.000</td>\n",
       "      <td>7.650660</td>\n",
       "      <td>6.444444</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>13.261239</td>\n",
       "      <td>10.930689</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>1.686667</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.479962</td>\n",
       "      <td>0.850073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>BiB3O6</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>5.75</td>\n",
       "      <td>146.000</td>\n",
       "      <td>16.864992</td>\n",
       "      <td>14.600000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>33.739258</td>\n",
       "      <td>11.388810</td>\n",
       "      <td>2.866000</td>\n",
       "      <td>1.695000</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.478464</td>\n",
       "      <td>1.686499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>K2Al2B2O7</td>\n",
       "      <td>trigonal</td>\n",
       "      <td>6.00</td>\n",
       "      <td>130.000</td>\n",
       "      <td>11.871324</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.307692</td>\n",
       "      <td>20.443453</td>\n",
       "      <td>10.198131</td>\n",
       "      <td>2.540000</td>\n",
       "      <td>1.820000</td>\n",
       "      <td>0.972308</td>\n",
       "      <td>0.489274</td>\n",
       "      <td>0.913179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Nd0.02GdLa0.16Ca4O(BO3)3</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>6.42</td>\n",
       "      <td>249.320</td>\n",
       "      <td>22.345960</td>\n",
       "      <td>13.713971</td>\n",
       "      <td>4.365237</td>\n",
       "      <td>29.432344</td>\n",
       "      <td>10.598482</td>\n",
       "      <td>2.525787</td>\n",
       "      <td>1.813894</td>\n",
       "      <td>0.992739</td>\n",
       "      <td>0.487604</td>\n",
       "      <td>1.229151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Nd0.02GdLa0.33Ca4O(BO3)3</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>6.35</td>\n",
       "      <td>259.010</td>\n",
       "      <td>23.392140</td>\n",
       "      <td>14.114986</td>\n",
       "      <td>4.343324</td>\n",
       "      <td>30.446537</td>\n",
       "      <td>10.551961</td>\n",
       "      <td>2.512578</td>\n",
       "      <td>1.819602</td>\n",
       "      <td>1.001515</td>\n",
       "      <td>0.486888</td>\n",
       "      <td>1.274776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>YVO4</td>\n",
       "      <td>tetragonal</td>\n",
       "      <td>5.00</td>\n",
       "      <td>94.000</td>\n",
       "      <td>10.584328</td>\n",
       "      <td>15.666667</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>33.973910</td>\n",
       "      <td>11.239317</td>\n",
       "      <td>2.768333</td>\n",
       "      <td>1.745000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.481708</td>\n",
       "      <td>1.764055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>LiNbO3</td>\n",
       "      <td>rhomohedral</td>\n",
       "      <td>5.00</td>\n",
       "      <td>68.000</td>\n",
       "      <td>9.107996</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>29.568292</td>\n",
       "      <td>10.600980</td>\n",
       "      <td>2.580000</td>\n",
       "      <td>1.712000</td>\n",
       "      <td>0.956000</td>\n",
       "      <td>0.474714</td>\n",
       "      <td>1.821599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Na3Ba2(B3O6)2F</td>\n",
       "      <td>hexagonal</td>\n",
       "      <td>4.50</td>\n",
       "      <td>280.000</td>\n",
       "      <td>24.150564</td>\n",
       "      <td>11.666667</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>25.810253</td>\n",
       "      <td>10.686196</td>\n",
       "      <td>2.586250</td>\n",
       "      <td>1.808333</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.479152</td>\n",
       "      <td>1.006274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>NdCa4O(BO3)3</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>5.40</td>\n",
       "      <td>235.000</td>\n",
       "      <td>20.223320</td>\n",
       "      <td>13.055556</td>\n",
       "      <td>4.388889</td>\n",
       "      <td>27.609017</td>\n",
       "      <td>10.614044</td>\n",
       "      <td>2.536667</td>\n",
       "      <td>1.810556</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.488869</td>\n",
       "      <td>1.123518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>LuAl3(BO3)4</td>\n",
       "      <td>trigonal</td>\n",
       "      <td>8.10</td>\n",
       "      <td>226.000</td>\n",
       "      <td>27.432984</td>\n",
       "      <td>11.300000</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>24.556189</td>\n",
       "      <td>10.999625</td>\n",
       "      <td>2.763500</td>\n",
       "      <td>1.684000</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.485063</td>\n",
       "      <td>1.371649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>BaWF8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00</td>\n",
       "      <td>202.000</td>\n",
       "      <td>22.812640</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>47.315423</td>\n",
       "      <td>15.245810</td>\n",
       "      <td>3.443000</td>\n",
       "      <td>1.662000</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.460005</td>\n",
       "      <td>2.281264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Na5B2PeO13</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>Nd0.02Sr2La0.667(VO4)2</td>\n",
       "      <td>hexagonal</td>\n",
       "      <td>5.00</td>\n",
       "      <td>225.219</td>\n",
       "      <td>21.553374</td>\n",
       "      <td>17.751951</td>\n",
       "      <td>4.522267</td>\n",
       "      <td>39.461656</td>\n",
       "      <td>10.850263</td>\n",
       "      <td>2.635493</td>\n",
       "      <td>1.808829</td>\n",
       "      <td>1.035042</td>\n",
       "      <td>0.477068</td>\n",
       "      <td>1.698855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>Cr0.02CaGdAlO4</td>\n",
       "      <td>tetragonal</td>\n",
       "      <td>6.00</td>\n",
       "      <td>129.480</td>\n",
       "      <td>12.297928</td>\n",
       "      <td>18.444444</td>\n",
       "      <td>4.418803</td>\n",
       "      <td>41.217319</td>\n",
       "      <td>10.378423</td>\n",
       "      <td>2.507578</td>\n",
       "      <td>1.796467</td>\n",
       "      <td>1.052137</td>\n",
       "      <td>0.483923</td>\n",
       "      <td>1.751842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>Y0.57La0.72Sc2.71(BO3)4</td>\n",
       "      <td>trigonal</td>\n",
       "      <td>6.50</td>\n",
       "      <td>236.180</td>\n",
       "      <td>24.574384</td>\n",
       "      <td>11.809000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>25.386557</td>\n",
       "      <td>11.097505</td>\n",
       "      <td>2.730650</td>\n",
       "      <td>1.740925</td>\n",
       "      <td>0.887445</td>\n",
       "      <td>0.483071</td>\n",
       "      <td>1.228719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>YCa9(VO4)7</td>\n",
       "      <td>rhomohedral</td>\n",
       "      <td>4.50</td>\n",
       "      <td>604.000</td>\n",
       "      <td>61.226296</td>\n",
       "      <td>13.422222</td>\n",
       "      <td>4.488889</td>\n",
       "      <td>27.870471</td>\n",
       "      <td>10.883696</td>\n",
       "      <td>2.621111</td>\n",
       "      <td>1.781333</td>\n",
       "      <td>1.009333</td>\n",
       "      <td>0.490911</td>\n",
       "      <td>1.360584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>Zn3BPO7</td>\n",
       "      <td>hexagonal</td>\n",
       "      <td>5.00</td>\n",
       "      <td>166.000</td>\n",
       "      <td>25.978324</td>\n",
       "      <td>13.833333</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>29.159414</td>\n",
       "      <td>11.857833</td>\n",
       "      <td>2.771667</td>\n",
       "      <td>1.699167</td>\n",
       "      <td>0.834167</td>\n",
       "      <td>0.485270</td>\n",
       "      <td>2.164860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>Nd0.02BaLaLiWO6</td>\n",
       "      <td>cubic</td>\n",
       "      <td>5.50</td>\n",
       "      <td>239.200</td>\n",
       "      <td>29.633992</td>\n",
       "      <td>23.872255</td>\n",
       "      <td>4.295409</td>\n",
       "      <td>56.475997</td>\n",
       "      <td>10.565210</td>\n",
       "      <td>2.528224</td>\n",
       "      <td>1.824132</td>\n",
       "      <td>1.065629</td>\n",
       "      <td>0.465199</td>\n",
       "      <td>2.957484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>Nd0.02YCa9(VO4)7</td>\n",
       "      <td>trigonal</td>\n",
       "      <td>4.50</td>\n",
       "      <td>605.200</td>\n",
       "      <td>61.364296</td>\n",
       "      <td>13.442914</td>\n",
       "      <td>4.487783</td>\n",
       "      <td>27.922168</td>\n",
       "      <td>10.881315</td>\n",
       "      <td>2.620453</td>\n",
       "      <td>1.781604</td>\n",
       "      <td>1.009720</td>\n",
       "      <td>0.490878</td>\n",
       "      <td>1.363045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>Nd30.02NaGd(MoO4)2</td>\n",
       "      <td>tetragonal</td>\n",
       "      <td>4.70</td>\n",
       "      <td>2024.200</td>\n",
       "      <td>236.459656</td>\n",
       "      <td>48.172299</td>\n",
       "      <td>2.690148</td>\n",
       "      <td>114.951853</td>\n",
       "      <td>7.146097</td>\n",
       "      <td>1.622865</td>\n",
       "      <td>2.209848</td>\n",
       "      <td>1.615840</td>\n",
       "      <td>0.434283</td>\n",
       "      <td>5.627312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>La2CaB10O19</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>6.50</td>\n",
       "      <td>336.000</td>\n",
       "      <td>37.583308</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>4.687500</td>\n",
       "      <td>22.810328</td>\n",
       "      <td>11.218466</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>1.726562</td>\n",
       "      <td>0.818125</td>\n",
       "      <td>0.482644</td>\n",
       "      <td>1.174478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>BaNaB9O15</td>\n",
       "      <td>trigonal</td>\n",
       "      <td>6.00</td>\n",
       "      <td>232.000</td>\n",
       "      <td>25.820980</td>\n",
       "      <td>8.923077</td>\n",
       "      <td>4.615385</td>\n",
       "      <td>19.136778</td>\n",
       "      <td>11.127088</td>\n",
       "      <td>2.760769</td>\n",
       "      <td>1.731923</td>\n",
       "      <td>0.800769</td>\n",
       "      <td>0.482639</td>\n",
       "      <td>0.993115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>BaBPO5</td>\n",
       "      <td>trigonal</td>\n",
       "      <td>6.50</td>\n",
       "      <td>116.000</td>\n",
       "      <td>8.076660</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>32.387739</td>\n",
       "      <td>11.510863</td>\n",
       "      <td>2.790000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.898750</td>\n",
       "      <td>0.481827</td>\n",
       "      <td>1.009583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>Y3Al5O12</td>\n",
       "      <td>cubic</td>\n",
       "      <td>8.50</td>\n",
       "      <td>278.000</td>\n",
       "      <td>26.917984</td>\n",
       "      <td>13.900000</td>\n",
       "      <td>4.650000</td>\n",
       "      <td>29.680679</td>\n",
       "      <td>10.599905</td>\n",
       "      <td>2.649500</td>\n",
       "      <td>1.720000</td>\n",
       "      <td>0.958000</td>\n",
       "      <td>0.486265</td>\n",
       "      <td>1.345899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>Nd0.02Ca9Gd(VO4)7</td>\n",
       "      <td>hexagonal</td>\n",
       "      <td>4.00</td>\n",
       "      <td>630.200</td>\n",
       "      <td>64.795296</td>\n",
       "      <td>13.998223</td>\n",
       "      <td>4.487783</td>\n",
       "      <td>29.440253</td>\n",
       "      <td>10.879816</td>\n",
       "      <td>2.620009</td>\n",
       "      <td>1.782048</td>\n",
       "      <td>1.011053</td>\n",
       "      <td>0.490174</td>\n",
       "      <td>1.439256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>BaCaBO3F</td>\n",
       "      <td>hexagonal</td>\n",
       "      <td>5.00</td>\n",
       "      <td>114.000</td>\n",
       "      <td>7.425576</td>\n",
       "      <td>16.285714</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>36.458070</td>\n",
       "      <td>11.128571</td>\n",
       "      <td>2.604286</td>\n",
       "      <td>1.848571</td>\n",
       "      <td>1.022857</td>\n",
       "      <td>0.477579</td>\n",
       "      <td>1.060797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>MgAl2O4</td>\n",
       "      <td>cubic</td>\n",
       "      <td>8.00</td>\n",
       "      <td>70.000</td>\n",
       "      <td>7.143328</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>20.323314</td>\n",
       "      <td>10.584314</td>\n",
       "      <td>2.612857</td>\n",
       "      <td>1.641429</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.493919</td>\n",
       "      <td>1.020475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>Zn3Nb2O8</td>\n",
       "      <td>tetragonal</td>\n",
       "      <td>5.50</td>\n",
       "      <td>236.000</td>\n",
       "      <td>38.549656</td>\n",
       "      <td>18.153846</td>\n",
       "      <td>4.307692</td>\n",
       "      <td>39.226537</td>\n",
       "      <td>11.588092</td>\n",
       "      <td>2.743846</td>\n",
       "      <td>1.734615</td>\n",
       "      <td>0.910769</td>\n",
       "      <td>0.481472</td>\n",
       "      <td>2.965358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>Ba3(VO4)2)</td>\n",
       "      <td>hexagonal</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>(Cd0.02Zn0.98)4O(BO2)6</td>\n",
       "      <td>orthorhombic</td>\n",
       "      <td>7.00</td>\n",
       "      <td>255.440</td>\n",
       "      <td>42.890676</td>\n",
       "      <td>11.106087</td>\n",
       "      <td>4.521739</td>\n",
       "      <td>23.395918</td>\n",
       "      <td>11.494264</td>\n",
       "      <td>2.763617</td>\n",
       "      <td>1.710157</td>\n",
       "      <td>0.790261</td>\n",
       "      <td>0.482938</td>\n",
       "      <td>1.864812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>Li2GeO3</td>\n",
       "      <td>orthorhombic</td>\n",
       "      <td>4.00</td>\n",
       "      <td>62.000</td>\n",
       "      <td>6.394996</td>\n",
       "      <td>10.333333</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>22.417182</td>\n",
       "      <td>9.922850</td>\n",
       "      <td>2.381667</td>\n",
       "      <td>1.718333</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.467532</td>\n",
       "      <td>1.065833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>SrO(B2O3)2</td>\n",
       "      <td>orthorhombic</td>\n",
       "      <td>9.00</td>\n",
       "      <td>114.000</td>\n",
       "      <td>12.029324</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>20.236434</td>\n",
       "      <td>11.184467</td>\n",
       "      <td>2.765833</td>\n",
       "      <td>1.734167</td>\n",
       "      <td>0.811667</td>\n",
       "      <td>0.481969</td>\n",
       "      <td>1.002444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>AlPO4</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>6.00</td>\n",
       "      <td>60.000</td>\n",
       "      <td>4.904328</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>20.325237</td>\n",
       "      <td>11.824150</td>\n",
       "      <td>2.926667</td>\n",
       "      <td>1.620000</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.494362</td>\n",
       "      <td>0.817388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>LiNb3O8</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>5.50</td>\n",
       "      <td>190.000</td>\n",
       "      <td>26.254656</td>\n",
       "      <td>15.833333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>34.470779</td>\n",
       "      <td>11.217767</td>\n",
       "      <td>2.775000</td>\n",
       "      <td>1.710000</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.479689</td>\n",
       "      <td>2.187888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>InBO3</td>\n",
       "      <td>rhomohedral</td>\n",
       "      <td>6.00</td>\n",
       "      <td>78.000</td>\n",
       "      <td>9.683996</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>34.724218</td>\n",
       "      <td>10.987740</td>\n",
       "      <td>2.828000</td>\n",
       "      <td>1.682000</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.477854</td>\n",
       "      <td>1.936799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>Nd0.02Ca8.53K1.09La0.95(VO4)7</td>\n",
       "      <td>trigonal</td>\n",
       "      <td>4.50</td>\n",
       "      <td>631.660</td>\n",
       "      <td>62.952676</td>\n",
       "      <td>13.855231</td>\n",
       "      <td>4.432770</td>\n",
       "      <td>29.039063</td>\n",
       "      <td>10.765863</td>\n",
       "      <td>2.593148</td>\n",
       "      <td>1.801011</td>\n",
       "      <td>1.028796</td>\n",
       "      <td>0.490143</td>\n",
       "      <td>1.380844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>Nd0.02YCa4O(BO3)3</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>6.50</td>\n",
       "      <td>215.200</td>\n",
       "      <td>17.930320</td>\n",
       "      <td>11.942286</td>\n",
       "      <td>4.386238</td>\n",
       "      <td>24.667646</td>\n",
       "      <td>10.646815</td>\n",
       "      <td>2.539556</td>\n",
       "      <td>1.807314</td>\n",
       "      <td>0.980999</td>\n",
       "      <td>0.490048</td>\n",
       "      <td>0.995023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>Nd0.02YAl3(BO3)4</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>8.00</td>\n",
       "      <td>195.200</td>\n",
       "      <td>22.199984</td>\n",
       "      <td>9.750250</td>\n",
       "      <td>4.747253</td>\n",
       "      <td>20.377006</td>\n",
       "      <td>11.033686</td>\n",
       "      <td>2.772867</td>\n",
       "      <td>1.688701</td>\n",
       "      <td>0.827053</td>\n",
       "      <td>0.486636</td>\n",
       "      <td>1.108890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>Mg2SiO4</td>\n",
       "      <td>orthorhombic</td>\n",
       "      <td>7.00</td>\n",
       "      <td>70.000</td>\n",
       "      <td>5.815328</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>20.098303</td>\n",
       "      <td>11.130929</td>\n",
       "      <td>2.611429</td>\n",
       "      <td>1.662857</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.498003</td>\n",
       "      <td>0.830761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>PbMoO4</td>\n",
       "      <td>tetragonal</td>\n",
       "      <td>3.00</td>\n",
       "      <td>156.000</td>\n",
       "      <td>21.575328</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>61.191020</td>\n",
       "      <td>11.496917</td>\n",
       "      <td>2.953333</td>\n",
       "      <td>1.711667</td>\n",
       "      <td>0.911667</td>\n",
       "      <td>0.472267</td>\n",
       "      <td>3.595888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>KPb2Cl5</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>2.50</td>\n",
       "      <td>268.000</td>\n",
       "      <td>23.576975</td>\n",
       "      <td>33.500000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>78.841037</td>\n",
       "      <td>10.501513</td>\n",
       "      <td>2.527500</td>\n",
       "      <td>1.942500</td>\n",
       "      <td>1.237500</td>\n",
       "      <td>0.459375</td>\n",
       "      <td>2.947122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>Pb2MoO5</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>3.00</td>\n",
       "      <td>246.000</td>\n",
       "      <td>32.926660</td>\n",
       "      <td>30.750000</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>73.793144</td>\n",
       "      <td>11.252038</td>\n",
       "      <td>2.870000</td>\n",
       "      <td>1.726250</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.466171</td>\n",
       "      <td>4.115832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>Mn2SiO4</td>\n",
       "      <td>orthorhombic</td>\n",
       "      <td>6.50</td>\n",
       "      <td>96.000</td>\n",
       "      <td>17.215328</td>\n",
       "      <td>13.714286</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>28.850887</td>\n",
       "      <td>11.070300</td>\n",
       "      <td>2.680000</td>\n",
       "      <td>1.754286</td>\n",
       "      <td>0.897143</td>\n",
       "      <td>0.486954</td>\n",
       "      <td>2.459333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>CaY2.5Er1.5Ho0.1(SiO4)3O</td>\n",
       "      <td>hexagonal</td>\n",
       "      <td>6.60</td>\n",
       "      <td>372.200</td>\n",
       "      <td>34.208316</td>\n",
       "      <td>17.639810</td>\n",
       "      <td>4.748815</td>\n",
       "      <td>38.955617</td>\n",
       "      <td>11.038415</td>\n",
       "      <td>2.675498</td>\n",
       "      <td>1.793128</td>\n",
       "      <td>0.981706</td>\n",
       "      <td>0.485396</td>\n",
       "      <td>1.621247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>PbGeO3</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>4.00</td>\n",
       "      <td>138.000</td>\n",
       "      <td>16.676996</td>\n",
       "      <td>27.600000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>65.565418</td>\n",
       "      <td>11.234080</td>\n",
       "      <td>2.826000</td>\n",
       "      <td>1.738000</td>\n",
       "      <td>0.914000</td>\n",
       "      <td>0.467304</td>\n",
       "      <td>3.335399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>Pb2FeTaO6</td>\n",
       "      <td>cubic</td>\n",
       "      <td>6.00</td>\n",
       "      <td>311.000</td>\n",
       "      <td>47.231992</td>\n",
       "      <td>31.100000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>74.718706</td>\n",
       "      <td>11.199400</td>\n",
       "      <td>2.757000</td>\n",
       "      <td>1.742000</td>\n",
       "      <td>0.956000</td>\n",
       "      <td>0.466061</td>\n",
       "      <td>4.723199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>Sb5O7I</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>4.00</td>\n",
       "      <td>364.000</td>\n",
       "      <td>38.394324</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>5.692308</td>\n",
       "      <td>65.207514</td>\n",
       "      <td>11.447692</td>\n",
       "      <td>2.845385</td>\n",
       "      <td>1.763077</td>\n",
       "      <td>0.987692</td>\n",
       "      <td>0.462479</td>\n",
       "      <td>2.953410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                        Formula Crystal structure  \\\n",
       "0            0                       MnTeMoO6        tetragonal   \n",
       "1            1                           MgH2        tetragonal   \n",
       "2            2                 CO(NH2)2C4H6O5        monoclinic   \n",
       "3            3                          GaPO4          trigonal   \n",
       "4            4                         Ga3PO7          trigonal   \n",
       "5            5                         LiB3O5      orthorhombic   \n",
       "6            6                         BiB3O6        monoclinic   \n",
       "7            7                      K2Al2B2O7          trigonal   \n",
       "8            8       Nd0.02GdLa0.16Ca4O(BO3)3        monoclinic   \n",
       "9            9       Nd0.02GdLa0.33Ca4O(BO3)3        monoclinic   \n",
       "10          10                           YVO4        tetragonal   \n",
       "11          11                         LiNbO3       rhomohedral   \n",
       "12          12                 Na3Ba2(B3O6)2F         hexagonal   \n",
       "13          13                   NdCa4O(BO3)3        monoclinic   \n",
       "14          14                    LuAl3(BO3)4          trigonal   \n",
       "15          15                          BaWF8               NaN   \n",
       "16          16                     Na5B2PeO13        monoclinic   \n",
       "17          17         Nd0.02Sr2La0.667(VO4)2         hexagonal   \n",
       "18          18                 Cr0.02CaGdAlO4        tetragonal   \n",
       "19          19        Y0.57La0.72Sc2.71(BO3)4          trigonal   \n",
       "20          20                     YCa9(VO4)7       rhomohedral   \n",
       "21          21                        Zn3BPO7         hexagonal   \n",
       "22          22                Nd0.02BaLaLiWO6             cubic   \n",
       "23          23               Nd0.02YCa9(VO4)7          trigonal   \n",
       "24          24             Nd30.02NaGd(MoO4)2        tetragonal   \n",
       "25          25                    La2CaB10O19        monoclinic   \n",
       "26          26                      BaNaB9O15          trigonal   \n",
       "27          27                         BaBPO5          trigonal   \n",
       "28          28                       Y3Al5O12             cubic   \n",
       "29          29              Nd0.02Ca9Gd(VO4)7         hexagonal   \n",
       "30          30                       BaCaBO3F         hexagonal   \n",
       "31          31                        MgAl2O4             cubic   \n",
       "32          32                       Zn3Nb2O8        tetragonal   \n",
       "33          33                     Ba3(VO4)2)         hexagonal   \n",
       "34          34         (Cd0.02Zn0.98)4O(BO2)6      orthorhombic   \n",
       "35          35                        Li2GeO3      orthorhombic   \n",
       "36          36                     SrO(B2O3)2      orthorhombic   \n",
       "37          37                          AlPO4        monoclinic   \n",
       "38          38                        LiNb3O8        monoclinic   \n",
       "39          39                          InBO3       rhomohedral   \n",
       "40          40  Nd0.02Ca8.53K1.09La0.95(VO4)7          trigonal   \n",
       "41          41              Nd0.02YCa4O(BO3)3        monoclinic   \n",
       "42          42               Nd0.02YAl3(BO3)4        monoclinic   \n",
       "43          43                        Mg2SiO4      orthorhombic   \n",
       "44          44                         PbMoO4        tetragonal   \n",
       "45          45                        KPb2Cl5        monoclinic   \n",
       "46          46                        Pb2MoO5        monoclinic   \n",
       "47          47                        Mn2SiO4      orthorhombic   \n",
       "48          48       CaY2.5Er1.5Ho0.1(SiO4)3O         hexagonal   \n",
       "49          49                         PbGeO3        monoclinic   \n",
       "50          50                      Pb2FeTaO6             cubic   \n",
       "51          51                         Sb5O7I        monoclinic   \n",
       "\n",
       "    Hardness (Mohs)  allelectrons_Total  density_Total  allelectrons_Average  \\\n",
       "0              4.50             167.000      23.907992             18.555556   \n",
       "1              4.00              14.000       1.740168              4.666667   \n",
       "2              2.50             102.000       8.511159              4.434783   \n",
       "3              5.50              78.000       8.109328             13.000000   \n",
       "4              6.50             164.000      19.921324             14.909091   \n",
       "5              6.50              58.000       7.650660              6.444444   \n",
       "6              5.75             146.000      16.864992             14.600000   \n",
       "7              6.00             130.000      11.871324             10.000000   \n",
       "8              6.42             249.320      22.345960             13.713971   \n",
       "9              6.35             259.010      23.392140             14.114986   \n",
       "10             5.00              94.000      10.584328             15.666667   \n",
       "11             5.00              68.000       9.107996             13.600000   \n",
       "12             4.50             280.000      24.150564             11.666667   \n",
       "13             5.40             235.000      20.223320             13.055556   \n",
       "14             8.10             226.000      27.432984             11.300000   \n",
       "15             3.00             202.000      22.812640             20.200000   \n",
       "16             3.00               0.000       0.000000              0.000000   \n",
       "17             5.00             225.219      21.553374             17.751951   \n",
       "18             6.00             129.480      12.297928             18.444444   \n",
       "19             6.50             236.180      24.574384             11.809000   \n",
       "20             4.50             604.000      61.226296             13.422222   \n",
       "21             5.00             166.000      25.978324             13.833333   \n",
       "22             5.50             239.200      29.633992             23.872255   \n",
       "23             4.50             605.200      61.364296             13.442914   \n",
       "24             4.70            2024.200     236.459656             48.172299   \n",
       "25             6.50             336.000      37.583308             10.500000   \n",
       "26             6.00             232.000      25.820980              8.923077   \n",
       "27             6.50             116.000       8.076660             14.500000   \n",
       "28             8.50             278.000      26.917984             13.900000   \n",
       "29             4.00             630.200      64.795296             13.998223   \n",
       "30             5.00             114.000       7.425576             16.285714   \n",
       "31             8.00              70.000       7.143328             10.000000   \n",
       "32             5.50             236.000      38.549656             18.153846   \n",
       "33             4.50               0.000       0.000000              0.000000   \n",
       "34             7.00             255.440      42.890676             11.106087   \n",
       "35             4.00              62.000       6.394996             10.333333   \n",
       "36             9.00             114.000      12.029324              9.500000   \n",
       "37             6.00              60.000       4.904328             10.000000   \n",
       "38             5.50             190.000      26.254656             15.833333   \n",
       "39             6.00              78.000       9.683996             15.600000   \n",
       "40             4.50             631.660      62.952676             13.855231   \n",
       "41             6.50             215.200      17.930320             11.942286   \n",
       "42             8.00             195.200      22.199984              9.750250   \n",
       "43             7.00              70.000       5.815328             10.000000   \n",
       "44             3.00             156.000      21.575328             26.000000   \n",
       "45             2.50             268.000      23.576975             33.500000   \n",
       "46             3.00             246.000      32.926660             30.750000   \n",
       "47             6.50              96.000      17.215328             13.714286   \n",
       "48             6.60             372.200      34.208316             17.639810   \n",
       "49             4.00             138.000      16.676996             27.600000   \n",
       "50             6.00             311.000      47.231992             31.100000   \n",
       "51             4.00             364.000      38.394324             28.000000   \n",
       "\n",
       "    val_e_Average  atomicweight_Average  ionenergy_Average  \\\n",
       "0        5.000000             41.609136          11.693844   \n",
       "1        1.333333              8.773227          11.614333   \n",
       "2        3.304348              8.440584          13.176622   \n",
       "3        5.333333             27.448814          11.826400   \n",
       "4        5.090909             32.012361          11.255573   \n",
       "5        4.444444             13.261239          10.930689   \n",
       "6        5.000000             33.739258          11.388810   \n",
       "7        4.307692             20.443453          10.198131   \n",
       "8        4.365237             29.432344          10.598482   \n",
       "9        4.343324             30.446537          10.551961   \n",
       "10       4.666667             33.973910          11.239317   \n",
       "11       4.000000             29.568292          10.600980   \n",
       "12       4.333333             25.810253          10.686196   \n",
       "13       4.388889             27.609017          10.614044   \n",
       "14       4.750000             24.556189          10.999625   \n",
       "15       6.000000             47.315423          15.245810   \n",
       "16       0.000000              0.000000           0.000000   \n",
       "17       4.522267             39.461656          10.850263   \n",
       "18       4.418803             41.217319          10.378423   \n",
       "19       4.600000             25.386557          11.097505   \n",
       "20       4.488889             27.870471          10.883696   \n",
       "21       4.666667             29.159414          11.857833   \n",
       "22       4.295409             56.475997          10.565210   \n",
       "23       4.487783             27.922168          10.881315   \n",
       "24       2.690148            114.951853           7.146097   \n",
       "25       4.687500             22.810328          11.218466   \n",
       "26       4.615385             19.136778          11.127088   \n",
       "27       5.000000             32.387739          11.510863   \n",
       "28       4.650000             29.680679          10.599905   \n",
       "29       4.487783             29.440253          10.879816   \n",
       "30       4.571429             36.458070          11.128571   \n",
       "31       4.571429             20.323314          10.584314   \n",
       "32       4.307692             39.226537          11.588092   \n",
       "33       0.000000              0.000000           0.000000   \n",
       "34       4.521739             23.395918          11.494264   \n",
       "35       4.000000             22.417182           9.922850   \n",
       "36       4.666667             20.236434          11.184467   \n",
       "37       5.333333             20.325237          11.824150   \n",
       "38       4.333333             34.470779          11.217767   \n",
       "39       4.800000             34.724218          10.987740   \n",
       "40       4.432770             29.039063          10.765863   \n",
       "41       4.386238             24.667646          10.646815   \n",
       "42       4.747253             20.377006          11.033686   \n",
       "43       4.571429             20.098303          11.130929   \n",
       "44       4.833333             61.191020          11.496917   \n",
       "45       5.500000             78.841037          10.501513   \n",
       "46       4.875000             73.793144          11.252038   \n",
       "47       4.571429             28.850887          11.070300   \n",
       "48       4.748815             38.955617          11.038415   \n",
       "49       5.200000             65.565418          11.234080   \n",
       "50       4.800000             74.718706          11.199400   \n",
       "51       5.692308             65.207514          11.447692   \n",
       "\n",
       "    el_neg_chi_Average  R_vdw_element_Average  R_cov_element_Average  \\\n",
       "0             2.938889               1.711111               0.884444   \n",
       "1             1.903333               1.310000               0.680000   \n",
       "2             2.672609               1.379130               0.530870   \n",
       "3             2.960000               1.625000               0.813333   \n",
       "4             2.881818               1.640909               0.841818   \n",
       "5             2.700000               1.686667               0.780000   \n",
       "6             2.866000               1.695000               0.786000   \n",
       "7             2.540000               1.820000               0.972308   \n",
       "8             2.525787               1.813894               0.992739   \n",
       "9             2.512578               1.819602               1.001515   \n",
       "10            2.768333               1.745000               0.960000   \n",
       "11            2.580000               1.712000               0.956000   \n",
       "12            2.586250               1.808333               0.926667   \n",
       "13            2.536667               1.810556               0.986667   \n",
       "14            2.763500               1.684000               0.825000   \n",
       "15            3.443000               1.662000               0.836000   \n",
       "16            0.000000               0.000000               0.000000   \n",
       "17            2.635493               1.808829               1.035042   \n",
       "18            2.507578               1.796467               1.052137   \n",
       "19            2.730650               1.740925               0.887445   \n",
       "20            2.621111               1.781333               1.009333   \n",
       "21            2.771667               1.699167               0.834167   \n",
       "22            2.528224               1.824132               1.065629   \n",
       "23            2.620453               1.781604               1.009720   \n",
       "24            1.622865               2.209848               1.615840   \n",
       "25            2.780000               1.726562               0.818125   \n",
       "26            2.760769               1.731923               0.800769   \n",
       "27            2.790000               1.750000               0.898750   \n",
       "28            2.649500               1.720000               0.958000   \n",
       "29            2.620009               1.782048               1.011053   \n",
       "30            2.604286               1.848571               1.022857   \n",
       "31            2.612857               1.641429               0.920000   \n",
       "32            2.743846               1.734615               0.910769   \n",
       "33            0.000000               0.000000               0.000000   \n",
       "34            2.763617               1.710157               0.790261   \n",
       "35            2.381667               1.718333               0.953333   \n",
       "36            2.765833               1.734167               0.811667   \n",
       "37            2.926667               1.620000               0.815000   \n",
       "38            2.775000               1.710000               0.925000   \n",
       "39            2.828000               1.682000               0.836000   \n",
       "40            2.593148               1.801011               1.028796   \n",
       "41            2.539556               1.807314               0.980999   \n",
       "42            2.772867               1.688701               0.827053   \n",
       "43            2.611429               1.662857               0.928571   \n",
       "44            2.953333               1.711667               0.911667   \n",
       "45            2.527500               1.942500               1.237500   \n",
       "46            2.870000               1.726250               0.945000   \n",
       "47            2.680000               1.754286               0.897143   \n",
       "48            2.675498               1.793128               0.981706   \n",
       "49            2.826000               1.738000               0.914000   \n",
       "50            2.757000               1.742000               0.956000   \n",
       "51            2.845385               1.763077               0.987692   \n",
       "\n",
       "    zaratio_Average  density_Average  \n",
       "0          0.477830         2.656444  \n",
       "1          0.825990         0.580056  \n",
       "2          0.713850         0.370050  \n",
       "3          0.488163         1.351555  \n",
       "4          0.483480         1.811029  \n",
       "5          0.479962         0.850073  \n",
       "6          0.478464         1.686499  \n",
       "7          0.489274         0.913179  \n",
       "8          0.487604         1.229151  \n",
       "9          0.486888         1.274776  \n",
       "10         0.481708         1.764055  \n",
       "11         0.474714         1.821599  \n",
       "12         0.479152         1.006274  \n",
       "13         0.488869         1.123518  \n",
       "14         0.485063         1.371649  \n",
       "15         0.460005         2.281264  \n",
       "16         0.000000         0.000000  \n",
       "17         0.477068         1.698855  \n",
       "18         0.483923         1.751842  \n",
       "19         0.483071         1.228719  \n",
       "20         0.490911         1.360584  \n",
       "21         0.485270         2.164860  \n",
       "22         0.465199         2.957484  \n",
       "23         0.490878         1.363045  \n",
       "24         0.434283         5.627312  \n",
       "25         0.482644         1.174478  \n",
       "26         0.482639         0.993115  \n",
       "27         0.481827         1.009583  \n",
       "28         0.486265         1.345899  \n",
       "29         0.490174         1.439256  \n",
       "30         0.477579         1.060797  \n",
       "31         0.493919         1.020475  \n",
       "32         0.481472         2.965358  \n",
       "33         0.000000         0.000000  \n",
       "34         0.482938         1.864812  \n",
       "35         0.467532         1.065833  \n",
       "36         0.481969         1.002444  \n",
       "37         0.494362         0.817388  \n",
       "38         0.479689         2.187888  \n",
       "39         0.477854         1.936799  \n",
       "40         0.490143         1.380844  \n",
       "41         0.490048         0.995023  \n",
       "42         0.486636         1.108890  \n",
       "43         0.498003         0.830761  \n",
       "44         0.472267         3.595888  \n",
       "45         0.459375         2.947122  \n",
       "46         0.466171         4.115832  \n",
       "47         0.486954         2.459333  \n",
       "48         0.485396         1.621247  \n",
       "49         0.467304         3.335399  \n",
       "50         0.466061         4.723199  \n",
       "51         0.462479         2.953410  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mostrando o Dataframe\n",
    "print(\"Dataset de teste:\")\n",
    "display(df_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamento do Datafram de teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os Dataframes prontos, foi retirado as 3 colunas que só faziam parte do dataset de teste, sendo as referentes a identificação de cada mineral:\n",
    "\n",
    "- 'Unnamed: 0'\n",
    "- 'Crysral structure'\n",
    "- 'Formula'\n",
    "\n",
    "Seguido pela renomeação da coluna do target, a qual passou de 'Hardness (Mohs)' para somente 'Hardness', isso foi feito visando agilizar a produção do código.\n",
    "\n",
    "Ao fim, foi aplicado a função dropna() para retirar quaisquer valores nan que pudessem existir e foi freita a redefinição do index do Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hardness</th>\n",
       "      <th>allelectrons_Total</th>\n",
       "      <th>density_Total</th>\n",
       "      <th>allelectrons_Average</th>\n",
       "      <th>val_e_Average</th>\n",
       "      <th>atomicweight_Average</th>\n",
       "      <th>ionenergy_Average</th>\n",
       "      <th>el_neg_chi_Average</th>\n",
       "      <th>R_vdw_element_Average</th>\n",
       "      <th>R_cov_element_Average</th>\n",
       "      <th>zaratio_Average</th>\n",
       "      <th>density_Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.50</td>\n",
       "      <td>167.000</td>\n",
       "      <td>23.907992</td>\n",
       "      <td>18.555556</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>41.609136</td>\n",
       "      <td>11.693844</td>\n",
       "      <td>2.938889</td>\n",
       "      <td>1.711111</td>\n",
       "      <td>0.884444</td>\n",
       "      <td>0.477830</td>\n",
       "      <td>2.656444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.00</td>\n",
       "      <td>14.000</td>\n",
       "      <td>1.740168</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>8.773227</td>\n",
       "      <td>11.614333</td>\n",
       "      <td>1.903333</td>\n",
       "      <td>1.310000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.825990</td>\n",
       "      <td>0.580056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.50</td>\n",
       "      <td>102.000</td>\n",
       "      <td>8.511159</td>\n",
       "      <td>4.434783</td>\n",
       "      <td>3.304348</td>\n",
       "      <td>8.440584</td>\n",
       "      <td>13.176622</td>\n",
       "      <td>2.672609</td>\n",
       "      <td>1.379130</td>\n",
       "      <td>0.530870</td>\n",
       "      <td>0.713850</td>\n",
       "      <td>0.370050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.50</td>\n",
       "      <td>78.000</td>\n",
       "      <td>8.109328</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>27.448814</td>\n",
       "      <td>11.826400</td>\n",
       "      <td>2.960000</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.488163</td>\n",
       "      <td>1.351555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.50</td>\n",
       "      <td>164.000</td>\n",
       "      <td>19.921324</td>\n",
       "      <td>14.909091</td>\n",
       "      <td>5.090909</td>\n",
       "      <td>32.012361</td>\n",
       "      <td>11.255573</td>\n",
       "      <td>2.881818</td>\n",
       "      <td>1.640909</td>\n",
       "      <td>0.841818</td>\n",
       "      <td>0.483480</td>\n",
       "      <td>1.811029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.50</td>\n",
       "      <td>58.000</td>\n",
       "      <td>7.650660</td>\n",
       "      <td>6.444444</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>13.261239</td>\n",
       "      <td>10.930689</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>1.686667</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.479962</td>\n",
       "      <td>0.850073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.75</td>\n",
       "      <td>146.000</td>\n",
       "      <td>16.864992</td>\n",
       "      <td>14.600000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>33.739258</td>\n",
       "      <td>11.388810</td>\n",
       "      <td>2.866000</td>\n",
       "      <td>1.695000</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.478464</td>\n",
       "      <td>1.686499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.00</td>\n",
       "      <td>130.000</td>\n",
       "      <td>11.871324</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.307692</td>\n",
       "      <td>20.443453</td>\n",
       "      <td>10.198131</td>\n",
       "      <td>2.540000</td>\n",
       "      <td>1.820000</td>\n",
       "      <td>0.972308</td>\n",
       "      <td>0.489274</td>\n",
       "      <td>0.913179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.42</td>\n",
       "      <td>249.320</td>\n",
       "      <td>22.345960</td>\n",
       "      <td>13.713971</td>\n",
       "      <td>4.365237</td>\n",
       "      <td>29.432344</td>\n",
       "      <td>10.598482</td>\n",
       "      <td>2.525787</td>\n",
       "      <td>1.813894</td>\n",
       "      <td>0.992739</td>\n",
       "      <td>0.487604</td>\n",
       "      <td>1.229151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.35</td>\n",
       "      <td>259.010</td>\n",
       "      <td>23.392140</td>\n",
       "      <td>14.114986</td>\n",
       "      <td>4.343324</td>\n",
       "      <td>30.446537</td>\n",
       "      <td>10.551961</td>\n",
       "      <td>2.512578</td>\n",
       "      <td>1.819602</td>\n",
       "      <td>1.001515</td>\n",
       "      <td>0.486888</td>\n",
       "      <td>1.274776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.00</td>\n",
       "      <td>94.000</td>\n",
       "      <td>10.584328</td>\n",
       "      <td>15.666667</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>33.973910</td>\n",
       "      <td>11.239317</td>\n",
       "      <td>2.768333</td>\n",
       "      <td>1.745000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.481708</td>\n",
       "      <td>1.764055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.00</td>\n",
       "      <td>68.000</td>\n",
       "      <td>9.107996</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>29.568292</td>\n",
       "      <td>10.600980</td>\n",
       "      <td>2.580000</td>\n",
       "      <td>1.712000</td>\n",
       "      <td>0.956000</td>\n",
       "      <td>0.474714</td>\n",
       "      <td>1.821599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.50</td>\n",
       "      <td>280.000</td>\n",
       "      <td>24.150564</td>\n",
       "      <td>11.666667</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>25.810253</td>\n",
       "      <td>10.686196</td>\n",
       "      <td>2.586250</td>\n",
       "      <td>1.808333</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.479152</td>\n",
       "      <td>1.006274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.40</td>\n",
       "      <td>235.000</td>\n",
       "      <td>20.223320</td>\n",
       "      <td>13.055556</td>\n",
       "      <td>4.388889</td>\n",
       "      <td>27.609017</td>\n",
       "      <td>10.614044</td>\n",
       "      <td>2.536667</td>\n",
       "      <td>1.810556</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.488869</td>\n",
       "      <td>1.123518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8.10</td>\n",
       "      <td>226.000</td>\n",
       "      <td>27.432984</td>\n",
       "      <td>11.300000</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>24.556189</td>\n",
       "      <td>10.999625</td>\n",
       "      <td>2.763500</td>\n",
       "      <td>1.684000</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.485063</td>\n",
       "      <td>1.371649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.00</td>\n",
       "      <td>202.000</td>\n",
       "      <td>22.812640</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>47.315423</td>\n",
       "      <td>15.245810</td>\n",
       "      <td>3.443000</td>\n",
       "      <td>1.662000</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.460005</td>\n",
       "      <td>2.281264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.00</td>\n",
       "      <td>225.219</td>\n",
       "      <td>21.553374</td>\n",
       "      <td>17.751951</td>\n",
       "      <td>4.522267</td>\n",
       "      <td>39.461656</td>\n",
       "      <td>10.850263</td>\n",
       "      <td>2.635493</td>\n",
       "      <td>1.808829</td>\n",
       "      <td>1.035042</td>\n",
       "      <td>0.477068</td>\n",
       "      <td>1.698855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.00</td>\n",
       "      <td>129.480</td>\n",
       "      <td>12.297928</td>\n",
       "      <td>18.444444</td>\n",
       "      <td>4.418803</td>\n",
       "      <td>41.217319</td>\n",
       "      <td>10.378423</td>\n",
       "      <td>2.507578</td>\n",
       "      <td>1.796467</td>\n",
       "      <td>1.052137</td>\n",
       "      <td>0.483923</td>\n",
       "      <td>1.751842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.50</td>\n",
       "      <td>236.180</td>\n",
       "      <td>24.574384</td>\n",
       "      <td>11.809000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>25.386557</td>\n",
       "      <td>11.097505</td>\n",
       "      <td>2.730650</td>\n",
       "      <td>1.740925</td>\n",
       "      <td>0.887445</td>\n",
       "      <td>0.483071</td>\n",
       "      <td>1.228719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.50</td>\n",
       "      <td>604.000</td>\n",
       "      <td>61.226296</td>\n",
       "      <td>13.422222</td>\n",
       "      <td>4.488889</td>\n",
       "      <td>27.870471</td>\n",
       "      <td>10.883696</td>\n",
       "      <td>2.621111</td>\n",
       "      <td>1.781333</td>\n",
       "      <td>1.009333</td>\n",
       "      <td>0.490911</td>\n",
       "      <td>1.360584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.00</td>\n",
       "      <td>166.000</td>\n",
       "      <td>25.978324</td>\n",
       "      <td>13.833333</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>29.159414</td>\n",
       "      <td>11.857833</td>\n",
       "      <td>2.771667</td>\n",
       "      <td>1.699167</td>\n",
       "      <td>0.834167</td>\n",
       "      <td>0.485270</td>\n",
       "      <td>2.164860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.50</td>\n",
       "      <td>239.200</td>\n",
       "      <td>29.633992</td>\n",
       "      <td>23.872255</td>\n",
       "      <td>4.295409</td>\n",
       "      <td>56.475997</td>\n",
       "      <td>10.565210</td>\n",
       "      <td>2.528224</td>\n",
       "      <td>1.824132</td>\n",
       "      <td>1.065629</td>\n",
       "      <td>0.465199</td>\n",
       "      <td>2.957484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.50</td>\n",
       "      <td>605.200</td>\n",
       "      <td>61.364296</td>\n",
       "      <td>13.442914</td>\n",
       "      <td>4.487783</td>\n",
       "      <td>27.922168</td>\n",
       "      <td>10.881315</td>\n",
       "      <td>2.620453</td>\n",
       "      <td>1.781604</td>\n",
       "      <td>1.009720</td>\n",
       "      <td>0.490878</td>\n",
       "      <td>1.363045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.70</td>\n",
       "      <td>2024.200</td>\n",
       "      <td>236.459656</td>\n",
       "      <td>48.172299</td>\n",
       "      <td>2.690148</td>\n",
       "      <td>114.951853</td>\n",
       "      <td>7.146097</td>\n",
       "      <td>1.622865</td>\n",
       "      <td>2.209848</td>\n",
       "      <td>1.615840</td>\n",
       "      <td>0.434283</td>\n",
       "      <td>5.627312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.50</td>\n",
       "      <td>336.000</td>\n",
       "      <td>37.583308</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>4.687500</td>\n",
       "      <td>22.810328</td>\n",
       "      <td>11.218466</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>1.726562</td>\n",
       "      <td>0.818125</td>\n",
       "      <td>0.482644</td>\n",
       "      <td>1.174478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6.00</td>\n",
       "      <td>232.000</td>\n",
       "      <td>25.820980</td>\n",
       "      <td>8.923077</td>\n",
       "      <td>4.615385</td>\n",
       "      <td>19.136778</td>\n",
       "      <td>11.127088</td>\n",
       "      <td>2.760769</td>\n",
       "      <td>1.731923</td>\n",
       "      <td>0.800769</td>\n",
       "      <td>0.482639</td>\n",
       "      <td>0.993115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6.50</td>\n",
       "      <td>116.000</td>\n",
       "      <td>8.076660</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>32.387739</td>\n",
       "      <td>11.510863</td>\n",
       "      <td>2.790000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.898750</td>\n",
       "      <td>0.481827</td>\n",
       "      <td>1.009583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8.50</td>\n",
       "      <td>278.000</td>\n",
       "      <td>26.917984</td>\n",
       "      <td>13.900000</td>\n",
       "      <td>4.650000</td>\n",
       "      <td>29.680679</td>\n",
       "      <td>10.599905</td>\n",
       "      <td>2.649500</td>\n",
       "      <td>1.720000</td>\n",
       "      <td>0.958000</td>\n",
       "      <td>0.486265</td>\n",
       "      <td>1.345899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.00</td>\n",
       "      <td>630.200</td>\n",
       "      <td>64.795296</td>\n",
       "      <td>13.998223</td>\n",
       "      <td>4.487783</td>\n",
       "      <td>29.440253</td>\n",
       "      <td>10.879816</td>\n",
       "      <td>2.620009</td>\n",
       "      <td>1.782048</td>\n",
       "      <td>1.011053</td>\n",
       "      <td>0.490174</td>\n",
       "      <td>1.439256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5.00</td>\n",
       "      <td>114.000</td>\n",
       "      <td>7.425576</td>\n",
       "      <td>16.285714</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>36.458070</td>\n",
       "      <td>11.128571</td>\n",
       "      <td>2.604286</td>\n",
       "      <td>1.848571</td>\n",
       "      <td>1.022857</td>\n",
       "      <td>0.477579</td>\n",
       "      <td>1.060797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8.00</td>\n",
       "      <td>70.000</td>\n",
       "      <td>7.143328</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>20.323314</td>\n",
       "      <td>10.584314</td>\n",
       "      <td>2.612857</td>\n",
       "      <td>1.641429</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.493919</td>\n",
       "      <td>1.020475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5.50</td>\n",
       "      <td>236.000</td>\n",
       "      <td>38.549656</td>\n",
       "      <td>18.153846</td>\n",
       "      <td>4.307692</td>\n",
       "      <td>39.226537</td>\n",
       "      <td>11.588092</td>\n",
       "      <td>2.743846</td>\n",
       "      <td>1.734615</td>\n",
       "      <td>0.910769</td>\n",
       "      <td>0.481472</td>\n",
       "      <td>2.965358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4.50</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>7.00</td>\n",
       "      <td>255.440</td>\n",
       "      <td>42.890676</td>\n",
       "      <td>11.106087</td>\n",
       "      <td>4.521739</td>\n",
       "      <td>23.395918</td>\n",
       "      <td>11.494264</td>\n",
       "      <td>2.763617</td>\n",
       "      <td>1.710157</td>\n",
       "      <td>0.790261</td>\n",
       "      <td>0.482938</td>\n",
       "      <td>1.864812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4.00</td>\n",
       "      <td>62.000</td>\n",
       "      <td>6.394996</td>\n",
       "      <td>10.333333</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>22.417182</td>\n",
       "      <td>9.922850</td>\n",
       "      <td>2.381667</td>\n",
       "      <td>1.718333</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.467532</td>\n",
       "      <td>1.065833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>9.00</td>\n",
       "      <td>114.000</td>\n",
       "      <td>12.029324</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>20.236434</td>\n",
       "      <td>11.184467</td>\n",
       "      <td>2.765833</td>\n",
       "      <td>1.734167</td>\n",
       "      <td>0.811667</td>\n",
       "      <td>0.481969</td>\n",
       "      <td>1.002444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>6.00</td>\n",
       "      <td>60.000</td>\n",
       "      <td>4.904328</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>20.325237</td>\n",
       "      <td>11.824150</td>\n",
       "      <td>2.926667</td>\n",
       "      <td>1.620000</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.494362</td>\n",
       "      <td>0.817388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>5.50</td>\n",
       "      <td>190.000</td>\n",
       "      <td>26.254656</td>\n",
       "      <td>15.833333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>34.470779</td>\n",
       "      <td>11.217767</td>\n",
       "      <td>2.775000</td>\n",
       "      <td>1.710000</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.479689</td>\n",
       "      <td>2.187888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>6.00</td>\n",
       "      <td>78.000</td>\n",
       "      <td>9.683996</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>34.724218</td>\n",
       "      <td>10.987740</td>\n",
       "      <td>2.828000</td>\n",
       "      <td>1.682000</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.477854</td>\n",
       "      <td>1.936799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4.50</td>\n",
       "      <td>631.660</td>\n",
       "      <td>62.952676</td>\n",
       "      <td>13.855231</td>\n",
       "      <td>4.432770</td>\n",
       "      <td>29.039063</td>\n",
       "      <td>10.765863</td>\n",
       "      <td>2.593148</td>\n",
       "      <td>1.801011</td>\n",
       "      <td>1.028796</td>\n",
       "      <td>0.490143</td>\n",
       "      <td>1.380844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>6.50</td>\n",
       "      <td>215.200</td>\n",
       "      <td>17.930320</td>\n",
       "      <td>11.942286</td>\n",
       "      <td>4.386238</td>\n",
       "      <td>24.667646</td>\n",
       "      <td>10.646815</td>\n",
       "      <td>2.539556</td>\n",
       "      <td>1.807314</td>\n",
       "      <td>0.980999</td>\n",
       "      <td>0.490048</td>\n",
       "      <td>0.995023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>8.00</td>\n",
       "      <td>195.200</td>\n",
       "      <td>22.199984</td>\n",
       "      <td>9.750250</td>\n",
       "      <td>4.747253</td>\n",
       "      <td>20.377006</td>\n",
       "      <td>11.033686</td>\n",
       "      <td>2.772867</td>\n",
       "      <td>1.688701</td>\n",
       "      <td>0.827053</td>\n",
       "      <td>0.486636</td>\n",
       "      <td>1.108890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>7.00</td>\n",
       "      <td>70.000</td>\n",
       "      <td>5.815328</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>20.098303</td>\n",
       "      <td>11.130929</td>\n",
       "      <td>2.611429</td>\n",
       "      <td>1.662857</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.498003</td>\n",
       "      <td>0.830761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3.00</td>\n",
       "      <td>156.000</td>\n",
       "      <td>21.575328</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>61.191020</td>\n",
       "      <td>11.496917</td>\n",
       "      <td>2.953333</td>\n",
       "      <td>1.711667</td>\n",
       "      <td>0.911667</td>\n",
       "      <td>0.472267</td>\n",
       "      <td>3.595888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2.50</td>\n",
       "      <td>268.000</td>\n",
       "      <td>23.576975</td>\n",
       "      <td>33.500000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>78.841037</td>\n",
       "      <td>10.501513</td>\n",
       "      <td>2.527500</td>\n",
       "      <td>1.942500</td>\n",
       "      <td>1.237500</td>\n",
       "      <td>0.459375</td>\n",
       "      <td>2.947122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3.00</td>\n",
       "      <td>246.000</td>\n",
       "      <td>32.926660</td>\n",
       "      <td>30.750000</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>73.793144</td>\n",
       "      <td>11.252038</td>\n",
       "      <td>2.870000</td>\n",
       "      <td>1.726250</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.466171</td>\n",
       "      <td>4.115832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>6.50</td>\n",
       "      <td>96.000</td>\n",
       "      <td>17.215328</td>\n",
       "      <td>13.714286</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>28.850887</td>\n",
       "      <td>11.070300</td>\n",
       "      <td>2.680000</td>\n",
       "      <td>1.754286</td>\n",
       "      <td>0.897143</td>\n",
       "      <td>0.486954</td>\n",
       "      <td>2.459333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>6.60</td>\n",
       "      <td>372.200</td>\n",
       "      <td>34.208316</td>\n",
       "      <td>17.639810</td>\n",
       "      <td>4.748815</td>\n",
       "      <td>38.955617</td>\n",
       "      <td>11.038415</td>\n",
       "      <td>2.675498</td>\n",
       "      <td>1.793128</td>\n",
       "      <td>0.981706</td>\n",
       "      <td>0.485396</td>\n",
       "      <td>1.621247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4.00</td>\n",
       "      <td>138.000</td>\n",
       "      <td>16.676996</td>\n",
       "      <td>27.600000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>65.565418</td>\n",
       "      <td>11.234080</td>\n",
       "      <td>2.826000</td>\n",
       "      <td>1.738000</td>\n",
       "      <td>0.914000</td>\n",
       "      <td>0.467304</td>\n",
       "      <td>3.335399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>6.00</td>\n",
       "      <td>311.000</td>\n",
       "      <td>47.231992</td>\n",
       "      <td>31.100000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>74.718706</td>\n",
       "      <td>11.199400</td>\n",
       "      <td>2.757000</td>\n",
       "      <td>1.742000</td>\n",
       "      <td>0.956000</td>\n",
       "      <td>0.466061</td>\n",
       "      <td>4.723199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>4.00</td>\n",
       "      <td>364.000</td>\n",
       "      <td>38.394324</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>5.692308</td>\n",
       "      <td>65.207514</td>\n",
       "      <td>11.447692</td>\n",
       "      <td>2.845385</td>\n",
       "      <td>1.763077</td>\n",
       "      <td>0.987692</td>\n",
       "      <td>0.462479</td>\n",
       "      <td>2.953410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Hardness  allelectrons_Total  density_Total  allelectrons_Average  \\\n",
       "0       4.50             167.000      23.907992             18.555556   \n",
       "1       4.00              14.000       1.740168              4.666667   \n",
       "2       2.50             102.000       8.511159              4.434783   \n",
       "3       5.50              78.000       8.109328             13.000000   \n",
       "4       6.50             164.000      19.921324             14.909091   \n",
       "5       6.50              58.000       7.650660              6.444444   \n",
       "6       5.75             146.000      16.864992             14.600000   \n",
       "7       6.00             130.000      11.871324             10.000000   \n",
       "8       6.42             249.320      22.345960             13.713971   \n",
       "9       6.35             259.010      23.392140             14.114986   \n",
       "10      5.00              94.000      10.584328             15.666667   \n",
       "11      5.00              68.000       9.107996             13.600000   \n",
       "12      4.50             280.000      24.150564             11.666667   \n",
       "13      5.40             235.000      20.223320             13.055556   \n",
       "14      8.10             226.000      27.432984             11.300000   \n",
       "15      3.00             202.000      22.812640             20.200000   \n",
       "16      3.00               0.000       0.000000              0.000000   \n",
       "17      5.00             225.219      21.553374             17.751951   \n",
       "18      6.00             129.480      12.297928             18.444444   \n",
       "19      6.50             236.180      24.574384             11.809000   \n",
       "20      4.50             604.000      61.226296             13.422222   \n",
       "21      5.00             166.000      25.978324             13.833333   \n",
       "22      5.50             239.200      29.633992             23.872255   \n",
       "23      4.50             605.200      61.364296             13.442914   \n",
       "24      4.70            2024.200     236.459656             48.172299   \n",
       "25      6.50             336.000      37.583308             10.500000   \n",
       "26      6.00             232.000      25.820980              8.923077   \n",
       "27      6.50             116.000       8.076660             14.500000   \n",
       "28      8.50             278.000      26.917984             13.900000   \n",
       "29      4.00             630.200      64.795296             13.998223   \n",
       "30      5.00             114.000       7.425576             16.285714   \n",
       "31      8.00              70.000       7.143328             10.000000   \n",
       "32      5.50             236.000      38.549656             18.153846   \n",
       "33      4.50               0.000       0.000000              0.000000   \n",
       "34      7.00             255.440      42.890676             11.106087   \n",
       "35      4.00              62.000       6.394996             10.333333   \n",
       "36      9.00             114.000      12.029324              9.500000   \n",
       "37      6.00              60.000       4.904328             10.000000   \n",
       "38      5.50             190.000      26.254656             15.833333   \n",
       "39      6.00              78.000       9.683996             15.600000   \n",
       "40      4.50             631.660      62.952676             13.855231   \n",
       "41      6.50             215.200      17.930320             11.942286   \n",
       "42      8.00             195.200      22.199984              9.750250   \n",
       "43      7.00              70.000       5.815328             10.000000   \n",
       "44      3.00             156.000      21.575328             26.000000   \n",
       "45      2.50             268.000      23.576975             33.500000   \n",
       "46      3.00             246.000      32.926660             30.750000   \n",
       "47      6.50              96.000      17.215328             13.714286   \n",
       "48      6.60             372.200      34.208316             17.639810   \n",
       "49      4.00             138.000      16.676996             27.600000   \n",
       "50      6.00             311.000      47.231992             31.100000   \n",
       "51      4.00             364.000      38.394324             28.000000   \n",
       "\n",
       "    val_e_Average  atomicweight_Average  ionenergy_Average  \\\n",
       "0        5.000000             41.609136          11.693844   \n",
       "1        1.333333              8.773227          11.614333   \n",
       "2        3.304348              8.440584          13.176622   \n",
       "3        5.333333             27.448814          11.826400   \n",
       "4        5.090909             32.012361          11.255573   \n",
       "5        4.444444             13.261239          10.930689   \n",
       "6        5.000000             33.739258          11.388810   \n",
       "7        4.307692             20.443453          10.198131   \n",
       "8        4.365237             29.432344          10.598482   \n",
       "9        4.343324             30.446537          10.551961   \n",
       "10       4.666667             33.973910          11.239317   \n",
       "11       4.000000             29.568292          10.600980   \n",
       "12       4.333333             25.810253          10.686196   \n",
       "13       4.388889             27.609017          10.614044   \n",
       "14       4.750000             24.556189          10.999625   \n",
       "15       6.000000             47.315423          15.245810   \n",
       "16       0.000000              0.000000           0.000000   \n",
       "17       4.522267             39.461656          10.850263   \n",
       "18       4.418803             41.217319          10.378423   \n",
       "19       4.600000             25.386557          11.097505   \n",
       "20       4.488889             27.870471          10.883696   \n",
       "21       4.666667             29.159414          11.857833   \n",
       "22       4.295409             56.475997          10.565210   \n",
       "23       4.487783             27.922168          10.881315   \n",
       "24       2.690148            114.951853           7.146097   \n",
       "25       4.687500             22.810328          11.218466   \n",
       "26       4.615385             19.136778          11.127088   \n",
       "27       5.000000             32.387739          11.510863   \n",
       "28       4.650000             29.680679          10.599905   \n",
       "29       4.487783             29.440253          10.879816   \n",
       "30       4.571429             36.458070          11.128571   \n",
       "31       4.571429             20.323314          10.584314   \n",
       "32       4.307692             39.226537          11.588092   \n",
       "33       0.000000              0.000000           0.000000   \n",
       "34       4.521739             23.395918          11.494264   \n",
       "35       4.000000             22.417182           9.922850   \n",
       "36       4.666667             20.236434          11.184467   \n",
       "37       5.333333             20.325237          11.824150   \n",
       "38       4.333333             34.470779          11.217767   \n",
       "39       4.800000             34.724218          10.987740   \n",
       "40       4.432770             29.039063          10.765863   \n",
       "41       4.386238             24.667646          10.646815   \n",
       "42       4.747253             20.377006          11.033686   \n",
       "43       4.571429             20.098303          11.130929   \n",
       "44       4.833333             61.191020          11.496917   \n",
       "45       5.500000             78.841037          10.501513   \n",
       "46       4.875000             73.793144          11.252038   \n",
       "47       4.571429             28.850887          11.070300   \n",
       "48       4.748815             38.955617          11.038415   \n",
       "49       5.200000             65.565418          11.234080   \n",
       "50       4.800000             74.718706          11.199400   \n",
       "51       5.692308             65.207514          11.447692   \n",
       "\n",
       "    el_neg_chi_Average  R_vdw_element_Average  R_cov_element_Average  \\\n",
       "0             2.938889               1.711111               0.884444   \n",
       "1             1.903333               1.310000               0.680000   \n",
       "2             2.672609               1.379130               0.530870   \n",
       "3             2.960000               1.625000               0.813333   \n",
       "4             2.881818               1.640909               0.841818   \n",
       "5             2.700000               1.686667               0.780000   \n",
       "6             2.866000               1.695000               0.786000   \n",
       "7             2.540000               1.820000               0.972308   \n",
       "8             2.525787               1.813894               0.992739   \n",
       "9             2.512578               1.819602               1.001515   \n",
       "10            2.768333               1.745000               0.960000   \n",
       "11            2.580000               1.712000               0.956000   \n",
       "12            2.586250               1.808333               0.926667   \n",
       "13            2.536667               1.810556               0.986667   \n",
       "14            2.763500               1.684000               0.825000   \n",
       "15            3.443000               1.662000               0.836000   \n",
       "16            0.000000               0.000000               0.000000   \n",
       "17            2.635493               1.808829               1.035042   \n",
       "18            2.507578               1.796467               1.052137   \n",
       "19            2.730650               1.740925               0.887445   \n",
       "20            2.621111               1.781333               1.009333   \n",
       "21            2.771667               1.699167               0.834167   \n",
       "22            2.528224               1.824132               1.065629   \n",
       "23            2.620453               1.781604               1.009720   \n",
       "24            1.622865               2.209848               1.615840   \n",
       "25            2.780000               1.726562               0.818125   \n",
       "26            2.760769               1.731923               0.800769   \n",
       "27            2.790000               1.750000               0.898750   \n",
       "28            2.649500               1.720000               0.958000   \n",
       "29            2.620009               1.782048               1.011053   \n",
       "30            2.604286               1.848571               1.022857   \n",
       "31            2.612857               1.641429               0.920000   \n",
       "32            2.743846               1.734615               0.910769   \n",
       "33            0.000000               0.000000               0.000000   \n",
       "34            2.763617               1.710157               0.790261   \n",
       "35            2.381667               1.718333               0.953333   \n",
       "36            2.765833               1.734167               0.811667   \n",
       "37            2.926667               1.620000               0.815000   \n",
       "38            2.775000               1.710000               0.925000   \n",
       "39            2.828000               1.682000               0.836000   \n",
       "40            2.593148               1.801011               1.028796   \n",
       "41            2.539556               1.807314               0.980999   \n",
       "42            2.772867               1.688701               0.827053   \n",
       "43            2.611429               1.662857               0.928571   \n",
       "44            2.953333               1.711667               0.911667   \n",
       "45            2.527500               1.942500               1.237500   \n",
       "46            2.870000               1.726250               0.945000   \n",
       "47            2.680000               1.754286               0.897143   \n",
       "48            2.675498               1.793128               0.981706   \n",
       "49            2.826000               1.738000               0.914000   \n",
       "50            2.757000               1.742000               0.956000   \n",
       "51            2.845385               1.763077               0.987692   \n",
       "\n",
       "    zaratio_Average  density_Average  \n",
       "0          0.477830         2.656444  \n",
       "1          0.825990         0.580056  \n",
       "2          0.713850         0.370050  \n",
       "3          0.488163         1.351555  \n",
       "4          0.483480         1.811029  \n",
       "5          0.479962         0.850073  \n",
       "6          0.478464         1.686499  \n",
       "7          0.489274         0.913179  \n",
       "8          0.487604         1.229151  \n",
       "9          0.486888         1.274776  \n",
       "10         0.481708         1.764055  \n",
       "11         0.474714         1.821599  \n",
       "12         0.479152         1.006274  \n",
       "13         0.488869         1.123518  \n",
       "14         0.485063         1.371649  \n",
       "15         0.460005         2.281264  \n",
       "16         0.000000         0.000000  \n",
       "17         0.477068         1.698855  \n",
       "18         0.483923         1.751842  \n",
       "19         0.483071         1.228719  \n",
       "20         0.490911         1.360584  \n",
       "21         0.485270         2.164860  \n",
       "22         0.465199         2.957484  \n",
       "23         0.490878         1.363045  \n",
       "24         0.434283         5.627312  \n",
       "25         0.482644         1.174478  \n",
       "26         0.482639         0.993115  \n",
       "27         0.481827         1.009583  \n",
       "28         0.486265         1.345899  \n",
       "29         0.490174         1.439256  \n",
       "30         0.477579         1.060797  \n",
       "31         0.493919         1.020475  \n",
       "32         0.481472         2.965358  \n",
       "33         0.000000         0.000000  \n",
       "34         0.482938         1.864812  \n",
       "35         0.467532         1.065833  \n",
       "36         0.481969         1.002444  \n",
       "37         0.494362         0.817388  \n",
       "38         0.479689         2.187888  \n",
       "39         0.477854         1.936799  \n",
       "40         0.490143         1.380844  \n",
       "41         0.490048         0.995023  \n",
       "42         0.486636         1.108890  \n",
       "43         0.498003         0.830761  \n",
       "44         0.472267         3.595888  \n",
       "45         0.459375         2.947122  \n",
       "46         0.466171         4.115832  \n",
       "47         0.486954         2.459333  \n",
       "48         0.485396         1.621247  \n",
       "49         0.467304         3.335399  \n",
       "50         0.466061         4.723199  \n",
       "51         0.462479         2.953410  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Tratando os dados'''\n",
    "\n",
    "# Retirando colunas desinteressantes\n",
    "df_teste = df_teste.drop(['Unnamed: 0', 'Crystal structure', 'Formula'], axis=1)\n",
    "\n",
    "# Trocando o nome de uma coluna para facilitar o codigo\n",
    "df_teste.rename(columns={'Hardness (Mohs)': 'Hardness'}, inplace=True)\n",
    "\n",
    "# Retirando valores nan\n",
    "df_teste = df_teste.dropna()\n",
    "\n",
    "# Ajustando o index\n",
    "df_teste = df_teste.reset_index(drop=True)\n",
    "\n",
    "df_teste # Mostrando nova tabela"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentação dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa é uma etapa crucial para o treinamento e aplicação de rede neural treinada. \n",
    "\n",
    "Em primeiro lugar, foi definido as colunas das features e do target. Tendo sido retiradas 3 colunas dos dataframes devido a relevância nula das características representadas para a definição da dureza do material. Sendo elas:\n",
    "\n",
    "- 'allelectrons_Total'\n",
    "- 'R_vdw_element_Average'\n",
    "- 'zaratio_Average'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo Features\n",
    "FEATURES = [ 'density_Total',  'allelectrons_Average',\n",
    "            'val_e_Average',  'atomicweight_Average',  'ionenergy_Average',\n",
    "            'el_neg_chi_Average',  'R_cov_element_Average',\n",
    "             'density_Average']\n",
    "\n",
    "# Definindo Targets\n",
    "TARGET = 'Hardness'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, foi feita a definição das features e targets para treino e teste(famoso split de dados). Como já foi mencionado diversas vezes, existem dois datasets separados, um para treino e outro para teste. Logo, só foi feita a definição de listas contendo as features e target para treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Separando Features e Targets'''\n",
    "\n",
    "\n",
    "# Separando dados para o treino\n",
    "X_train = df_treino[FEATURES]\n",
    "Y_train = df_treino[TARGET]\n",
    "\n",
    "# Separando dados para teste\n",
    "X_test = df_teste[FEATURES]\n",
    "Y_test = df_teste[TARGET]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vale ressaltar que foi de suma importância a normalização dos dados para o uso das features sem erros, tendo sido utilizada a MinMaxScaler do módulo sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Normalizando os dados'''\n",
    "\n",
    "# Criando um objeto MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Ajuste o scaler aos dados e transforme-os\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o uso dos valores das do treino e teste pela rede neural, é necessário a transformação das listas de cada um em tensores pelo próprio PyTorch. Também, foi realizado o ajuste das dimensões do teste, já que as linhas e colunas estavam invertidas em relação ao treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Transformando em tensores'''\n",
    "\n",
    "# Tensores do treino\n",
    "X_train_scaled_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "\n",
    "# Tensores do teste\n",
    "X_test_scaled_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
    "\n",
    "# Ajustando dimensões dos targets\n",
    "Y_train_tensor_dimensionado = Y_train_tensor.view(-1,1)\n",
    "Y_test_tensor_dimensionado = Y_test_tensor.view(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Montando a Rede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente a montagem da rede neural por meio do PyTorch. Não vamos entrar em detalhes no funcionamento de uma rede neural, para isso recomendamos a leitura das referências utilizadas para o projeto, porém vale ressaltar as características específicas da rede em questão.\n",
    "\n",
    "Foi utilizado o método de dropout e momento com o intuito da melhora da performace da rede neural, além disso foi utilizado SGD para otimizar os parâmetros da rede considerando o momento. Por fim, foi definido o MSE como função de perda e foi definido que a cada 100 épocas é mostrado a perda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Redes Neural'''\n",
    "\n",
    "class NeuralNetworkRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, learning_rate=0.1, dropout_prob=0.3, momentum=0.8):\n",
    "        super(NeuralNetworkRegressor, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size1 = hidden_size1\n",
    "        self.hidden_size2 = hidden_size2\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Primeira camada oculta\n",
    "        self.hidden_layer1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.hidden_activation1 = nn.ReLU()\n",
    "        \n",
    "        # Segunda camada oculta\n",
    "        self.hidden_layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.hidden_activation2 = nn.ReLU()\n",
    "        \n",
    "        # Camada de saída\n",
    "        self.output_layer = nn.Linear(hidden_size2, 1)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "        # Otimizador com Momento\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        \n",
    "        # Função de perda (Mean Squared Error)\n",
    "        self.criterion = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Primeira camada oculta\n",
    "        hidden_output1 = self.hidden_layer1(x)\n",
    "        hidden_output1 = self.hidden_activation1(hidden_output1)\n",
    "        \n",
    "        # Dropout (apenas durante o treinamento)\n",
    "        if self.training:\n",
    "            hidden_output1 = self.dropout(hidden_output1)\n",
    "        \n",
    "        # Segunda camada oculta\n",
    "        hidden_output2 = self.hidden_layer2(hidden_output1)\n",
    "        hidden_output2 = self.hidden_activation2(hidden_output2)\n",
    "        \n",
    "        # Dropout (apenas durante o treinamento)\n",
    "        if self.training:\n",
    "            hidden_output2 = self.dropout(hidden_output2)\n",
    "        \n",
    "        # Camada de saída\n",
    "        output = self.output_layer(hidden_output2)\n",
    "        return output\n",
    "    \n",
    "    def train_model(self, X_train, y_train, epochs, print_perda=False):\n",
    "        for epoch in range(epochs):\n",
    "            # Ativa o modo de treinamento\n",
    "            self.train()\n",
    "            \n",
    "            # Realiza a propagação direta\n",
    "            outputs = self(X_train)\n",
    "            \n",
    "            # Calcula a perda\n",
    "            loss = self.criterion(outputs, y_train)\n",
    "            \n",
    "            # Retropropagação e atualização dos pesos\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            if print_perda:\n",
    "                # Mostra a perda a cada 100 épocas\n",
    "                if (epoch + 1) % 100 == 0:\n",
    "                    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}')\n",
    "            elif type(print_perda) != bool:\n",
    "                print(\"Variavel 'print_perda' com valor fora dos parâmetros\")\n",
    "            \n",
    "                \n",
    "    def predict(self, X, num_samples=10, ):\n",
    "        \"\"\"\n",
    "        Realiza a previsão com base nos dados de entrada fornecidos usando Monte Carlo Dropout.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): Os dados de entrada para prever.\n",
    "            num_samples (int): O número de amostras de dropout a serem usadas (default 10).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A média das previsões feitas com diferentes amostras de dropout.\n",
    "        \"\"\"\n",
    "        # Ativa o modo de avaliação\n",
    "        self.eval()\n",
    "        \n",
    "        # Lista para armazenar as previsões\n",
    "        self.predictions = []\n",
    "        \n",
    "        # Realiza múltiplas previsões com diferentes amostras de dropout\n",
    "        for _ in range(num_samples):\n",
    "            with torch.no_grad():\n",
    "                outputs = self(X)\n",
    "                self.predictions.append(outputs)\n",
    "        \n",
    "        # Calcula a média das previsões\n",
    "        predictions_mean = torch.stack(self.predictions).mean(dim=0)\n",
    "        return predictions_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando a rede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, realizando o treinamento da rede. Para isso, foi utilizado o optuna com o intuito de tetar diversas arquiteturas de rede e selecionar a melhor para aplicação no teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 07:13:11,314] A new study created in memory with name: no-name-9d06565a-7396-4735-996d-ba464f099b44\n",
      "  0%|          | 0/100 [00:00<?, ?it/s][I 2024-05-07 07:13:18,228] Trial 0 finished with value: 1.4377954006195068 and parameters: {'hidden_size1': 42, 'hidden_size2': 152, 'learning_rate': 0.004051740742084953, 'dropout_prob': 0.34964443687291924, 'momentum': 0.9161700561928035}. Best is trial 0 with value: 1.4377954006195068.\n",
      "  1%|          | 1/100 [00:06<11:22,  6.89s/it][I 2024-05-07 07:13:23,849] Trial 1 finished with value: 2.0990991592407227 and parameters: {'hidden_size1': 16, 'hidden_size2': 33, 'learning_rate': 0.0016893847897238717, 'dropout_prob': 0.22316471896688972, 'momentum': 0.8367786657668997}. Best is trial 0 with value: 1.4377954006195068.\n",
      "  2%|▏         | 2/100 [00:12<10:02,  6.14s/it][I 2024-05-07 07:13:36,071] Trial 2 finished with value: 1.7321953773498535 and parameters: {'hidden_size1': 240, 'hidden_size2': 120, 'learning_rate': 0.02868724182683144, 'dropout_prob': 0.38888554183828034, 'momentum': 0.8493810520895421}. Best is trial 0 with value: 1.4377954006195068.\n",
      "  3%|▎         | 3/100 [00:24<14:25,  8.92s/it][I 2024-05-07 07:13:43,778] Trial 3 finished with value: 3.3807294368743896 and parameters: {'hidden_size1': 138, 'hidden_size2': 42, 'learning_rate': 0.0001216789636679657, 'dropout_prob': 0.3313360274342215, 'momentum': 0.45369771226975664}. Best is trial 0 with value: 1.4377954006195068.\n",
      "  4%|▍         | 4/100 [00:32<13:30,  8.44s/it][I 2024-05-07 07:13:50,125] Trial 4 finished with value: 3.04083514213562 and parameters: {'hidden_size1': 23, 'hidden_size2': 73, 'learning_rate': 0.00027124532001621707, 'dropout_prob': 0.07688892519299034, 'momentum': 0.20168308356957854}. Best is trial 0 with value: 1.4377954006195068.\n",
      "  5%|▌         | 5/100 [00:38<12:10,  7.69s/it][I 2024-05-07 07:13:59,151] Trial 5 finished with value: 3.684504270553589 and parameters: {'hidden_size1': 62, 'hidden_size2': 113, 'learning_rate': 7.943772886440494e-05, 'dropout_prob': 0.4002373492077528, 'momentum': 0.2906938600057959}. Best is trial 0 with value: 1.4377954006195068.\n",
      "  6%|▌         | 6/100 [00:47<12:45,  8.14s/it][I 2024-05-07 07:14:05,342] Trial 6 finished with value: 2.4946913719177246 and parameters: {'hidden_size1': 23, 'hidden_size2': 47, 'learning_rate': 0.0018305157205877822, 'dropout_prob': 0.37988625769891854, 'momentum': 0.49710129025252914}. Best is trial 0 with value: 1.4377954006195068.\n",
      "  7%|▋         | 7/100 [00:54<11:37,  7.50s/it][I 2024-05-07 07:14:13,228] Trial 7 finished with value: 4.031614303588867 and parameters: {'hidden_size1': 105, 'hidden_size2': 39, 'learning_rate': 2.36773926630212e-05, 'dropout_prob': 0.41866141242883415, 'momentum': 0.7931678504557594}. Best is trial 0 with value: 1.4377954006195068.\n",
      "  8%|▊         | 8/100 [01:01<11:41,  7.62s/it][I 2024-05-07 07:14:20,434] Trial 8 finished with value: 1.3972913026809692 and parameters: {'hidden_size1': 146, 'hidden_size2': 20, 'learning_rate': 0.02109762378106558, 'dropout_prob': 0.07435248772516823, 'momentum': 0.4518808176206154}. Best is trial 8 with value: 1.3972913026809692.\n",
      "  9%|▉         | 9/100 [01:09<11:21,  7.49s/it][I 2024-05-07 07:14:29,691] Trial 9 finished with value: 1.798432469367981 and parameters: {'hidden_size1': 206, 'hidden_size2': 71, 'learning_rate': 0.0019225237506496162, 'dropout_prob': 0.10889459962227704, 'momentum': 0.691326704421665}. Best is trial 8 with value: 1.3972913026809692.\n",
      " 10%|█         | 10/100 [01:18<12:03,  8.04s/it][I 2024-05-07 07:14:36,029] Trial 10 finished with value: 2.04105544090271 and parameters: {'hidden_size1': 114, 'hidden_size2': 17, 'learning_rate': 0.08330066711304054, 'dropout_prob': 0.011977689970281542, 'momentum': 0.06849922940194536}. Best is trial 8 with value: 1.3972913026809692.\n",
      " 11%|█         | 11/100 [01:24<11:09,  7.52s/it][I 2024-05-07 07:14:44,682] Trial 11 finished with value: 1.5210843086242676 and parameters: {'hidden_size1': 55, 'hidden_size2': 242, 'learning_rate': 0.011528578856111603, 'dropout_prob': 0.24654354385569804, 'momentum': 0.9952570064781512}. Best is trial 8 with value: 1.3972913026809692.\n",
      " 12%|█▏        | 12/100 [01:33<11:31,  7.86s/it][I 2024-05-07 07:14:51,038] Trial 12 finished with value: 1.8085808753967285 and parameters: {'hidden_size1': 48, 'hidden_size2': 21, 'learning_rate': 0.007327706927508374, 'dropout_prob': 0.4967714160116993, 'momentum': 0.6195855323368842}. Best is trial 8 with value: 1.3972913026809692.\n",
      " 13%|█▎        | 13/100 [01:39<10:44,  7.41s/it][I 2024-05-07 07:14:58,331] Trial 13 finished with value: 1.5971964597702026 and parameters: {'hidden_size1': 40, 'hidden_size2': 24, 'learning_rate': 0.008629007087372173, 'dropout_prob': 0.1719106748186468, 'momentum': 0.3565410968156679}. Best is trial 8 with value: 1.3972913026809692.\n",
      " 14%|█▍        | 14/100 [01:46<10:34,  7.37s/it][I 2024-05-07 07:15:03,904] Trial 14 finished with value: 2.37872052192688 and parameters: {'hidden_size1': 85, 'hidden_size2': 16, 'learning_rate': 0.08840076335950026, 'dropout_prob': 0.3036326951796171, 'momentum': 0.596054910398197}. Best is trial 8 with value: 1.3972913026809692.\n",
      " 15%|█▌        | 15/100 [01:52<09:40,  6.83s/it][I 2024-05-07 07:15:10,525] Trial 15 finished with value: 1.8190405368804932 and parameters: {'hidden_size1': 167, 'hidden_size2': 29, 'learning_rate': 0.004957093328137426, 'dropout_prob': 0.18810224926023267, 'momentum': 0.9873419958887432}. Best is trial 8 with value: 1.3972913026809692.\n",
      " 16%|█▌        | 16/100 [01:59<09:28,  6.77s/it][I 2024-05-07 07:15:16,644] Trial 16 finished with value: 2.930262327194214 and parameters: {'hidden_size1': 81, 'hidden_size2': 24, 'learning_rate': 0.0005709141734482001, 'dropout_prob': 0.2835317953570006, 'momentum': 0.45688776148193383}. Best is trial 8 with value: 1.3972913026809692.\n",
      " 17%|█▋        | 17/100 [02:05<09:05,  6.57s/it][I 2024-05-07 07:15:24,860] Trial 17 finished with value: 1.7321393489837646 and parameters: {'hidden_size1': 165, 'hidden_size2': 57, 'learning_rate': 0.023765505179371895, 'dropout_prob': 0.004840934556621879, 'momentum': 0.5939935200506347}. Best is trial 8 with value: 1.3972913026809692.\n",
      " 18%|█▊        | 18/100 [02:13<09:39,  7.07s/it][I 2024-05-07 07:15:30,900] Trial 18 finished with value: 1.601040005683899 and parameters: {'hidden_size1': 76, 'hidden_size2': 30, 'learning_rate': 0.0032549817365171477, 'dropout_prob': 0.13369656307042663, 'momentum': 0.71682008828512}. Best is trial 8 with value: 1.3972913026809692.\n",
      " 19%|█▉        | 19/100 [02:19<09:07,  6.76s/it][I 2024-05-07 07:15:40,964] Trial 19 finished with value: 1.4628913402557373 and parameters: {'hidden_size1': 41, 'hidden_size2': 219, 'learning_rate': 0.018573773493310693, 'dropout_prob': 0.20973859129467648, 'momentum': 0.37678849211075705}. Best is trial 8 with value: 1.3972913026809692.\n",
      " 20%|██        | 20/100 [02:29<10:20,  7.75s/it][I 2024-05-07 07:15:52,364] Trial 20 finished with value: 2.6978800296783447 and parameters: {'hidden_size1': 234, 'hidden_size2': 96, 'learning_rate': 0.0005631235114128168, 'dropout_prob': 0.260804787322911, 'momentum': 0.5569335922347822}. Best is trial 8 with value: 1.3972913026809692.\n",
      " 21%|██        | 21/100 [02:41<11:38,  8.85s/it][I 2024-05-07 07:16:03,301] Trial 21 finished with value: 1.5754119157791138 and parameters: {'hidden_size1': 34, 'hidden_size2': 213, 'learning_rate': 0.021127689892607233, 'dropout_prob': 0.22284032071439966, 'momentum': 0.3895506663996744}. Best is trial 8 with value: 1.3972913026809692.\n",
      " 22%|██▏       | 22/100 [02:51<12:18,  9.47s/it][I 2024-05-07 07:16:11,424] Trial 22 finished with value: 1.9693584442138672 and parameters: {'hidden_size1': 45, 'hidden_size2': 184, 'learning_rate': 0.005212118379553304, 'dropout_prob': 0.06588986803791691, 'momentum': 0.29879100595049923}. Best is trial 8 with value: 1.3972913026809692.\n",
      " 23%|██▎       | 23/100 [03:00<11:38,  9.07s/it][I 2024-05-07 07:16:20,265] Trial 23 finished with value: 2.187014102935791 and parameters: {'hidden_size1': 62, 'hidden_size2': 166, 'learning_rate': 0.016257701705329666, 'dropout_prob': 0.16389622553334127, 'momentum': 0.5193532307587841}. Best is trial 8 with value: 1.3972913026809692.\n",
      " 24%|██▍       | 24/100 [03:08<11:23,  9.00s/it][I 2024-05-07 07:16:29,101] Trial 24 finished with value: 2.049989700317383 and parameters: {'hidden_size1': 35, 'hidden_size2': 256, 'learning_rate': 0.032445266626142236, 'dropout_prob': 0.3235000094860425, 'momentum': 0.42747222512556776}. Best is trial 8 with value: 1.3972913026809692.\n",
      " 25%|██▌       | 25/100 [03:17<11:11,  8.95s/it][I 2024-05-07 07:16:36,773] Trial 25 finished with value: 1.436431646347046 and parameters: {'hidden_size1': 53, 'hidden_size2': 140, 'learning_rate': 0.045037646295525605, 'dropout_prob': 0.20050659581574914, 'momentum': 0.369682914901007}. Best is trial 8 with value: 1.3972913026809692.\n",
      " 26%|██▌       | 26/100 [03:25<10:33,  8.57s/it][I 2024-05-07 07:16:45,375] Trial 26 finished with value: 1.711881399154663 and parameters: {'hidden_size1': 72, 'hidden_size2': 149, 'learning_rate': 0.05358951505260478, 'dropout_prob': 0.1477738828727119, 'momentum': 0.5139850279562838}. Best is trial 8 with value: 1.3972913026809692.\n",
      " 27%|██▋       | 27/100 [03:34<10:26,  8.58s/it][I 2024-05-07 07:16:56,274] Trial 27 finished with value: 2.6172053813934326 and parameters: {'hidden_size1': 101, 'hidden_size2': 139, 'learning_rate': 0.038808313077863615, 'dropout_prob': 0.12165644880600279, 'momentum': 0.2060042257794188}. Best is trial 8 with value: 1.3972913026809692.\n",
      " 28%|██▊       | 28/100 [03:44<11:07,  9.27s/it][I 2024-05-07 07:17:03,589] Trial 28 finished with value: 1.572830080986023 and parameters: {'hidden_size1': 56, 'hidden_size2': 86, 'learning_rate': 0.010736263944470444, 'dropout_prob': 0.19449196489934945, 'momentum': 0.6719393390789011}. Best is trial 8 with value: 1.3972913026809692.\n",
      " 29%|██▉       | 29/100 [03:52<10:16,  8.69s/it][I 2024-05-07 07:17:10,995] Trial 29 finished with value: 1.9480031728744507 and parameters: {'hidden_size1': 89, 'hidden_size2': 57, 'learning_rate': 0.05310594748008773, 'dropout_prob': 0.20028499930481408, 'momentum': 0.5289937343580194}. Best is trial 8 with value: 1.3972913026809692.\n",
      " 30%|███       | 30/100 [03:59<09:41,  8.30s/it][I 2024-05-07 07:17:18,723] Trial 30 finished with value: 1.3766496181488037 and parameters: {'hidden_size1': 69, 'hidden_size2': 102, 'learning_rate': 0.0028142279811142735, 'dropout_prob': 0.2443999152433482, 'momentum': 0.9032252386475496}. Best is trial 30 with value: 1.3766496181488037.\n",
      " 31%|███       | 31/100 [04:07<09:20,  8.13s/it][I 2024-05-07 07:17:26,596] Trial 31 finished with value: 1.4150805473327637 and parameters: {'hidden_size1': 69, 'hidden_size2': 111, 'learning_rate': 0.0032819142618659503, 'dropout_prob': 0.2625488806154405, 'momentum': 0.9117034384000244}. Best is trial 30 with value: 1.3766496181488037.\n",
      " 32%|███▏      | 32/100 [04:15<09:07,  8.05s/it][I 2024-05-07 07:17:34,099] Trial 32 finished with value: 1.3910224437713623 and parameters: {'hidden_size1': 69, 'hidden_size2': 104, 'learning_rate': 0.0029536721565668034, 'dropout_prob': 0.251105724002493, 'momentum': 0.9207808298729059}. Best is trial 30 with value: 1.3766496181488037.\n",
      " 33%|███▎      | 33/100 [04:22<08:48,  7.89s/it][I 2024-05-07 07:17:41,710] Trial 33 finished with value: 1.3978787660598755 and parameters: {'hidden_size1': 68, 'hidden_size2': 102, 'learning_rate': 0.0034912124441458486, 'dropout_prob': 0.24903324194508206, 'momentum': 0.8908984805339712}. Best is trial 30 with value: 1.3766496181488037.\n",
      " 34%|███▍      | 34/100 [04:30<08:35,  7.81s/it][I 2024-05-07 07:17:50,572] Trial 34 finished with value: 1.6647833585739136 and parameters: {'hidden_size1': 120, 'hidden_size2': 91, 'learning_rate': 0.001158438754137274, 'dropout_prob': 0.24899098946712936, 'momentum': 0.8996365449045247}. Best is trial 30 with value: 1.3766496181488037.\n",
      " 35%|███▌      | 35/100 [04:39<08:47,  8.12s/it][I 2024-05-07 07:18:01,380] Trial 35 finished with value: 1.6267470121383667 and parameters: {'hidden_size1': 97, 'hidden_size2': 72, 'learning_rate': 0.0035141815227327368, 'dropout_prob': 0.23099393197704607, 'momentum': 0.8004312616632151}. Best is trial 30 with value: 1.3766496181488037.\n",
      " 36%|███▌      | 36/100 [04:50<09:31,  8.93s/it][I 2024-05-07 07:18:11,841] Trial 36 finished with value: 1.4039944410324097 and parameters: {'hidden_size1': 138, 'hidden_size2': 125, 'learning_rate': 0.006378721445708243, 'dropout_prob': 0.28421548200269103, 'momentum': 0.8684059024820467}. Best is trial 30 with value: 1.3766496181488037.\n",
      " 37%|███▋      | 37/100 [05:00<09:51,  9.39s/it][I 2024-05-07 07:18:19,537] Trial 37 finished with value: 1.747262954711914 and parameters: {'hidden_size1': 69, 'hidden_size2': 104, 'learning_rate': 0.001987801826080642, 'dropout_prob': 0.16548804572976067, 'momentum': 0.8183267743103446}. Best is trial 30 with value: 1.3766496181488037.\n",
      " 38%|███▊      | 38/100 [05:08<09:10,  8.88s/it][I 2024-05-07 07:18:27,095] Trial 38 finished with value: 1.5851598978042603 and parameters: {'hidden_size1': 81, 'hidden_size2': 90, 'learning_rate': 0.011175780141548044, 'dropout_prob': 0.3464951075072291, 'momentum': 0.9474827488371269}. Best is trial 30 with value: 1.3766496181488037.\n",
      " 39%|███▉      | 39/100 [05:15<08:37,  8.48s/it][I 2024-05-07 07:18:34,715] Trial 39 finished with value: 1.420175313949585 and parameters: {'hidden_size1': 92, 'hidden_size2': 78, 'learning_rate': 0.0025428271161263795, 'dropout_prob': 0.23294745184988583, 'momentum': 0.8596799469218943}. Best is trial 30 with value: 1.3766496181488037.\n",
      " 40%|████      | 40/100 [05:23<08:13,  8.22s/it][I 2024-05-07 07:18:42,265] Trial 40 finished with value: 2.027524948120117 and parameters: {'hidden_size1': 62, 'hidden_size2': 114, 'learning_rate': 0.0013526621301109603, 'dropout_prob': 0.3104577595281238, 'momentum': 0.7900317110253989}. Best is trial 30 with value: 1.3766496181488037.\n",
      " 41%|████      | 41/100 [05:30<07:53,  8.02s/it][I 2024-05-07 07:18:53,479] Trial 41 finished with value: 1.373124599456787 and parameters: {'hidden_size1': 131, 'hidden_size2': 120, 'learning_rate': 0.005695476346276257, 'dropout_prob': 0.2747784822217712, 'momentum': 0.8816649383530023}. Best is trial 41 with value: 1.373124599456787.\n",
      " 42%|████▏     | 42/100 [05:42<08:40,  8.98s/it][I 2024-05-07 07:19:04,744] Trial 42 finished with value: 1.5124313831329346 and parameters: {'hidden_size1': 124, 'hidden_size2': 125, 'learning_rate': 0.005690798586256094, 'dropout_prob': 0.22702688872289173, 'momentum': 0.9263546234031017}. Best is trial 41 with value: 1.373124599456787.\n",
      " 43%|████▎     | 43/100 [05:53<09:10,  9.67s/it][I 2024-05-07 07:19:13,699] Trial 43 finished with value: 1.6847476959228516 and parameters: {'hidden_size1': 141, 'hidden_size2': 65, 'learning_rate': 0.004269136872720714, 'dropout_prob': 0.279931330564208, 'momentum': 0.9602711203393052}. Best is trial 41 with value: 1.373124599456787.\n",
      " 44%|████▍     | 44/100 [06:02<08:49,  9.45s/it][I 2024-05-07 07:19:25,451] Trial 44 finished with value: 1.5288007259368896 and parameters: {'hidden_size1': 107, 'hidden_size2': 83, 'learning_rate': 0.0022963100242110166, 'dropout_prob': 0.34927842408090815, 'momentum': 0.8850628963885132}. Best is trial 41 with value: 1.373124599456787.\n",
      " 45%|████▌     | 45/100 [06:14<09:17, 10.14s/it][I 2024-05-07 07:19:35,000] Trial 45 finished with value: 1.8607172966003418 and parameters: {'hidden_size1': 94, 'hidden_size2': 108, 'learning_rate': 0.008862832754120134, 'dropout_prob': 0.30075910426170815, 'momentum': 0.9903249098109734}. Best is trial 41 with value: 1.373124599456787.\n",
      " 46%|████▌     | 46/100 [06:23<08:58,  9.96s/it][I 2024-05-07 07:19:43,200] Trial 46 finished with value: 1.5115907192230225 and parameters: {'hidden_size1': 80, 'hidden_size2': 98, 'learning_rate': 0.013988754341603456, 'dropout_prob': 0.2563201949055242, 'momentum': 0.7476252936737184}. Best is trial 41 with value: 1.373124599456787.\n",
      " 47%|████▋     | 47/100 [06:31<08:20,  9.43s/it][I 2024-05-07 07:19:53,814] Trial 47 finished with value: 1.5924835205078125 and parameters: {'hidden_size1': 188, 'hidden_size2': 40, 'learning_rate': 0.0008473835292224453, 'dropout_prob': 0.0964611709005447, 'momentum': 0.9384294948110352}. Best is trial 41 with value: 1.373124599456787.\n",
      " 48%|████▊     | 48/100 [06:42<08:29,  9.79s/it][I 2024-05-07 07:20:03,859] Trial 48 finished with value: 1.3252571821212769 and parameters: {'hidden_size1': 112, 'hidden_size2': 48, 'learning_rate': 0.006848125051855567, 'dropout_prob': 0.38122164417681814, 'momentum': 0.8550332072899295}. Best is trial 48 with value: 1.3252571821212769.\n",
      " 49%|████▉     | 49/100 [06:52<08:23,  9.87s/it][I 2024-05-07 07:20:14,508] Trial 49 finished with value: 1.5027625560760498 and parameters: {'hidden_size1': 121, 'hidden_size2': 35, 'learning_rate': 0.008125555272029834, 'dropout_prob': 0.3286983797348261, 'momentum': 0.8208864396219434}. Best is trial 48 with value: 1.3252571821212769.\n",
      " 50%|█████     | 50/100 [07:03<08:25, 10.10s/it][I 2024-05-07 07:20:27,186] Trial 50 finished with value: 1.496759295463562 and parameters: {'hidden_size1': 152, 'hidden_size2': 56, 'learning_rate': 0.014062784823785167, 'dropout_prob': 0.3715145079164206, 'momentum': 0.8481792208702009}. Best is trial 48 with value: 1.3252571821212769.\n",
      " 51%|█████     | 51/100 [07:15<08:52, 10.87s/it][I 2024-05-07 07:20:39,114] Trial 51 finished with value: 1.3881434202194214 and parameters: {'hidden_size1': 108, 'hidden_size2': 46, 'learning_rate': 0.004274308524404924, 'dropout_prob': 0.41915399932019015, 'momentum': 0.8904825153052549}. Best is trial 48 with value: 1.3252571821212769.\n",
      " 52%|█████▏    | 52/100 [07:27<08:57, 11.19s/it][I 2024-05-07 07:20:54,162] Trial 52 finished with value: 1.2797335386276245 and parameters: {'hidden_size1': 113, 'hidden_size2': 45, 'learning_rate': 0.007388950581919834, 'dropout_prob': 0.42173333561765664, 'momentum': 0.7633867950760307}. Best is trial 52 with value: 1.2797335386276245.\n",
      " 53%|█████▎    | 53/100 [07:42<09:40, 12.35s/it][I 2024-05-07 07:21:05,883] Trial 53 finished with value: 1.490374207496643 and parameters: {'hidden_size1': 114, 'hidden_size2': 44, 'learning_rate': 0.006055389149854303, 'dropout_prob': 0.428285829400126, 'momentum': 0.7660174801205298}. Best is trial 52 with value: 1.2797335386276245.\n",
      " 54%|█████▍    | 54/100 [07:54<09:19, 12.16s/it][I 2024-05-07 07:21:13,230] Trial 54 finished with value: 1.5286939144134521 and parameters: {'hidden_size1': 128, 'hidden_size2': 48, 'learning_rate': 0.002658941892802533, 'dropout_prob': 0.4281159543815971, 'momentum': 0.8513376055659131}. Best is trial 52 with value: 1.2797335386276245.\n",
      " 55%|█████▌    | 55/100 [08:01<08:02, 10.72s/it][I 2024-05-07 07:21:20,845] Trial 55 finished with value: 1.4419188499450684 and parameters: {'hidden_size1': 100, 'hidden_size2': 48, 'learning_rate': 0.004370666095952346, 'dropout_prob': 0.4129704317173947, 'momentum': 0.9122944491225438}. Best is trial 52 with value: 1.2797335386276245.\n",
      " 56%|█████▌    | 56/100 [08:09<07:10,  9.79s/it][I 2024-05-07 07:21:28,286] Trial 56 finished with value: 1.4542275667190552 and parameters: {'hidden_size1': 110, 'hidden_size2': 61, 'learning_rate': 0.0016067795548222207, 'dropout_prob': 0.4474543807240709, 'momentum': 0.9694310449447027}. Best is trial 52 with value: 1.2797335386276245.\n",
      " 57%|█████▋    | 57/100 [08:16<06:30,  9.08s/it][I 2024-05-07 07:21:35,827] Trial 57 finished with value: 1.3262505531311035 and parameters: {'hidden_size1': 92, 'hidden_size2': 52, 'learning_rate': 0.007491527801877715, 'dropout_prob': 0.381317623587057, 'momentum': 0.8301457163747804}. Best is trial 52 with value: 1.2797335386276245.\n",
      " 58%|█████▊    | 58/100 [08:24<06:02,  8.62s/it][I 2024-05-07 07:21:43,925] Trial 58 finished with value: 1.323723316192627 and parameters: {'hidden_size1': 110, 'hidden_size2': 51, 'learning_rate': 0.007469687954325315, 'dropout_prob': 0.37816080623434556, 'momentum': 0.8320294247456181}. Best is trial 52 with value: 1.2797335386276245.\n",
      " 59%|█████▉    | 59/100 [08:32<05:46,  8.46s/it][I 2024-05-07 07:21:53,864] Trial 59 finished with value: 1.3930566310882568 and parameters: {'hidden_size1': 87, 'hidden_size2': 39, 'learning_rate': 0.006183150576479444, 'dropout_prob': 0.3761410083261302, 'momentum': 0.829680444783734}. Best is trial 52 with value: 1.2797335386276245.\n",
      " 60%|██████    | 60/100 [08:42<05:56,  8.91s/it][I 2024-05-07 07:22:04,424] Trial 60 finished with value: 1.2900198698043823 and parameters: {'hidden_size1': 128, 'hidden_size2': 68, 'learning_rate': 0.008549301382765571, 'dropout_prob': 0.3755063062019925, 'momentum': 0.7800261206497616}. Best is trial 52 with value: 1.2797335386276245.\n",
      " 61%|██████    | 61/100 [08:53<06:06,  9.40s/it][I 2024-05-07 07:22:13,214] Trial 61 finished with value: 1.1989573240280151 and parameters: {'hidden_size1': 155, 'hidden_size2': 52, 'learning_rate': 0.00927671882950898, 'dropout_prob': 0.3871969246216686, 'momentum': 0.7765929041786017}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 62%|██████▏   | 62/100 [09:01<05:50,  9.22s/it][I 2024-05-07 07:22:21,210] Trial 62 finished with value: 1.257379412651062 and parameters: {'hidden_size1': 129, 'hidden_size2': 52, 'learning_rate': 0.00913149694255356, 'dropout_prob': 0.38362662242934825, 'momentum': 0.7633558454524401}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 63%|██████▎   | 63/100 [09:09<05:27,  8.85s/it][I 2024-05-07 07:22:29,676] Trial 63 finished with value: 1.552097201347351 and parameters: {'hidden_size1': 155, 'hidden_size2': 51, 'learning_rate': 0.02549818026975274, 'dropout_prob': 0.3912626906422022, 'momentum': 0.7408925831737784}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 64%|██████▍   | 64/100 [09:18<05:14,  8.74s/it][I 2024-05-07 07:22:37,590] Trial 64 finished with value: 1.4021424055099487 and parameters: {'hidden_size1': 135, 'hidden_size2': 53, 'learning_rate': 0.008977287868041839, 'dropout_prob': 0.36399262780745034, 'momentum': 0.7018905388684497}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 65%|██████▌   | 65/100 [09:26<04:57,  8.49s/it][I 2024-05-07 07:22:46,149] Trial 65 finished with value: 1.559937596321106 and parameters: {'hidden_size1': 166, 'hidden_size2': 65, 'learning_rate': 0.020453879121106744, 'dropout_prob': 0.39079473915087104, 'momentum': 0.7785962579502581}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 66%|██████▌   | 66/100 [09:34<04:49,  8.51s/it][I 2024-05-07 07:22:54,987] Trial 66 finished with value: 1.577156662940979 and parameters: {'hidden_size1': 117, 'hidden_size2': 43, 'learning_rate': 0.012958575672422422, 'dropout_prob': 0.3588356195088456, 'momentum': 0.6671502307302182}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 67%|██████▋   | 67/100 [09:43<04:44,  8.61s/it][I 2024-05-07 07:23:05,548] Trial 67 finished with value: 1.3444453477859497 and parameters: {'hidden_size1': 147, 'hidden_size2': 52, 'learning_rate': 0.009666532223221329, 'dropout_prob': 0.4032706440868053, 'momentum': 0.7425314150580328}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 68%|██████▊   | 68/100 [09:54<04:54,  9.19s/it][I 2024-05-07 07:23:15,185] Trial 68 finished with value: 2.1637184619903564 and parameters: {'hidden_size1': 103, 'hidden_size2': 61, 'learning_rate': 0.0173323952092742, 'dropout_prob': 0.3759384111600783, 'momentum': 0.8064490661943176}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 69%|██████▉   | 69/100 [10:03<04:49,  9.33s/it][I 2024-05-07 07:23:22,688] Trial 69 finished with value: 1.3699027299880981 and parameters: {'hidden_size1': 126, 'hidden_size2': 68, 'learning_rate': 0.007313578967040688, 'dropout_prob': 0.34453026584496343, 'momentum': 0.8368431863033187}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 70%|███████   | 70/100 [10:11<04:23,  8.78s/it][I 2024-05-07 07:23:31,026] Trial 70 finished with value: 1.824698567390442 and parameters: {'hidden_size1': 186, 'hidden_size2': 59, 'learning_rate': 0.027351110613449477, 'dropout_prob': 0.3861373308843704, 'momentum': 0.7667150782638131}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 71%|███████   | 71/100 [10:19<04:10,  8.65s/it][I 2024-05-07 07:23:39,504] Trial 71 finished with value: 1.5860931873321533 and parameters: {'hidden_size1': 141, 'hidden_size2': 52, 'learning_rate': 0.010467751781871098, 'dropout_prob': 0.40016950036638643, 'momentum': 0.7336952789161771}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 72%|███████▏  | 72/100 [10:28<04:00,  8.60s/it][I 2024-05-07 07:23:47,721] Trial 72 finished with value: 1.4693758487701416 and parameters: {'hidden_size1': 153, 'hidden_size2': 53, 'learning_rate': 0.016549988320154207, 'dropout_prob': 0.40476003394460425, 'momentum': 0.7209952280702543}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 73%|███████▎  | 73/100 [10:36<03:49,  8.48s/it][I 2024-05-07 07:23:56,460] Trial 73 finished with value: 1.5924263000488281 and parameters: {'hidden_size1': 116, 'hidden_size2': 49, 'learning_rate': 0.009255822854712735, 'dropout_prob': 0.4497813296626135, 'momentum': 0.7864507228537976}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 74%|███████▍  | 74/100 [10:45<03:42,  8.56s/it][I 2024-05-07 07:24:04,152] Trial 74 finished with value: 1.4295941591262817 and parameters: {'hidden_size1': 130, 'hidden_size2': 45, 'learning_rate': 0.013083856545981118, 'dropout_prob': 0.3634421407521613, 'momentum': 0.6836475314958236}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 75%|███████▌  | 75/100 [10:52<03:27,  8.30s/it][I 2024-05-07 07:24:12,241] Trial 75 finished with value: 1.4602735042572021 and parameters: {'hidden_size1': 146, 'hidden_size2': 58, 'learning_rate': 0.006923354980603455, 'dropout_prob': 0.3377415544329559, 'momentum': 0.8102345558487631}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 76%|███████▌  | 76/100 [11:00<03:17,  8.24s/it][I 2024-05-07 07:24:19,832] Trial 76 finished with value: 1.4702849388122559 and parameters: {'hidden_size1': 103, 'hidden_size2': 55, 'learning_rate': 0.004657791317110223, 'dropout_prob': 0.3949395447784599, 'momentum': 0.7600067483483995}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 77%|███████▋  | 77/100 [11:08<03:05,  8.04s/it][I 2024-05-07 07:24:27,155] Trial 77 finished with value: 1.2791125774383545 and parameters: {'hidden_size1': 97, 'hidden_size2': 42, 'learning_rate': 0.0107860569786743, 'dropout_prob': 0.3807670753096247, 'momentum': 0.7202909248772732}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 78%|███████▊  | 78/100 [11:15<02:52,  7.83s/it][I 2024-05-07 07:24:33,714] Trial 78 finished with value: 2.132829189300537 and parameters: {'hidden_size1': 89, 'hidden_size2': 41, 'learning_rate': 0.02027551117609348, 'dropout_prob': 0.3770307383088847, 'momentum': 0.8612615859740153}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 79%|███████▉  | 79/100 [11:22<02:36,  7.45s/it][I 2024-05-07 07:24:40,788] Trial 79 finished with value: 1.4168986082077026 and parameters: {'hidden_size1': 110, 'hidden_size2': 37, 'learning_rate': 0.0078082776861016605, 'dropout_prob': 0.35425577141550335, 'momentum': 0.7843157896639679}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 80%|████████  | 80/100 [11:29<02:26,  7.34s/it][I 2024-05-07 07:24:48,302] Trial 80 finished with value: 1.4029587507247925 and parameters: {'hidden_size1': 98, 'hidden_size2': 43, 'learning_rate': 0.010887079979567749, 'dropout_prob': 0.3836751433797469, 'momentum': 0.7106373429921582}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 81%|████████  | 81/100 [11:36<02:20,  7.39s/it][I 2024-05-07 07:24:56,940] Trial 81 finished with value: 1.4760410785675049 and parameters: {'hidden_size1': 122, 'hidden_size2': 50, 'learning_rate': 0.01525085151832321, 'dropout_prob': 0.40873965675492346, 'momentum': 0.6451282898002443}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 82%|████████▏ | 82/100 [11:45<02:19,  7.76s/it][I 2024-05-07 07:25:07,101] Trial 82 finished with value: 1.4291151762008667 and parameters: {'hidden_size1': 134, 'hidden_size2': 46, 'learning_rate': 0.011079867712624219, 'dropout_prob': 0.3653097051821174, 'momentum': 0.7522720487272232}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 83%|████████▎ | 83/100 [11:55<02:24,  8.48s/it][I 2024-05-07 07:25:14,528] Trial 83 finished with value: 1.7139531373977661 and parameters: {'hidden_size1': 93, 'hidden_size2': 63, 'learning_rate': 0.003650176076793475, 'dropout_prob': 0.3985303226131968, 'momentum': 0.728954379720663}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 84%|████████▍ | 84/100 [12:03<02:10,  8.17s/it][I 2024-05-07 07:25:22,414] Trial 84 finished with value: 1.3477325439453125 and parameters: {'hidden_size1': 113, 'hidden_size2': 69, 'learning_rate': 0.005233618977934415, 'dropout_prob': 0.35485123945538266, 'momentum': 0.8269072590871711}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 85%|████████▌ | 85/100 [12:11<02:01,  8.08s/it][I 2024-05-07 07:25:30,821] Trial 85 finished with value: 1.4563966989517212 and parameters: {'hidden_size1': 145, 'hidden_size2': 49, 'learning_rate': 0.0072611446229000195, 'dropout_prob': 0.38287486909010493, 'momentum': 0.7014078210865747}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 86%|████████▌ | 86/100 [12:19<01:54,  8.18s/it][I 2024-05-07 07:25:38,005] Trial 86 finished with value: 1.348259687423706 and parameters: {'hidden_size1': 97, 'hidden_size2': 54, 'learning_rate': 0.009409034742676844, 'dropout_prob': 0.33355867162110986, 'momentum': 0.8044391223893914}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 87%|████████▋ | 87/100 [12:26<01:42,  7.88s/it][I 2024-05-07 07:25:45,727] Trial 87 finished with value: 1.7631454467773438 and parameters: {'hidden_size1': 121, 'hidden_size2': 75, 'learning_rate': 0.03382100165062434, 'dropout_prob': 0.4178094453065232, 'momentum': 0.7634991808880754}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 88%|████████▊ | 88/100 [12:34<01:33,  7.83s/it][I 2024-05-07 07:25:53,567] Trial 88 finished with value: 1.47151780128479 and parameters: {'hidden_size1': 84, 'hidden_size2': 41, 'learning_rate': 0.005406862317260276, 'dropout_prob': 0.3697480142500618, 'momentum': 0.8621533076123603}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 89%|████████▉ | 89/100 [12:42<01:26,  7.84s/it][I 2024-05-07 07:26:01,535] Trial 89 finished with value: 1.3431944847106934 and parameters: {'hidden_size1': 104, 'hidden_size2': 37, 'learning_rate': 0.012289951690835283, 'dropout_prob': 0.3444730794696814, 'momentum': 0.7391919761513502}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 90%|█████████ | 90/100 [12:50<01:18,  7.88s/it][I 2024-05-07 07:26:08,445] Trial 90 finished with value: 1.7517447471618652 and parameters: {'hidden_size1': 105, 'hidden_size2': 34, 'learning_rate': 0.01814167656692973, 'dropout_prob': 0.34656682395054106, 'momentum': 0.7823454179222183}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 91%|█████████ | 91/100 [12:57<01:08,  7.59s/it][I 2024-05-07 07:26:16,196] Trial 91 finished with value: 1.6084434986114502 and parameters: {'hidden_size1': 129, 'hidden_size2': 45, 'learning_rate': 0.013095592110991327, 'dropout_prob': 0.4023732468601453, 'momentum': 0.7351229349849613}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 92%|█████████▏| 92/100 [13:04<01:01,  7.63s/it][I 2024-05-07 07:26:22,998] Trial 92 finished with value: 1.4260478019714355 and parameters: {'hidden_size1': 112, 'hidden_size2': 38, 'learning_rate': 0.007979314171804676, 'dropout_prob': 0.3881767168806027, 'momentum': 0.8353251409186363}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 93%|█████████▎| 93/100 [13:11<00:51,  7.39s/it][I 2024-05-07 07:26:29,778] Trial 93 finished with value: 1.6109684705734253 and parameters: {'hidden_size1': 76, 'hidden_size2': 57, 'learning_rate': 0.022909349698459962, 'dropout_prob': 0.32500427428135453, 'momentum': 0.6912320373051926}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 94%|█████████▍| 94/100 [13:18<00:43,  7.20s/it][I 2024-05-07 07:26:36,567] Trial 94 finished with value: 1.4313125610351562 and parameters: {'hidden_size1': 92, 'hidden_size2': 42, 'learning_rate': 0.006812416258656685, 'dropout_prob': 0.37159085974785666, 'momentum': 0.7975730692954515}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 95%|█████████▌| 95/100 [13:25<00:35,  7.08s/it][I 2024-05-07 07:26:43,266] Trial 95 finished with value: 1.3898329734802246 and parameters: {'hidden_size1': 136, 'hidden_size2': 32, 'learning_rate': 0.010237305585363236, 'dropout_prob': 0.4075130160974946, 'momentum': 0.716851308892223}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 96%|█████████▌| 96/100 [13:31<00:27,  6.96s/it][I 2024-05-07 07:26:53,441] Trial 96 finished with value: 1.5468217134475708 and parameters: {'hidden_size1': 159, 'hidden_size2': 51, 'learning_rate': 0.003803304478973316, 'dropout_prob': 0.35920750548802205, 'momentum': 0.7598206064484377}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 97%|█████████▋| 97/100 [13:42<00:23,  7.93s/it][I 2024-05-07 07:27:02,657] Trial 97 finished with value: 1.6298677921295166 and parameters: {'hidden_size1': 102, 'hidden_size2': 47, 'learning_rate': 0.011838682656635884, 'dropout_prob': 0.4327916611658709, 'momentum': 0.6568143907580036}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 98%|█████████▊| 98/100 [13:51<00:16,  8.31s/it][I 2024-05-07 07:27:09,543] Trial 98 finished with value: 1.6468380689620972 and parameters: {'hidden_size1': 118, 'hidden_size2': 36, 'learning_rate': 0.015497395142247137, 'dropout_prob': 0.38088088608949255, 'momentum': 0.8221590148113829}. Best is trial 61 with value: 1.1989573240280151.\n",
      " 99%|█████████▉| 99/100 [13:58<00:07,  7.89s/it][I 2024-05-07 07:27:17,346] Trial 99 finished with value: 1.3544085025787354 and parameters: {'hidden_size1': 147, 'hidden_size2': 54, 'learning_rate': 0.0052938735131123175, 'dropout_prob': 0.3930823352263163, 'momentum': 0.8444342257532776}. Best is trial 61 with value: 1.1989573240280151.\n",
      "100%|██████████| 100/100 [14:06<00:00,  8.46s/it]\n"
     ]
    }
   ],
   "source": [
    "'''Procurando melhor arquitetura'''\n",
    "\n",
    "# Definindo parametro de entrada da rede\n",
    "input_size = 8\n",
    "epochs = 2000\n",
    "\n",
    "# Definindo o número de arquiteturas a serem testadas\n",
    "num_arch = 100\n",
    "\n",
    "# Função objetivo para otimização do Optuna\n",
    "def objective(trial):\n",
    "    # Definindo os hiperparâmetros a serem otimizados\n",
    "    hidden_size1 = trial.suggest_int('hidden_size1', 16, 256, log=True)\n",
    "    hidden_size2 = trial.suggest_int('hidden_size2', 16, 256, log=True)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    dropout_prob = trial.suggest_float('dropout_prob', 0.0, 0.5)\n",
    "    momentum = trial.suggest_float('momentum', 0.0, 1.0)\n",
    "\n",
    "    \n",
    "    # Criando uma instância do modelo com os hiperparâmetros sugeridos\n",
    "    model = NeuralNetworkRegressor(input_size, hidden_size1, hidden_size2, learning_rate, dropout_prob, momentum)\n",
    "    \n",
    "    # Treinando o modelo\n",
    "    model.train_model(X_train_scaled_tensor, Y_train_tensor_dimensionado, epochs)\n",
    "    \n",
    "    # Fazendo previsões no conjunto de validação\n",
    "    predictions = model.predict(X_test_scaled_tensor)\n",
    "\n",
    "    # Calcula o MSE no conjunto de validação\n",
    "    mse = mean_squared_error(Y_test_tensor_dimensionado, predictions)\n",
    "    \n",
    "    return mse\n",
    "\n",
    "\n",
    "# Executando a otimização do Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "# Cria uma  barra de progresso com um total igual ao número de arquiteturas a serem testadas\n",
    "with tqdm(total=num_arch) as pbar:\n",
    "    # Define da função de atualização da barra de progresso\n",
    "    def update_progress_bar(study, trial):\n",
    "        pbar.update(1)  # Atualiza a barra de progresso em uma unidade a cada chamada\n",
    "    \n",
    "    # Executa a otimização com Optuna\n",
    "    study.optimize(objective, n_trials=num_arch, callbacks=[update_progress_bar])\n",
    "\n",
    "# Obtendo os melhores hiperparâmetros encontrados\n",
    "best_params = study.best_params\n",
    "\n",
    "# Criando uma instância do modelo com os melhores hiperparâmetros\n",
    "best_model = NeuralNetworkRegressor(input_size, **best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Treinando a melhor rede'''\n",
    "\n",
    "# Treinando o modelo com todos os dados de treinamento\n",
    "best_model.train_model(X_train_scaled_tensor, Y_train_tensor_dimensionado, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando a rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "tensor([[5.3599],\n",
      "        [2.7786],\n",
      "        [2.8750],\n",
      "        [4.9992],\n",
      "        [5.6032],\n",
      "        [6.6854],\n",
      "        [5.0182],\n",
      "        [5.6403],\n",
      "        [5.1783],\n",
      "        [5.0581],\n",
      "        [5.6009],\n",
      "        [4.8218],\n",
      "        [5.3545],\n",
      "        [5.2288],\n",
      "        [5.8930],\n",
      "        [2.9595],\n",
      "        [2.6373],\n",
      "        [5.1576],\n",
      "        [4.6335],\n",
      "        [5.5807],\n",
      "        [5.1871],\n",
      "        [5.3857],\n",
      "        [4.7171],\n",
      "        [5.1820],\n",
      "        [2.5951],\n",
      "        [5.2775],\n",
      "        [5.6571],\n",
      "        [4.9250],\n",
      "        [5.7151],\n",
      "        [5.1456],\n",
      "        [4.6115],\n",
      "        [6.2804],\n",
      "        [5.4030],\n",
      "        [2.6373],\n",
      "        [5.1639],\n",
      "        [5.0126],\n",
      "        [5.8450],\n",
      "        [5.4958],\n",
      "        [5.4180],\n",
      "        [5.7904],\n",
      "        [4.9597],\n",
      "        [5.2788],\n",
      "        [6.1019],\n",
      "        [5.6660],\n",
      "        [4.9076],\n",
      "        [2.9064],\n",
      "        [4.1666],\n",
      "        [6.2061],\n",
      "        [5.3641],\n",
      "        [3.7771],\n",
      "        [4.4981],\n",
      "        [3.0434]])\n"
     ]
    }
   ],
   "source": [
    "'''Testando a rede'''\n",
    "\n",
    "# Fazendo previsões finais com o melhor modelo\n",
    "final_predictions = best_model.predict(X_test_scaled_tensor)\n",
    "print(\"Predictions:\")\n",
    "print(final_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo dos erros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.42014741897583\n",
      "MAE: 0.9559291005134583\n",
      "RMSE: 1.1916993856430054\n",
      "R²: 0.37609307378301937\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Calculando erros'''\n",
    "\n",
    "# Calculando as métricas\n",
    "mse = mean_squared_error(Y_test_tensor_dimensionado, final_predictions)\n",
    "mae = mean_absolute_error(Y_test_tensor_dimensionado, final_predictions)\n",
    "rmse = root_mean_squared_error(Y_test_tensor_dimensionado, final_predictions)\n",
    "r2 = r_squared(Y_test_tensor_dimensionado, final_predictions)\n",
    "\n",
    "# Exibindo os resultados\n",
    "print(f'MSE: {mse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'R²: {r2}')\n",
    "\n",
    "\n",
    "# Plotando o gráfico de resíduos\n",
    "plot_residuals(Y_test_tensor_dimensionado, final_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir dos resultados da dureza dos minerais preditos pela rede neural, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dark_Kernel",
   "language": "python",
   "name": "dark_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "25a19fbe0a9132dfb9279d48d161753c6352f8f9478c2e74383d340069b907c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
