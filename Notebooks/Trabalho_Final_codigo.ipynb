{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center; align-items: center;\">\n",
    "<p align=\"center\">\n",
    "<img src=\"../imagens/Capa_c.jpeg\" style=\"width:250px;height:250px\" alt='imagem do daniel'>\n",
    "</p>\n",
    "\n",
    "<div style=\"margin-left: 20px;\">\n",
    "<h1 style=\"font-family: times new roman;\">DreamCoders</h1>\n",
    "<p style=\"margin-bottom: 5px;\">Trabalho Final de Redes Neurais</p>\n",
    "<p style=\"margin-bottom: 5px;\">INTEGRANTES: Eric Leandro e Samuel Araujo </p>\n",
    "<p style=\"margin-bottom: 5px;\">DISCIPLINA: Redes Neurais e Algoritmos Genéticos</p>\n",
    "<p style=\"margin-bottom: 5px;\">MESTRE: Daniel Cassar</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextualização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Ciência de Materiais é uma área vasta que abrange uma grande quantidade de áreas da ciência, desde físico-química até as biociências. O desenvolvimento da Ciência de Materiais é de suma importância para o descobrimento e aperfeiçoamento de materiais para as mais diversas aplicações nos setores da indústria e, dessa forma, permitndo o avanço de diversas tecnologias que imapactam o dia a dia de toda a humanidade seja diretamente ou indiretamente. Tendo como uma parte muito importante para as pesquisas em materiais o entendimento das características/propriedades intrísecas de cada material, através delas é possível prever e entender como aplicar o material ou aperfeiçoa-lo.\n",
    "\n",
    "Com o advento da ciência de dados, foi possível avanços mais rápidos na descoberta e caracterização dos materiais, tornando-se uma parte fundamental no avanço da ciência de materiais. Dentro da área de dados, existem diversas técnicas de previsão de características de materiais, sendo comum a aplicação de redes neurais para previsão de características específicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As redes neurais são uma técnica de aprendizado de máquina inspirada no funcionamento do cérebro humano. Tendo como ideia fundamental o conceito de camadas de neurônios, onde cada neurônio aplica uma função de ativação após calcular uma combinação linear das entradas com pesos passadas pelos neurônios das camadas anteriores, isso só é possível devido a conexão entre os neurôneos de diferentes camadas.\n",
    "\n",
    "Existem três divisões principais de camadas:\n",
    "- Camada de Entrada -> Recebe os dados originais que serão a base para o treinamento da rede, e transmitem para os neurônios da primeira camada oculta.Cada neurônio corresponde a uma característica de entrada, no nosso caso, a uma feature\n",
    "- Camadas Ocultas -> As camadas ocultas são compostas por neurônios que recebem entradas dos neurôneos da camada anterior, aplicam uma combinação linear, em seguida aplicam a função de ativação e, por fim, tranmitem uma saída para os neurôneos da próxima camada. Cada camada oculta pode ter um número de neurôneos diferentes, mudando assim, os resultados da combinação linear e, por conseguinte, da saída de cada neurônio.\n",
    "- Camada de Saída -> Recebe as saídas dos neurônios da última camada oculta e mostram um resultado. Sendo que cada neurônio da camada de saída corresponde a uma características específica. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O projeto consiste na aplicação e na otimização de hiperparâmetros de uma rede neural do tipo MLP para prever a dureza, na escala de Mohs, de minerais a partir de 11 features inerentes a cada material. A partir dos resultados da predição, analisar a eficácia de redes neurais MLP para predição de dureza de minerais com estruturas cristalinas distintas e explorar as variações para cada tipo de estrutura cristalina, considerando as amostragens utilizadas.\n",
    "Vale ressaltar que não faz parte do escopo do projeto se aprofundar nos aspectos técnicos, sendo necessário, para isso, a leitura das referências bibliográficas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados extraídos consiste em dois databases de minerais, uma para treino com 622 minerais e outro para teste com 51 minerais. Ambos os dataset possuem um total de 11 descrições atômicas de cada mineral, ou seja, 11 features e a dureza como target.\n",
    "*Features*\n",
    "- Número de elétrons -> O número total de elétrons presentes em um átomo neutro\n",
    "\n",
    "- Número de elétrons de valência -> O número de elétrons que ocupam o orbital de energia mais externa do átomo, diretamente envolvidos na formação de ligações químicas\n",
    "\n",
    "- Número atômico -> O número de prótons no núcleo do átomo. Como cada átomo neutro possui o mesmo número de elétrons e prótons, o número atômico também indica o número total de elétrons\n",
    "\n",
    "- Eletronegativa de Pauling -> Uma medida da tendência de um átomo em um composto atrair elétrons para si em uma ligação química, considerando o estado de oxidação mais frequente do elemento. Valores mais altos indicam maior atratividade por elétrons\n",
    "\n",
    "- Eletronegatividade do estado de oxidação mais comum -> Similar à eletronegatividade de Pauling, mas se concentra no estado de oxidação específico do elemento em questão\n",
    "\n",
    "- Raios atômicos covalentes -> Metade da distância média entre os núcleos de dois átomos iguais unidos por uma ligação covalente simples\n",
    "\n",
    "- Raio de Vander Waals -> Distância mínima permitida entre os núcleos de dois átomos que não estão ligados quimicamente, mas sentem forças de atração de van der Waals fracas\n",
    "\n",
    "- Energia de ionização do neutro -> A quantidade mínima de energia necessária para remover o elétron mais fracamente ligado (geralmente o do orbital de valência) de um átomo neutro, formando um cátion (íon positivo)\n",
    "\n",
    "- Média de todos os elétrons -> A média da distância de todos os elétrons em relação ao núcleo atômico\n",
    "\n",
    "- Densidade média -> A massa por unidade de volume do átomo, obtida pela razão entre a massa atômica e o volume atômico\n",
    "\n",
    "- Peso atômico -> A massa média ponderada de um elemento, considerando a abundância natural de seus isótopos\n",
    "\n",
    "Entretanto, o dataset de teste possui duas colunas a mais. A primeira corresponde a fórmula molecular do mineral e a segunda ao tipo de estrutura ceistalina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset de Treino\n",
    "O Dataset de treino é composto por 622 minerais com composições únicas obtidos por permutações composicionais de uma base de dados com 369 minerais únicos retirados do *Physical and Optical Properties of Minerals CRC Handbook of Chemestry and Physics* e da *American Mineralogist Crystal Structure Database* . Os minerais presentes do dataset possuem estruturas critalinas diversas como já foi mencionado, sendo: \n",
    "\n",
    "- 210 de estrutura monoclínica;\n",
    "- 96 de estrutura romboédrica;\n",
    "- 89 de estrutura hexagonal;\n",
    "- 80 de estrutura tetragonal;\n",
    "- 73 de estrutura cúbica;\n",
    "- 50 de estrutura ortorrômbica;\n",
    "- 22 de estrutura triclínica;\n",
    "- 1 de estrutura trigonal;\n",
    "- 1 de estrutura amorfa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset de Teste\n",
    "O dataset de teste é composto por 51 cristais sintéticos singulares retirados da literatura, sendo a distribuição das estruturas cristalinas:\n",
    "\n",
    "- 15 de estrutura monoclínica;\n",
    "- 7 de estrutura tetragonal;\n",
    "- 7 de estrutura hexagonal;\n",
    "- 6 de estrutura ortorrômbicas;\n",
    "- 4 de estrutura cúbica;\n",
    "- 3 de estrutura romboédrica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metodologia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para alcançar o objetivo do trabalho de forma plena, foi realizado uma série de etapas: \n",
    "\n",
    "- Primeiro, a manipulação dos dados partindo da transformação dos dados de csv para o formato dataframe de tratamento para os datasets junto ao tratamento dos datasets a partir da exploração dos mesmos, permitindo, assim, filtrar as features necessária e relevantes para previsão da dureza, depois, foi feita a segmentação dos dois datasets em treino e teste junto a normalização de dados. \n",
    "- Após a etapa de manipulação, foi construída uma rede neural MLP do tipo regressora, logo depois, fez-se a otimização dos hiperparâmetros usando SGD do próprio PyTorch, ademais, foi feito o treinamento aprimorando a perda através da aplicação de 100 arquiteturas de rede distintas. \n",
    "- Por fim, aplicou-se a rede neural treinada no dataset de teste e computou-se diversas métricas de erros para análise da precisão da rede neural para os diferentes tipos de estrutura cristalina.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definições"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro é necessário a importação das bibliotecas do python que serão utilizadas.\n",
    "\n",
    "**Manipulação de dados**\n",
    "- Numpy;\n",
    "- Pandas.\n",
    "\n",
    "**Rede Neural**\n",
    "- Pytorch -> Incluindo \"torch.nn\" para criação da rede neural e \"torch.optim\" para optimização da rede.\n",
    "\n",
    "**Métricas e Dimensionamento**\n",
    "- Scikitlearn  -> Sendo \"sklearn.metrics\" para as métricas dos erros das previsões e \"sklearn.preprocessing\", em específico, \"MinMaxScaler\" para redimenssionamento.\n",
    "\n",
    "**Visualização**\n",
    "- Matplotlib -> \"matplotlib.pyplot\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir daqui, será dado início a manipulação dos datasets e o desenvolvimento da rede neural junto com a sua otimização e aplicação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante o desenvolvimento do presente trebalho, fez-se necessário o uso de bibliotecas específicas para cada etapa.\n",
    "\n",
    "**Manipulação e Tratamento dos Datasets**\n",
    "- Numpy\n",
    "- Pandas\n",
    "- sklearn.preprocessing\n",
    "\n",
    "**Rede Neural**\n",
    "- torch.nn\n",
    "- torch.optim\n",
    "- optuna\n",
    "- tqdm\n",
    "\n",
    "**Cálculo das Métricas**\n",
    "- sklearn.metrics\n",
    "\n",
    "**Visualização**\n",
    "- matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''IMPORTS'''\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import optuna\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, há a definição de algumas funções que serão utilizadas ao decorrer do notebook para o cálculo das métricas de erro(RMSE e R2) junto com a função para o plot do gráfico dos resíduos de erros, sendo que todas as funções recebem como parâmetros a lista dos valores verdadeiros do target e a lista dos valores preditos para os targets.\n",
    "\n",
    "- root_mean_squared_error -> Cálculo da raiz do erro quadrático médio (RMSE)\n",
    "- r_squared -> Cálculo do coeficiente de determinação (R<sup>2</sup>)\n",
    "- plot_residuals -> Plota o gráfico dos resíduos dos erros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''FUNÇÕES'''\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula a raiz do erro quadrático médio (RMSE).\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): Os valores verdadeiros.\n",
    "        y_pred (array-like): Os valores previstos.\n",
    "\n",
    "    Returns:\n",
    "        float: A raiz do erro quadrático médio.\n",
    "    \"\"\"\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula o coeficiente de determinação (R²).\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): Os valores verdadeiros.\n",
    "        y_pred (array-like): Os valores previstos.\n",
    "\n",
    "    Returns:\n",
    "        float: O coeficiente de determinação.\n",
    "    \"\"\"\n",
    "    return r2_score(y_true, y_pred)\n",
    "\n",
    "def plot_residuals(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Plota o gráfico de resíduos.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): Os valores verdadeiros.\n",
    "        y_pred (array-like): Os valores previstos.\n",
    "    \"\"\"\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "    plt.title('Gráfico de Resíduos')\n",
    "    plt.xlabel('Valores Previstos')\n",
    "    plt.ylabel('Resíduos')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulação dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, é feita a leitura dos arquivos dos datasets no formato .csv e a transformação para o formato DataFrame do pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Pegando os dados'''\n",
    "\n",
    "# Carregando o arquivo CSV para o teste\n",
    "minerais_treino = pd.read_csv(\"../Datasets/Mineral_Dataset_Supplementary_Info.csv\")\n",
    "\n",
    "# Transformando o dataset em um Dataframe pandas\n",
    "df_treino = pd.DataFrame(minerais_treino) \n",
    "\n",
    "# Carregando o arquivo CSV para o teste\n",
    "minerais_teste = pd.read_csv('../Datasets/Artificial_Crystals_Dataset.csv')\n",
    "\n",
    "# Transformando o dataset em um Dataframe pandas\n",
    "df_teste = pd.DataFrame(minerais_teste) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, a mostragem do dataframe referente ao dataset de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de treino:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>allelectrons_Total</th>\n",
       "      <th>density_Total</th>\n",
       "      <th>allelectrons_Average</th>\n",
       "      <th>val_e_Average</th>\n",
       "      <th>atomicweight_Average</th>\n",
       "      <th>ionenergy_Average</th>\n",
       "      <th>el_neg_chi_Average</th>\n",
       "      <th>R_vdw_element_Average</th>\n",
       "      <th>R_cov_element_Average</th>\n",
       "      <th>zaratio_Average</th>\n",
       "      <th>density_Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>110.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>36.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>82.598467</td>\n",
       "      <td>8.504133</td>\n",
       "      <td>2.146667</td>\n",
       "      <td>2.006667</td>\n",
       "      <td>1.253333</td>\n",
       "      <td>0.456803</td>\n",
       "      <td>7.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>406.0</td>\n",
       "      <td>30.472136</td>\n",
       "      <td>9.902439</td>\n",
       "      <td>4.682927</td>\n",
       "      <td>19.813180</td>\n",
       "      <td>11.456151</td>\n",
       "      <td>2.700244</td>\n",
       "      <td>1.676829</td>\n",
       "      <td>0.868293</td>\n",
       "      <td>0.522909</td>\n",
       "      <td>0.743223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>406.0</td>\n",
       "      <td>30.472464</td>\n",
       "      <td>10.410256</td>\n",
       "      <td>4.923077</td>\n",
       "      <td>20.931371</td>\n",
       "      <td>11.541405</td>\n",
       "      <td>2.753590</td>\n",
       "      <td>1.703846</td>\n",
       "      <td>0.894359</td>\n",
       "      <td>0.497498</td>\n",
       "      <td>0.781345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5.5</td>\n",
       "      <td>476.0</td>\n",
       "      <td>61.142136</td>\n",
       "      <td>11.609756</td>\n",
       "      <td>4.682927</td>\n",
       "      <td>23.659644</td>\n",
       "      <td>11.487395</td>\n",
       "      <td>2.763659</td>\n",
       "      <td>1.714634</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.519474</td>\n",
       "      <td>1.491272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.5</td>\n",
       "      <td>476.0</td>\n",
       "      <td>61.142464</td>\n",
       "      <td>12.205128</td>\n",
       "      <td>4.923077</td>\n",
       "      <td>24.975089</td>\n",
       "      <td>11.574251</td>\n",
       "      <td>2.820256</td>\n",
       "      <td>1.743590</td>\n",
       "      <td>0.873846</td>\n",
       "      <td>0.493887</td>\n",
       "      <td>1.567755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>625</td>\n",
       "      <td>3.8</td>\n",
       "      <td>46.0</td>\n",
       "      <td>9.133000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>48.719500</td>\n",
       "      <td>9.877100</td>\n",
       "      <td>2.115000</td>\n",
       "      <td>1.905000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>0.478880</td>\n",
       "      <td>4.566500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>626</td>\n",
       "      <td>4.5</td>\n",
       "      <td>86.0</td>\n",
       "      <td>6.674328</td>\n",
       "      <td>14.333333</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>30.645954</td>\n",
       "      <td>11.862733</td>\n",
       "      <td>2.861667</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.901667</td>\n",
       "      <td>0.487172</td>\n",
       "      <td>1.112388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>628</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>7.134332</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>40.689515</td>\n",
       "      <td>11.506150</td>\n",
       "      <td>2.545000</td>\n",
       "      <td>1.765000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.479405</td>\n",
       "      <td>3.567166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>629</td>\n",
       "      <td>7.5</td>\n",
       "      <td>86.0</td>\n",
       "      <td>8.841328</td>\n",
       "      <td>14.333333</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>30.550687</td>\n",
       "      <td>11.543000</td>\n",
       "      <td>2.831667</td>\n",
       "      <td>1.735000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.489507</td>\n",
       "      <td>1.473555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>630</td>\n",
       "      <td>6.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>18.204400</td>\n",
       "      <td>10.272727</td>\n",
       "      <td>4.727273</td>\n",
       "      <td>20.652175</td>\n",
       "      <td>11.148755</td>\n",
       "      <td>2.702273</td>\n",
       "      <td>1.695455</td>\n",
       "      <td>0.875455</td>\n",
       "      <td>0.519605</td>\n",
       "      <td>0.827473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>622 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Hardness  allelectrons_Total  density_Total  \\\n",
       "0             0       2.3               110.0      23.000000   \n",
       "1             1       5.5               406.0      30.472136   \n",
       "2             2       5.5               406.0      30.472464   \n",
       "3             3       5.5               476.0      61.142136   \n",
       "4             4       5.5               476.0      61.142464   \n",
       "..          ...       ...                 ...            ...   \n",
       "617         625       3.8                46.0       9.133000   \n",
       "618         626       4.5                86.0       6.674328   \n",
       "619         628       4.0                38.0       7.134332   \n",
       "620         629       7.5                86.0       8.841328   \n",
       "621         630       6.0               226.0      18.204400   \n",
       "\n",
       "     allelectrons_Average  val_e_Average  atomicweight_Average  \\\n",
       "0               36.666667       2.666667             82.598467   \n",
       "1                9.902439       4.682927             19.813180   \n",
       "2               10.410256       4.923077             20.931371   \n",
       "3               11.609756       4.682927             23.659644   \n",
       "4               12.205128       4.923077             24.975089   \n",
       "..                    ...            ...                   ...   \n",
       "617             23.000000       4.000000             48.719500   \n",
       "618             14.333333       5.166667             30.645954   \n",
       "619             19.000000       4.000000             40.689515   \n",
       "620             14.333333       5.000000             30.550687   \n",
       "621             10.272727       4.727273             20.652175   \n",
       "\n",
       "     ionenergy_Average  el_neg_chi_Average  R_vdw_element_Average  \\\n",
       "0             8.504133            2.146667               2.006667   \n",
       "1            11.456151            2.700244               1.676829   \n",
       "2            11.541405            2.753590               1.703846   \n",
       "3            11.487395            2.763659               1.714634   \n",
       "4            11.574251            2.820256               1.743590   \n",
       "..                 ...                 ...                    ...   \n",
       "617           9.877100            2.115000               1.905000   \n",
       "618          11.862733            2.861667               1.700000   \n",
       "619          11.506150            2.545000               1.765000   \n",
       "620          11.543000            2.831667               1.735000   \n",
       "621          11.148755            2.702273               1.695455   \n",
       "\n",
       "     R_cov_element_Average  zaratio_Average  density_Average  \n",
       "0                 1.253333         0.456803         7.666667  \n",
       "1                 0.868293         0.522909         0.743223  \n",
       "2                 0.894359         0.497498         0.781345  \n",
       "3                 0.848780         0.519474         1.491272  \n",
       "4                 0.873846         0.493887         1.567755  \n",
       "..                     ...              ...              ...  \n",
       "617               1.120000         0.478880         4.566500  \n",
       "618               0.901667         0.487172         1.112388  \n",
       "619               0.920000         0.479405         3.567166  \n",
       "620               0.890000         0.489507         1.473555  \n",
       "621               0.875455         0.519605         0.827473  \n",
       "\n",
       "[622 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mostrando o Dataframe\n",
    "print(\"Dataset de treino:\")\n",
    "display(df_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, o Dataframe do dataset de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de teste:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Formula</th>\n",
       "      <th>Crystal structure</th>\n",
       "      <th>Hardness (Mohs)</th>\n",
       "      <th>allelectrons_Total</th>\n",
       "      <th>density_Total</th>\n",
       "      <th>allelectrons_Average</th>\n",
       "      <th>val_e_Average</th>\n",
       "      <th>atomicweight_Average</th>\n",
       "      <th>ionenergy_Average</th>\n",
       "      <th>el_neg_chi_Average</th>\n",
       "      <th>R_vdw_element_Average</th>\n",
       "      <th>R_cov_element_Average</th>\n",
       "      <th>zaratio_Average</th>\n",
       "      <th>density_Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MnTeMoO6</td>\n",
       "      <td>tetragonal</td>\n",
       "      <td>4.50</td>\n",
       "      <td>167.000</td>\n",
       "      <td>23.907992</td>\n",
       "      <td>18.555556</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>41.609136</td>\n",
       "      <td>11.693844</td>\n",
       "      <td>2.938889</td>\n",
       "      <td>1.711111</td>\n",
       "      <td>0.884444</td>\n",
       "      <td>0.477830</td>\n",
       "      <td>2.656444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>MgH2</td>\n",
       "      <td>tetragonal</td>\n",
       "      <td>4.00</td>\n",
       "      <td>14.000</td>\n",
       "      <td>1.740168</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>8.773227</td>\n",
       "      <td>11.614333</td>\n",
       "      <td>1.903333</td>\n",
       "      <td>1.310000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.825990</td>\n",
       "      <td>0.580056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CO(NH2)2C4H6O5</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>2.50</td>\n",
       "      <td>102.000</td>\n",
       "      <td>8.511159</td>\n",
       "      <td>4.434783</td>\n",
       "      <td>3.304348</td>\n",
       "      <td>8.440584</td>\n",
       "      <td>13.176622</td>\n",
       "      <td>2.672609</td>\n",
       "      <td>1.379130</td>\n",
       "      <td>0.530870</td>\n",
       "      <td>0.713850</td>\n",
       "      <td>0.370050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>GaPO4</td>\n",
       "      <td>trigonal</td>\n",
       "      <td>5.50</td>\n",
       "      <td>78.000</td>\n",
       "      <td>8.109328</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>27.448814</td>\n",
       "      <td>11.826400</td>\n",
       "      <td>2.960000</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.488163</td>\n",
       "      <td>1.351555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Ga3PO7</td>\n",
       "      <td>trigonal</td>\n",
       "      <td>6.50</td>\n",
       "      <td>164.000</td>\n",
       "      <td>19.921324</td>\n",
       "      <td>14.909091</td>\n",
       "      <td>5.090909</td>\n",
       "      <td>32.012361</td>\n",
       "      <td>11.255573</td>\n",
       "      <td>2.881818</td>\n",
       "      <td>1.640909</td>\n",
       "      <td>0.841818</td>\n",
       "      <td>0.483480</td>\n",
       "      <td>1.811029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>LiB3O5</td>\n",
       "      <td>orthorhombic</td>\n",
       "      <td>6.50</td>\n",
       "      <td>58.000</td>\n",
       "      <td>7.650660</td>\n",
       "      <td>6.444444</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>13.261239</td>\n",
       "      <td>10.930689</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>1.686667</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.479962</td>\n",
       "      <td>0.850073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>BiB3O6</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>5.75</td>\n",
       "      <td>146.000</td>\n",
       "      <td>16.864992</td>\n",
       "      <td>14.600000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>33.739258</td>\n",
       "      <td>11.388810</td>\n",
       "      <td>2.866000</td>\n",
       "      <td>1.695000</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.478464</td>\n",
       "      <td>1.686499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>K2Al2B2O7</td>\n",
       "      <td>trigonal</td>\n",
       "      <td>6.00</td>\n",
       "      <td>130.000</td>\n",
       "      <td>11.871324</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.307692</td>\n",
       "      <td>20.443453</td>\n",
       "      <td>10.198131</td>\n",
       "      <td>2.540000</td>\n",
       "      <td>1.820000</td>\n",
       "      <td>0.972308</td>\n",
       "      <td>0.489274</td>\n",
       "      <td>0.913179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Nd0.02GdLa0.16Ca4O(BO3)3</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>6.42</td>\n",
       "      <td>249.320</td>\n",
       "      <td>22.345960</td>\n",
       "      <td>13.713971</td>\n",
       "      <td>4.365237</td>\n",
       "      <td>29.432344</td>\n",
       "      <td>10.598482</td>\n",
       "      <td>2.525787</td>\n",
       "      <td>1.813894</td>\n",
       "      <td>0.992739</td>\n",
       "      <td>0.487604</td>\n",
       "      <td>1.229151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Nd0.02GdLa0.33Ca4O(BO3)3</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>6.35</td>\n",
       "      <td>259.010</td>\n",
       "      <td>23.392140</td>\n",
       "      <td>14.114986</td>\n",
       "      <td>4.343324</td>\n",
       "      <td>30.446537</td>\n",
       "      <td>10.551961</td>\n",
       "      <td>2.512578</td>\n",
       "      <td>1.819602</td>\n",
       "      <td>1.001515</td>\n",
       "      <td>0.486888</td>\n",
       "      <td>1.274776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>YVO4</td>\n",
       "      <td>tetragonal</td>\n",
       "      <td>5.00</td>\n",
       "      <td>94.000</td>\n",
       "      <td>10.584328</td>\n",
       "      <td>15.666667</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>33.973910</td>\n",
       "      <td>11.239317</td>\n",
       "      <td>2.768333</td>\n",
       "      <td>1.745000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.481708</td>\n",
       "      <td>1.764055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>LiNbO3</td>\n",
       "      <td>rhomohedral</td>\n",
       "      <td>5.00</td>\n",
       "      <td>68.000</td>\n",
       "      <td>9.107996</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>29.568292</td>\n",
       "      <td>10.600980</td>\n",
       "      <td>2.580000</td>\n",
       "      <td>1.712000</td>\n",
       "      <td>0.956000</td>\n",
       "      <td>0.474714</td>\n",
       "      <td>1.821599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Na3Ba2(B3O6)2F</td>\n",
       "      <td>hexagonal</td>\n",
       "      <td>4.50</td>\n",
       "      <td>280.000</td>\n",
       "      <td>24.150564</td>\n",
       "      <td>11.666667</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>25.810253</td>\n",
       "      <td>10.686196</td>\n",
       "      <td>2.586250</td>\n",
       "      <td>1.808333</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.479152</td>\n",
       "      <td>1.006274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>NdCa4O(BO3)3</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>5.40</td>\n",
       "      <td>235.000</td>\n",
       "      <td>20.223320</td>\n",
       "      <td>13.055556</td>\n",
       "      <td>4.388889</td>\n",
       "      <td>27.609017</td>\n",
       "      <td>10.614044</td>\n",
       "      <td>2.536667</td>\n",
       "      <td>1.810556</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.488869</td>\n",
       "      <td>1.123518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>LuAl3(BO3)4</td>\n",
       "      <td>trigonal</td>\n",
       "      <td>8.10</td>\n",
       "      <td>226.000</td>\n",
       "      <td>27.432984</td>\n",
       "      <td>11.300000</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>24.556189</td>\n",
       "      <td>10.999625</td>\n",
       "      <td>2.763500</td>\n",
       "      <td>1.684000</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.485063</td>\n",
       "      <td>1.371649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>BaWF8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00</td>\n",
       "      <td>202.000</td>\n",
       "      <td>22.812640</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>47.315423</td>\n",
       "      <td>15.245810</td>\n",
       "      <td>3.443000</td>\n",
       "      <td>1.662000</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.460005</td>\n",
       "      <td>2.281264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Na5B2PeO13</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>Nd0.02Sr2La0.667(VO4)2</td>\n",
       "      <td>hexagonal</td>\n",
       "      <td>5.00</td>\n",
       "      <td>225.219</td>\n",
       "      <td>21.553374</td>\n",
       "      <td>17.751951</td>\n",
       "      <td>4.522267</td>\n",
       "      <td>39.461656</td>\n",
       "      <td>10.850263</td>\n",
       "      <td>2.635493</td>\n",
       "      <td>1.808829</td>\n",
       "      <td>1.035042</td>\n",
       "      <td>0.477068</td>\n",
       "      <td>1.698855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>Cr0.02CaGdAlO4</td>\n",
       "      <td>tetragonal</td>\n",
       "      <td>6.00</td>\n",
       "      <td>129.480</td>\n",
       "      <td>12.297928</td>\n",
       "      <td>18.444444</td>\n",
       "      <td>4.418803</td>\n",
       "      <td>41.217319</td>\n",
       "      <td>10.378423</td>\n",
       "      <td>2.507578</td>\n",
       "      <td>1.796467</td>\n",
       "      <td>1.052137</td>\n",
       "      <td>0.483923</td>\n",
       "      <td>1.751842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>Y0.57La0.72Sc2.71(BO3)4</td>\n",
       "      <td>trigonal</td>\n",
       "      <td>6.50</td>\n",
       "      <td>236.180</td>\n",
       "      <td>24.574384</td>\n",
       "      <td>11.809000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>25.386557</td>\n",
       "      <td>11.097505</td>\n",
       "      <td>2.730650</td>\n",
       "      <td>1.740925</td>\n",
       "      <td>0.887445</td>\n",
       "      <td>0.483071</td>\n",
       "      <td>1.228719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>YCa9(VO4)7</td>\n",
       "      <td>rhomohedral</td>\n",
       "      <td>4.50</td>\n",
       "      <td>604.000</td>\n",
       "      <td>61.226296</td>\n",
       "      <td>13.422222</td>\n",
       "      <td>4.488889</td>\n",
       "      <td>27.870471</td>\n",
       "      <td>10.883696</td>\n",
       "      <td>2.621111</td>\n",
       "      <td>1.781333</td>\n",
       "      <td>1.009333</td>\n",
       "      <td>0.490911</td>\n",
       "      <td>1.360584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>Zn3BPO7</td>\n",
       "      <td>hexagonal</td>\n",
       "      <td>5.00</td>\n",
       "      <td>166.000</td>\n",
       "      <td>25.978324</td>\n",
       "      <td>13.833333</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>29.159414</td>\n",
       "      <td>11.857833</td>\n",
       "      <td>2.771667</td>\n",
       "      <td>1.699167</td>\n",
       "      <td>0.834167</td>\n",
       "      <td>0.485270</td>\n",
       "      <td>2.164860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>Nd0.02BaLaLiWO6</td>\n",
       "      <td>cubic</td>\n",
       "      <td>5.50</td>\n",
       "      <td>239.200</td>\n",
       "      <td>29.633992</td>\n",
       "      <td>23.872255</td>\n",
       "      <td>4.295409</td>\n",
       "      <td>56.475997</td>\n",
       "      <td>10.565210</td>\n",
       "      <td>2.528224</td>\n",
       "      <td>1.824132</td>\n",
       "      <td>1.065629</td>\n",
       "      <td>0.465199</td>\n",
       "      <td>2.957484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>Nd0.02YCa9(VO4)7</td>\n",
       "      <td>trigonal</td>\n",
       "      <td>4.50</td>\n",
       "      <td>605.200</td>\n",
       "      <td>61.364296</td>\n",
       "      <td>13.442914</td>\n",
       "      <td>4.487783</td>\n",
       "      <td>27.922168</td>\n",
       "      <td>10.881315</td>\n",
       "      <td>2.620453</td>\n",
       "      <td>1.781604</td>\n",
       "      <td>1.009720</td>\n",
       "      <td>0.490878</td>\n",
       "      <td>1.363045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>Nd30.02NaGd(MoO4)2</td>\n",
       "      <td>tetragonal</td>\n",
       "      <td>4.70</td>\n",
       "      <td>2024.200</td>\n",
       "      <td>236.459656</td>\n",
       "      <td>48.172299</td>\n",
       "      <td>2.690148</td>\n",
       "      <td>114.951853</td>\n",
       "      <td>7.146097</td>\n",
       "      <td>1.622865</td>\n",
       "      <td>2.209848</td>\n",
       "      <td>1.615840</td>\n",
       "      <td>0.434283</td>\n",
       "      <td>5.627312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>La2CaB10O19</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>6.50</td>\n",
       "      <td>336.000</td>\n",
       "      <td>37.583308</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>4.687500</td>\n",
       "      <td>22.810328</td>\n",
       "      <td>11.218466</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>1.726562</td>\n",
       "      <td>0.818125</td>\n",
       "      <td>0.482644</td>\n",
       "      <td>1.174478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>BaNaB9O15</td>\n",
       "      <td>trigonal</td>\n",
       "      <td>6.00</td>\n",
       "      <td>232.000</td>\n",
       "      <td>25.820980</td>\n",
       "      <td>8.923077</td>\n",
       "      <td>4.615385</td>\n",
       "      <td>19.136778</td>\n",
       "      <td>11.127088</td>\n",
       "      <td>2.760769</td>\n",
       "      <td>1.731923</td>\n",
       "      <td>0.800769</td>\n",
       "      <td>0.482639</td>\n",
       "      <td>0.993115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>BaBPO5</td>\n",
       "      <td>trigonal</td>\n",
       "      <td>6.50</td>\n",
       "      <td>116.000</td>\n",
       "      <td>8.076660</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>32.387739</td>\n",
       "      <td>11.510863</td>\n",
       "      <td>2.790000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.898750</td>\n",
       "      <td>0.481827</td>\n",
       "      <td>1.009583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>Y3Al5O12</td>\n",
       "      <td>cubic</td>\n",
       "      <td>8.50</td>\n",
       "      <td>278.000</td>\n",
       "      <td>26.917984</td>\n",
       "      <td>13.900000</td>\n",
       "      <td>4.650000</td>\n",
       "      <td>29.680679</td>\n",
       "      <td>10.599905</td>\n",
       "      <td>2.649500</td>\n",
       "      <td>1.720000</td>\n",
       "      <td>0.958000</td>\n",
       "      <td>0.486265</td>\n",
       "      <td>1.345899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>Nd0.02Ca9Gd(VO4)7</td>\n",
       "      <td>hexagonal</td>\n",
       "      <td>4.00</td>\n",
       "      <td>630.200</td>\n",
       "      <td>64.795296</td>\n",
       "      <td>13.998223</td>\n",
       "      <td>4.487783</td>\n",
       "      <td>29.440253</td>\n",
       "      <td>10.879816</td>\n",
       "      <td>2.620009</td>\n",
       "      <td>1.782048</td>\n",
       "      <td>1.011053</td>\n",
       "      <td>0.490174</td>\n",
       "      <td>1.439256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>BaCaBO3F</td>\n",
       "      <td>hexagonal</td>\n",
       "      <td>5.00</td>\n",
       "      <td>114.000</td>\n",
       "      <td>7.425576</td>\n",
       "      <td>16.285714</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>36.458070</td>\n",
       "      <td>11.128571</td>\n",
       "      <td>2.604286</td>\n",
       "      <td>1.848571</td>\n",
       "      <td>1.022857</td>\n",
       "      <td>0.477579</td>\n",
       "      <td>1.060797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>MgAl2O4</td>\n",
       "      <td>cubic</td>\n",
       "      <td>8.00</td>\n",
       "      <td>70.000</td>\n",
       "      <td>7.143328</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>20.323314</td>\n",
       "      <td>10.584314</td>\n",
       "      <td>2.612857</td>\n",
       "      <td>1.641429</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.493919</td>\n",
       "      <td>1.020475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>Zn3Nb2O8</td>\n",
       "      <td>tetragonal</td>\n",
       "      <td>5.50</td>\n",
       "      <td>236.000</td>\n",
       "      <td>38.549656</td>\n",
       "      <td>18.153846</td>\n",
       "      <td>4.307692</td>\n",
       "      <td>39.226537</td>\n",
       "      <td>11.588092</td>\n",
       "      <td>2.743846</td>\n",
       "      <td>1.734615</td>\n",
       "      <td>0.910769</td>\n",
       "      <td>0.481472</td>\n",
       "      <td>2.965358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>Ba3(VO4)2)</td>\n",
       "      <td>hexagonal</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>(Cd0.02Zn0.98)4O(BO2)6</td>\n",
       "      <td>orthorhombic</td>\n",
       "      <td>7.00</td>\n",
       "      <td>255.440</td>\n",
       "      <td>42.890676</td>\n",
       "      <td>11.106087</td>\n",
       "      <td>4.521739</td>\n",
       "      <td>23.395918</td>\n",
       "      <td>11.494264</td>\n",
       "      <td>2.763617</td>\n",
       "      <td>1.710157</td>\n",
       "      <td>0.790261</td>\n",
       "      <td>0.482938</td>\n",
       "      <td>1.864812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>Li2GeO3</td>\n",
       "      <td>orthorhombic</td>\n",
       "      <td>4.00</td>\n",
       "      <td>62.000</td>\n",
       "      <td>6.394996</td>\n",
       "      <td>10.333333</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>22.417182</td>\n",
       "      <td>9.922850</td>\n",
       "      <td>2.381667</td>\n",
       "      <td>1.718333</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.467532</td>\n",
       "      <td>1.065833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>SrO(B2O3)2</td>\n",
       "      <td>orthorhombic</td>\n",
       "      <td>9.00</td>\n",
       "      <td>114.000</td>\n",
       "      <td>12.029324</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>20.236434</td>\n",
       "      <td>11.184467</td>\n",
       "      <td>2.765833</td>\n",
       "      <td>1.734167</td>\n",
       "      <td>0.811667</td>\n",
       "      <td>0.481969</td>\n",
       "      <td>1.002444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>AlPO4</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>6.00</td>\n",
       "      <td>60.000</td>\n",
       "      <td>4.904328</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>20.325237</td>\n",
       "      <td>11.824150</td>\n",
       "      <td>2.926667</td>\n",
       "      <td>1.620000</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.494362</td>\n",
       "      <td>0.817388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>LiNb3O8</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>5.50</td>\n",
       "      <td>190.000</td>\n",
       "      <td>26.254656</td>\n",
       "      <td>15.833333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>34.470779</td>\n",
       "      <td>11.217767</td>\n",
       "      <td>2.775000</td>\n",
       "      <td>1.710000</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.479689</td>\n",
       "      <td>2.187888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>InBO3</td>\n",
       "      <td>rhomohedral</td>\n",
       "      <td>6.00</td>\n",
       "      <td>78.000</td>\n",
       "      <td>9.683996</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>34.724218</td>\n",
       "      <td>10.987740</td>\n",
       "      <td>2.828000</td>\n",
       "      <td>1.682000</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.477854</td>\n",
       "      <td>1.936799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>Nd0.02Ca8.53K1.09La0.95(VO4)7</td>\n",
       "      <td>trigonal</td>\n",
       "      <td>4.50</td>\n",
       "      <td>631.660</td>\n",
       "      <td>62.952676</td>\n",
       "      <td>13.855231</td>\n",
       "      <td>4.432770</td>\n",
       "      <td>29.039063</td>\n",
       "      <td>10.765863</td>\n",
       "      <td>2.593148</td>\n",
       "      <td>1.801011</td>\n",
       "      <td>1.028796</td>\n",
       "      <td>0.490143</td>\n",
       "      <td>1.380844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>Nd0.02YCa4O(BO3)3</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>6.50</td>\n",
       "      <td>215.200</td>\n",
       "      <td>17.930320</td>\n",
       "      <td>11.942286</td>\n",
       "      <td>4.386238</td>\n",
       "      <td>24.667646</td>\n",
       "      <td>10.646815</td>\n",
       "      <td>2.539556</td>\n",
       "      <td>1.807314</td>\n",
       "      <td>0.980999</td>\n",
       "      <td>0.490048</td>\n",
       "      <td>0.995023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>Nd0.02YAl3(BO3)4</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>8.00</td>\n",
       "      <td>195.200</td>\n",
       "      <td>22.199984</td>\n",
       "      <td>9.750250</td>\n",
       "      <td>4.747253</td>\n",
       "      <td>20.377006</td>\n",
       "      <td>11.033686</td>\n",
       "      <td>2.772867</td>\n",
       "      <td>1.688701</td>\n",
       "      <td>0.827053</td>\n",
       "      <td>0.486636</td>\n",
       "      <td>1.108890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>Mg2SiO4</td>\n",
       "      <td>orthorhombic</td>\n",
       "      <td>7.00</td>\n",
       "      <td>70.000</td>\n",
       "      <td>5.815328</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>20.098303</td>\n",
       "      <td>11.130929</td>\n",
       "      <td>2.611429</td>\n",
       "      <td>1.662857</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.498003</td>\n",
       "      <td>0.830761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>PbMoO4</td>\n",
       "      <td>tetragonal</td>\n",
       "      <td>3.00</td>\n",
       "      <td>156.000</td>\n",
       "      <td>21.575328</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>61.191020</td>\n",
       "      <td>11.496917</td>\n",
       "      <td>2.953333</td>\n",
       "      <td>1.711667</td>\n",
       "      <td>0.911667</td>\n",
       "      <td>0.472267</td>\n",
       "      <td>3.595888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>KPb2Cl5</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>2.50</td>\n",
       "      <td>268.000</td>\n",
       "      <td>23.576975</td>\n",
       "      <td>33.500000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>78.841037</td>\n",
       "      <td>10.501513</td>\n",
       "      <td>2.527500</td>\n",
       "      <td>1.942500</td>\n",
       "      <td>1.237500</td>\n",
       "      <td>0.459375</td>\n",
       "      <td>2.947122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>Pb2MoO5</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>3.00</td>\n",
       "      <td>246.000</td>\n",
       "      <td>32.926660</td>\n",
       "      <td>30.750000</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>73.793144</td>\n",
       "      <td>11.252038</td>\n",
       "      <td>2.870000</td>\n",
       "      <td>1.726250</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.466171</td>\n",
       "      <td>4.115832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>Mn2SiO4</td>\n",
       "      <td>orthorhombic</td>\n",
       "      <td>6.50</td>\n",
       "      <td>96.000</td>\n",
       "      <td>17.215328</td>\n",
       "      <td>13.714286</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>28.850887</td>\n",
       "      <td>11.070300</td>\n",
       "      <td>2.680000</td>\n",
       "      <td>1.754286</td>\n",
       "      <td>0.897143</td>\n",
       "      <td>0.486954</td>\n",
       "      <td>2.459333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>CaY2.5Er1.5Ho0.1(SiO4)3O</td>\n",
       "      <td>hexagonal</td>\n",
       "      <td>6.60</td>\n",
       "      <td>372.200</td>\n",
       "      <td>34.208316</td>\n",
       "      <td>17.639810</td>\n",
       "      <td>4.748815</td>\n",
       "      <td>38.955617</td>\n",
       "      <td>11.038415</td>\n",
       "      <td>2.675498</td>\n",
       "      <td>1.793128</td>\n",
       "      <td>0.981706</td>\n",
       "      <td>0.485396</td>\n",
       "      <td>1.621247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>PbGeO3</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>4.00</td>\n",
       "      <td>138.000</td>\n",
       "      <td>16.676996</td>\n",
       "      <td>27.600000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>65.565418</td>\n",
       "      <td>11.234080</td>\n",
       "      <td>2.826000</td>\n",
       "      <td>1.738000</td>\n",
       "      <td>0.914000</td>\n",
       "      <td>0.467304</td>\n",
       "      <td>3.335399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>Pb2FeTaO6</td>\n",
       "      <td>cubic</td>\n",
       "      <td>6.00</td>\n",
       "      <td>311.000</td>\n",
       "      <td>47.231992</td>\n",
       "      <td>31.100000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>74.718706</td>\n",
       "      <td>11.199400</td>\n",
       "      <td>2.757000</td>\n",
       "      <td>1.742000</td>\n",
       "      <td>0.956000</td>\n",
       "      <td>0.466061</td>\n",
       "      <td>4.723199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>Sb5O7I</td>\n",
       "      <td>monoclinic</td>\n",
       "      <td>4.00</td>\n",
       "      <td>364.000</td>\n",
       "      <td>38.394324</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>5.692308</td>\n",
       "      <td>65.207514</td>\n",
       "      <td>11.447692</td>\n",
       "      <td>2.845385</td>\n",
       "      <td>1.763077</td>\n",
       "      <td>0.987692</td>\n",
       "      <td>0.462479</td>\n",
       "      <td>2.953410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                        Formula Crystal structure  \\\n",
       "0            0                       MnTeMoO6        tetragonal   \n",
       "1            1                           MgH2        tetragonal   \n",
       "2            2                 CO(NH2)2C4H6O5        monoclinic   \n",
       "3            3                          GaPO4          trigonal   \n",
       "4            4                         Ga3PO7          trigonal   \n",
       "5            5                         LiB3O5      orthorhombic   \n",
       "6            6                         BiB3O6        monoclinic   \n",
       "7            7                      K2Al2B2O7          trigonal   \n",
       "8            8       Nd0.02GdLa0.16Ca4O(BO3)3        monoclinic   \n",
       "9            9       Nd0.02GdLa0.33Ca4O(BO3)3        monoclinic   \n",
       "10          10                           YVO4        tetragonal   \n",
       "11          11                         LiNbO3       rhomohedral   \n",
       "12          12                 Na3Ba2(B3O6)2F         hexagonal   \n",
       "13          13                   NdCa4O(BO3)3        monoclinic   \n",
       "14          14                    LuAl3(BO3)4          trigonal   \n",
       "15          15                          BaWF8               NaN   \n",
       "16          16                     Na5B2PeO13        monoclinic   \n",
       "17          17         Nd0.02Sr2La0.667(VO4)2         hexagonal   \n",
       "18          18                 Cr0.02CaGdAlO4        tetragonal   \n",
       "19          19        Y0.57La0.72Sc2.71(BO3)4          trigonal   \n",
       "20          20                     YCa9(VO4)7       rhomohedral   \n",
       "21          21                        Zn3BPO7         hexagonal   \n",
       "22          22                Nd0.02BaLaLiWO6             cubic   \n",
       "23          23               Nd0.02YCa9(VO4)7          trigonal   \n",
       "24          24             Nd30.02NaGd(MoO4)2        tetragonal   \n",
       "25          25                    La2CaB10O19        monoclinic   \n",
       "26          26                      BaNaB9O15          trigonal   \n",
       "27          27                         BaBPO5          trigonal   \n",
       "28          28                       Y3Al5O12             cubic   \n",
       "29          29              Nd0.02Ca9Gd(VO4)7         hexagonal   \n",
       "30          30                       BaCaBO3F         hexagonal   \n",
       "31          31                        MgAl2O4             cubic   \n",
       "32          32                       Zn3Nb2O8        tetragonal   \n",
       "33          33                     Ba3(VO4)2)         hexagonal   \n",
       "34          34         (Cd0.02Zn0.98)4O(BO2)6      orthorhombic   \n",
       "35          35                        Li2GeO3      orthorhombic   \n",
       "36          36                     SrO(B2O3)2      orthorhombic   \n",
       "37          37                          AlPO4        monoclinic   \n",
       "38          38                        LiNb3O8        monoclinic   \n",
       "39          39                          InBO3       rhomohedral   \n",
       "40          40  Nd0.02Ca8.53K1.09La0.95(VO4)7          trigonal   \n",
       "41          41              Nd0.02YCa4O(BO3)3        monoclinic   \n",
       "42          42               Nd0.02YAl3(BO3)4        monoclinic   \n",
       "43          43                        Mg2SiO4      orthorhombic   \n",
       "44          44                         PbMoO4        tetragonal   \n",
       "45          45                        KPb2Cl5        monoclinic   \n",
       "46          46                        Pb2MoO5        monoclinic   \n",
       "47          47                        Mn2SiO4      orthorhombic   \n",
       "48          48       CaY2.5Er1.5Ho0.1(SiO4)3O         hexagonal   \n",
       "49          49                         PbGeO3        monoclinic   \n",
       "50          50                      Pb2FeTaO6             cubic   \n",
       "51          51                         Sb5O7I        monoclinic   \n",
       "\n",
       "    Hardness (Mohs)  allelectrons_Total  density_Total  allelectrons_Average  \\\n",
       "0              4.50             167.000      23.907992             18.555556   \n",
       "1              4.00              14.000       1.740168              4.666667   \n",
       "2              2.50             102.000       8.511159              4.434783   \n",
       "3              5.50              78.000       8.109328             13.000000   \n",
       "4              6.50             164.000      19.921324             14.909091   \n",
       "5              6.50              58.000       7.650660              6.444444   \n",
       "6              5.75             146.000      16.864992             14.600000   \n",
       "7              6.00             130.000      11.871324             10.000000   \n",
       "8              6.42             249.320      22.345960             13.713971   \n",
       "9              6.35             259.010      23.392140             14.114986   \n",
       "10             5.00              94.000      10.584328             15.666667   \n",
       "11             5.00              68.000       9.107996             13.600000   \n",
       "12             4.50             280.000      24.150564             11.666667   \n",
       "13             5.40             235.000      20.223320             13.055556   \n",
       "14             8.10             226.000      27.432984             11.300000   \n",
       "15             3.00             202.000      22.812640             20.200000   \n",
       "16             3.00               0.000       0.000000              0.000000   \n",
       "17             5.00             225.219      21.553374             17.751951   \n",
       "18             6.00             129.480      12.297928             18.444444   \n",
       "19             6.50             236.180      24.574384             11.809000   \n",
       "20             4.50             604.000      61.226296             13.422222   \n",
       "21             5.00             166.000      25.978324             13.833333   \n",
       "22             5.50             239.200      29.633992             23.872255   \n",
       "23             4.50             605.200      61.364296             13.442914   \n",
       "24             4.70            2024.200     236.459656             48.172299   \n",
       "25             6.50             336.000      37.583308             10.500000   \n",
       "26             6.00             232.000      25.820980              8.923077   \n",
       "27             6.50             116.000       8.076660             14.500000   \n",
       "28             8.50             278.000      26.917984             13.900000   \n",
       "29             4.00             630.200      64.795296             13.998223   \n",
       "30             5.00             114.000       7.425576             16.285714   \n",
       "31             8.00              70.000       7.143328             10.000000   \n",
       "32             5.50             236.000      38.549656             18.153846   \n",
       "33             4.50               0.000       0.000000              0.000000   \n",
       "34             7.00             255.440      42.890676             11.106087   \n",
       "35             4.00              62.000       6.394996             10.333333   \n",
       "36             9.00             114.000      12.029324              9.500000   \n",
       "37             6.00              60.000       4.904328             10.000000   \n",
       "38             5.50             190.000      26.254656             15.833333   \n",
       "39             6.00              78.000       9.683996             15.600000   \n",
       "40             4.50             631.660      62.952676             13.855231   \n",
       "41             6.50             215.200      17.930320             11.942286   \n",
       "42             8.00             195.200      22.199984              9.750250   \n",
       "43             7.00              70.000       5.815328             10.000000   \n",
       "44             3.00             156.000      21.575328             26.000000   \n",
       "45             2.50             268.000      23.576975             33.500000   \n",
       "46             3.00             246.000      32.926660             30.750000   \n",
       "47             6.50              96.000      17.215328             13.714286   \n",
       "48             6.60             372.200      34.208316             17.639810   \n",
       "49             4.00             138.000      16.676996             27.600000   \n",
       "50             6.00             311.000      47.231992             31.100000   \n",
       "51             4.00             364.000      38.394324             28.000000   \n",
       "\n",
       "    val_e_Average  atomicweight_Average  ionenergy_Average  \\\n",
       "0        5.000000             41.609136          11.693844   \n",
       "1        1.333333              8.773227          11.614333   \n",
       "2        3.304348              8.440584          13.176622   \n",
       "3        5.333333             27.448814          11.826400   \n",
       "4        5.090909             32.012361          11.255573   \n",
       "5        4.444444             13.261239          10.930689   \n",
       "6        5.000000             33.739258          11.388810   \n",
       "7        4.307692             20.443453          10.198131   \n",
       "8        4.365237             29.432344          10.598482   \n",
       "9        4.343324             30.446537          10.551961   \n",
       "10       4.666667             33.973910          11.239317   \n",
       "11       4.000000             29.568292          10.600980   \n",
       "12       4.333333             25.810253          10.686196   \n",
       "13       4.388889             27.609017          10.614044   \n",
       "14       4.750000             24.556189          10.999625   \n",
       "15       6.000000             47.315423          15.245810   \n",
       "16       0.000000              0.000000           0.000000   \n",
       "17       4.522267             39.461656          10.850263   \n",
       "18       4.418803             41.217319          10.378423   \n",
       "19       4.600000             25.386557          11.097505   \n",
       "20       4.488889             27.870471          10.883696   \n",
       "21       4.666667             29.159414          11.857833   \n",
       "22       4.295409             56.475997          10.565210   \n",
       "23       4.487783             27.922168          10.881315   \n",
       "24       2.690148            114.951853           7.146097   \n",
       "25       4.687500             22.810328          11.218466   \n",
       "26       4.615385             19.136778          11.127088   \n",
       "27       5.000000             32.387739          11.510863   \n",
       "28       4.650000             29.680679          10.599905   \n",
       "29       4.487783             29.440253          10.879816   \n",
       "30       4.571429             36.458070          11.128571   \n",
       "31       4.571429             20.323314          10.584314   \n",
       "32       4.307692             39.226537          11.588092   \n",
       "33       0.000000              0.000000           0.000000   \n",
       "34       4.521739             23.395918          11.494264   \n",
       "35       4.000000             22.417182           9.922850   \n",
       "36       4.666667             20.236434          11.184467   \n",
       "37       5.333333             20.325237          11.824150   \n",
       "38       4.333333             34.470779          11.217767   \n",
       "39       4.800000             34.724218          10.987740   \n",
       "40       4.432770             29.039063          10.765863   \n",
       "41       4.386238             24.667646          10.646815   \n",
       "42       4.747253             20.377006          11.033686   \n",
       "43       4.571429             20.098303          11.130929   \n",
       "44       4.833333             61.191020          11.496917   \n",
       "45       5.500000             78.841037          10.501513   \n",
       "46       4.875000             73.793144          11.252038   \n",
       "47       4.571429             28.850887          11.070300   \n",
       "48       4.748815             38.955617          11.038415   \n",
       "49       5.200000             65.565418          11.234080   \n",
       "50       4.800000             74.718706          11.199400   \n",
       "51       5.692308             65.207514          11.447692   \n",
       "\n",
       "    el_neg_chi_Average  R_vdw_element_Average  R_cov_element_Average  \\\n",
       "0             2.938889               1.711111               0.884444   \n",
       "1             1.903333               1.310000               0.680000   \n",
       "2             2.672609               1.379130               0.530870   \n",
       "3             2.960000               1.625000               0.813333   \n",
       "4             2.881818               1.640909               0.841818   \n",
       "5             2.700000               1.686667               0.780000   \n",
       "6             2.866000               1.695000               0.786000   \n",
       "7             2.540000               1.820000               0.972308   \n",
       "8             2.525787               1.813894               0.992739   \n",
       "9             2.512578               1.819602               1.001515   \n",
       "10            2.768333               1.745000               0.960000   \n",
       "11            2.580000               1.712000               0.956000   \n",
       "12            2.586250               1.808333               0.926667   \n",
       "13            2.536667               1.810556               0.986667   \n",
       "14            2.763500               1.684000               0.825000   \n",
       "15            3.443000               1.662000               0.836000   \n",
       "16            0.000000               0.000000               0.000000   \n",
       "17            2.635493               1.808829               1.035042   \n",
       "18            2.507578               1.796467               1.052137   \n",
       "19            2.730650               1.740925               0.887445   \n",
       "20            2.621111               1.781333               1.009333   \n",
       "21            2.771667               1.699167               0.834167   \n",
       "22            2.528224               1.824132               1.065629   \n",
       "23            2.620453               1.781604               1.009720   \n",
       "24            1.622865               2.209848               1.615840   \n",
       "25            2.780000               1.726562               0.818125   \n",
       "26            2.760769               1.731923               0.800769   \n",
       "27            2.790000               1.750000               0.898750   \n",
       "28            2.649500               1.720000               0.958000   \n",
       "29            2.620009               1.782048               1.011053   \n",
       "30            2.604286               1.848571               1.022857   \n",
       "31            2.612857               1.641429               0.920000   \n",
       "32            2.743846               1.734615               0.910769   \n",
       "33            0.000000               0.000000               0.000000   \n",
       "34            2.763617               1.710157               0.790261   \n",
       "35            2.381667               1.718333               0.953333   \n",
       "36            2.765833               1.734167               0.811667   \n",
       "37            2.926667               1.620000               0.815000   \n",
       "38            2.775000               1.710000               0.925000   \n",
       "39            2.828000               1.682000               0.836000   \n",
       "40            2.593148               1.801011               1.028796   \n",
       "41            2.539556               1.807314               0.980999   \n",
       "42            2.772867               1.688701               0.827053   \n",
       "43            2.611429               1.662857               0.928571   \n",
       "44            2.953333               1.711667               0.911667   \n",
       "45            2.527500               1.942500               1.237500   \n",
       "46            2.870000               1.726250               0.945000   \n",
       "47            2.680000               1.754286               0.897143   \n",
       "48            2.675498               1.793128               0.981706   \n",
       "49            2.826000               1.738000               0.914000   \n",
       "50            2.757000               1.742000               0.956000   \n",
       "51            2.845385               1.763077               0.987692   \n",
       "\n",
       "    zaratio_Average  density_Average  \n",
       "0          0.477830         2.656444  \n",
       "1          0.825990         0.580056  \n",
       "2          0.713850         0.370050  \n",
       "3          0.488163         1.351555  \n",
       "4          0.483480         1.811029  \n",
       "5          0.479962         0.850073  \n",
       "6          0.478464         1.686499  \n",
       "7          0.489274         0.913179  \n",
       "8          0.487604         1.229151  \n",
       "9          0.486888         1.274776  \n",
       "10         0.481708         1.764055  \n",
       "11         0.474714         1.821599  \n",
       "12         0.479152         1.006274  \n",
       "13         0.488869         1.123518  \n",
       "14         0.485063         1.371649  \n",
       "15         0.460005         2.281264  \n",
       "16         0.000000         0.000000  \n",
       "17         0.477068         1.698855  \n",
       "18         0.483923         1.751842  \n",
       "19         0.483071         1.228719  \n",
       "20         0.490911         1.360584  \n",
       "21         0.485270         2.164860  \n",
       "22         0.465199         2.957484  \n",
       "23         0.490878         1.363045  \n",
       "24         0.434283         5.627312  \n",
       "25         0.482644         1.174478  \n",
       "26         0.482639         0.993115  \n",
       "27         0.481827         1.009583  \n",
       "28         0.486265         1.345899  \n",
       "29         0.490174         1.439256  \n",
       "30         0.477579         1.060797  \n",
       "31         0.493919         1.020475  \n",
       "32         0.481472         2.965358  \n",
       "33         0.000000         0.000000  \n",
       "34         0.482938         1.864812  \n",
       "35         0.467532         1.065833  \n",
       "36         0.481969         1.002444  \n",
       "37         0.494362         0.817388  \n",
       "38         0.479689         2.187888  \n",
       "39         0.477854         1.936799  \n",
       "40         0.490143         1.380844  \n",
       "41         0.490048         0.995023  \n",
       "42         0.486636         1.108890  \n",
       "43         0.498003         0.830761  \n",
       "44         0.472267         3.595888  \n",
       "45         0.459375         2.947122  \n",
       "46         0.466171         4.115832  \n",
       "47         0.486954         2.459333  \n",
       "48         0.485396         1.621247  \n",
       "49         0.467304         3.335399  \n",
       "50         0.466061         4.723199  \n",
       "51         0.462479         2.953410  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mostrando o Dataframe\n",
    "print(\"Dataset de teste:\")\n",
    "display(df_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamento do Datafram de teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os Dataframes prontos, foi retirado as 3 colunas que só faziam parte do dataset de teste, sendo as referentes a identificação de cada mineral:\n",
    "\n",
    "- 'Unnamed: 0'\n",
    "- 'Crysral structure'\n",
    "- 'Formula'\n",
    "\n",
    "Seguido pela renomeação da coluna do target, a qual passou de 'Hardness (Mohs)' para somente 'Hardness', isso foi feito visando agilizar a produção do código.\n",
    "\n",
    "Ao fim, foi aplicado a função dropna() para retirar quaisquer valores nan que pudessem existir e foi freita a redefinição do index do Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hardness</th>\n",
       "      <th>allelectrons_Total</th>\n",
       "      <th>density_Total</th>\n",
       "      <th>allelectrons_Average</th>\n",
       "      <th>val_e_Average</th>\n",
       "      <th>atomicweight_Average</th>\n",
       "      <th>ionenergy_Average</th>\n",
       "      <th>el_neg_chi_Average</th>\n",
       "      <th>R_vdw_element_Average</th>\n",
       "      <th>R_cov_element_Average</th>\n",
       "      <th>zaratio_Average</th>\n",
       "      <th>density_Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.50</td>\n",
       "      <td>167.000</td>\n",
       "      <td>23.907992</td>\n",
       "      <td>18.555556</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>41.609136</td>\n",
       "      <td>11.693844</td>\n",
       "      <td>2.938889</td>\n",
       "      <td>1.711111</td>\n",
       "      <td>0.884444</td>\n",
       "      <td>0.477830</td>\n",
       "      <td>2.656444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.00</td>\n",
       "      <td>14.000</td>\n",
       "      <td>1.740168</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>8.773227</td>\n",
       "      <td>11.614333</td>\n",
       "      <td>1.903333</td>\n",
       "      <td>1.310000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.825990</td>\n",
       "      <td>0.580056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.50</td>\n",
       "      <td>102.000</td>\n",
       "      <td>8.511159</td>\n",
       "      <td>4.434783</td>\n",
       "      <td>3.304348</td>\n",
       "      <td>8.440584</td>\n",
       "      <td>13.176622</td>\n",
       "      <td>2.672609</td>\n",
       "      <td>1.379130</td>\n",
       "      <td>0.530870</td>\n",
       "      <td>0.713850</td>\n",
       "      <td>0.370050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.50</td>\n",
       "      <td>78.000</td>\n",
       "      <td>8.109328</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>27.448814</td>\n",
       "      <td>11.826400</td>\n",
       "      <td>2.960000</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.488163</td>\n",
       "      <td>1.351555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.50</td>\n",
       "      <td>164.000</td>\n",
       "      <td>19.921324</td>\n",
       "      <td>14.909091</td>\n",
       "      <td>5.090909</td>\n",
       "      <td>32.012361</td>\n",
       "      <td>11.255573</td>\n",
       "      <td>2.881818</td>\n",
       "      <td>1.640909</td>\n",
       "      <td>0.841818</td>\n",
       "      <td>0.483480</td>\n",
       "      <td>1.811029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.50</td>\n",
       "      <td>58.000</td>\n",
       "      <td>7.650660</td>\n",
       "      <td>6.444444</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>13.261239</td>\n",
       "      <td>10.930689</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>1.686667</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.479962</td>\n",
       "      <td>0.850073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.75</td>\n",
       "      <td>146.000</td>\n",
       "      <td>16.864992</td>\n",
       "      <td>14.600000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>33.739258</td>\n",
       "      <td>11.388810</td>\n",
       "      <td>2.866000</td>\n",
       "      <td>1.695000</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.478464</td>\n",
       "      <td>1.686499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.00</td>\n",
       "      <td>130.000</td>\n",
       "      <td>11.871324</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.307692</td>\n",
       "      <td>20.443453</td>\n",
       "      <td>10.198131</td>\n",
       "      <td>2.540000</td>\n",
       "      <td>1.820000</td>\n",
       "      <td>0.972308</td>\n",
       "      <td>0.489274</td>\n",
       "      <td>0.913179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.42</td>\n",
       "      <td>249.320</td>\n",
       "      <td>22.345960</td>\n",
       "      <td>13.713971</td>\n",
       "      <td>4.365237</td>\n",
       "      <td>29.432344</td>\n",
       "      <td>10.598482</td>\n",
       "      <td>2.525787</td>\n",
       "      <td>1.813894</td>\n",
       "      <td>0.992739</td>\n",
       "      <td>0.487604</td>\n",
       "      <td>1.229151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.35</td>\n",
       "      <td>259.010</td>\n",
       "      <td>23.392140</td>\n",
       "      <td>14.114986</td>\n",
       "      <td>4.343324</td>\n",
       "      <td>30.446537</td>\n",
       "      <td>10.551961</td>\n",
       "      <td>2.512578</td>\n",
       "      <td>1.819602</td>\n",
       "      <td>1.001515</td>\n",
       "      <td>0.486888</td>\n",
       "      <td>1.274776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.00</td>\n",
       "      <td>94.000</td>\n",
       "      <td>10.584328</td>\n",
       "      <td>15.666667</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>33.973910</td>\n",
       "      <td>11.239317</td>\n",
       "      <td>2.768333</td>\n",
       "      <td>1.745000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.481708</td>\n",
       "      <td>1.764055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.00</td>\n",
       "      <td>68.000</td>\n",
       "      <td>9.107996</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>29.568292</td>\n",
       "      <td>10.600980</td>\n",
       "      <td>2.580000</td>\n",
       "      <td>1.712000</td>\n",
       "      <td>0.956000</td>\n",
       "      <td>0.474714</td>\n",
       "      <td>1.821599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.50</td>\n",
       "      <td>280.000</td>\n",
       "      <td>24.150564</td>\n",
       "      <td>11.666667</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>25.810253</td>\n",
       "      <td>10.686196</td>\n",
       "      <td>2.586250</td>\n",
       "      <td>1.808333</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.479152</td>\n",
       "      <td>1.006274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.40</td>\n",
       "      <td>235.000</td>\n",
       "      <td>20.223320</td>\n",
       "      <td>13.055556</td>\n",
       "      <td>4.388889</td>\n",
       "      <td>27.609017</td>\n",
       "      <td>10.614044</td>\n",
       "      <td>2.536667</td>\n",
       "      <td>1.810556</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.488869</td>\n",
       "      <td>1.123518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8.10</td>\n",
       "      <td>226.000</td>\n",
       "      <td>27.432984</td>\n",
       "      <td>11.300000</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>24.556189</td>\n",
       "      <td>10.999625</td>\n",
       "      <td>2.763500</td>\n",
       "      <td>1.684000</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.485063</td>\n",
       "      <td>1.371649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.00</td>\n",
       "      <td>202.000</td>\n",
       "      <td>22.812640</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>47.315423</td>\n",
       "      <td>15.245810</td>\n",
       "      <td>3.443000</td>\n",
       "      <td>1.662000</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.460005</td>\n",
       "      <td>2.281264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.00</td>\n",
       "      <td>225.219</td>\n",
       "      <td>21.553374</td>\n",
       "      <td>17.751951</td>\n",
       "      <td>4.522267</td>\n",
       "      <td>39.461656</td>\n",
       "      <td>10.850263</td>\n",
       "      <td>2.635493</td>\n",
       "      <td>1.808829</td>\n",
       "      <td>1.035042</td>\n",
       "      <td>0.477068</td>\n",
       "      <td>1.698855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.00</td>\n",
       "      <td>129.480</td>\n",
       "      <td>12.297928</td>\n",
       "      <td>18.444444</td>\n",
       "      <td>4.418803</td>\n",
       "      <td>41.217319</td>\n",
       "      <td>10.378423</td>\n",
       "      <td>2.507578</td>\n",
       "      <td>1.796467</td>\n",
       "      <td>1.052137</td>\n",
       "      <td>0.483923</td>\n",
       "      <td>1.751842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.50</td>\n",
       "      <td>236.180</td>\n",
       "      <td>24.574384</td>\n",
       "      <td>11.809000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>25.386557</td>\n",
       "      <td>11.097505</td>\n",
       "      <td>2.730650</td>\n",
       "      <td>1.740925</td>\n",
       "      <td>0.887445</td>\n",
       "      <td>0.483071</td>\n",
       "      <td>1.228719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.50</td>\n",
       "      <td>604.000</td>\n",
       "      <td>61.226296</td>\n",
       "      <td>13.422222</td>\n",
       "      <td>4.488889</td>\n",
       "      <td>27.870471</td>\n",
       "      <td>10.883696</td>\n",
       "      <td>2.621111</td>\n",
       "      <td>1.781333</td>\n",
       "      <td>1.009333</td>\n",
       "      <td>0.490911</td>\n",
       "      <td>1.360584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.00</td>\n",
       "      <td>166.000</td>\n",
       "      <td>25.978324</td>\n",
       "      <td>13.833333</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>29.159414</td>\n",
       "      <td>11.857833</td>\n",
       "      <td>2.771667</td>\n",
       "      <td>1.699167</td>\n",
       "      <td>0.834167</td>\n",
       "      <td>0.485270</td>\n",
       "      <td>2.164860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.50</td>\n",
       "      <td>239.200</td>\n",
       "      <td>29.633992</td>\n",
       "      <td>23.872255</td>\n",
       "      <td>4.295409</td>\n",
       "      <td>56.475997</td>\n",
       "      <td>10.565210</td>\n",
       "      <td>2.528224</td>\n",
       "      <td>1.824132</td>\n",
       "      <td>1.065629</td>\n",
       "      <td>0.465199</td>\n",
       "      <td>2.957484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.50</td>\n",
       "      <td>605.200</td>\n",
       "      <td>61.364296</td>\n",
       "      <td>13.442914</td>\n",
       "      <td>4.487783</td>\n",
       "      <td>27.922168</td>\n",
       "      <td>10.881315</td>\n",
       "      <td>2.620453</td>\n",
       "      <td>1.781604</td>\n",
       "      <td>1.009720</td>\n",
       "      <td>0.490878</td>\n",
       "      <td>1.363045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.70</td>\n",
       "      <td>2024.200</td>\n",
       "      <td>236.459656</td>\n",
       "      <td>48.172299</td>\n",
       "      <td>2.690148</td>\n",
       "      <td>114.951853</td>\n",
       "      <td>7.146097</td>\n",
       "      <td>1.622865</td>\n",
       "      <td>2.209848</td>\n",
       "      <td>1.615840</td>\n",
       "      <td>0.434283</td>\n",
       "      <td>5.627312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.50</td>\n",
       "      <td>336.000</td>\n",
       "      <td>37.583308</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>4.687500</td>\n",
       "      <td>22.810328</td>\n",
       "      <td>11.218466</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>1.726562</td>\n",
       "      <td>0.818125</td>\n",
       "      <td>0.482644</td>\n",
       "      <td>1.174478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6.00</td>\n",
       "      <td>232.000</td>\n",
       "      <td>25.820980</td>\n",
       "      <td>8.923077</td>\n",
       "      <td>4.615385</td>\n",
       "      <td>19.136778</td>\n",
       "      <td>11.127088</td>\n",
       "      <td>2.760769</td>\n",
       "      <td>1.731923</td>\n",
       "      <td>0.800769</td>\n",
       "      <td>0.482639</td>\n",
       "      <td>0.993115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6.50</td>\n",
       "      <td>116.000</td>\n",
       "      <td>8.076660</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>32.387739</td>\n",
       "      <td>11.510863</td>\n",
       "      <td>2.790000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.898750</td>\n",
       "      <td>0.481827</td>\n",
       "      <td>1.009583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8.50</td>\n",
       "      <td>278.000</td>\n",
       "      <td>26.917984</td>\n",
       "      <td>13.900000</td>\n",
       "      <td>4.650000</td>\n",
       "      <td>29.680679</td>\n",
       "      <td>10.599905</td>\n",
       "      <td>2.649500</td>\n",
       "      <td>1.720000</td>\n",
       "      <td>0.958000</td>\n",
       "      <td>0.486265</td>\n",
       "      <td>1.345899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.00</td>\n",
       "      <td>630.200</td>\n",
       "      <td>64.795296</td>\n",
       "      <td>13.998223</td>\n",
       "      <td>4.487783</td>\n",
       "      <td>29.440253</td>\n",
       "      <td>10.879816</td>\n",
       "      <td>2.620009</td>\n",
       "      <td>1.782048</td>\n",
       "      <td>1.011053</td>\n",
       "      <td>0.490174</td>\n",
       "      <td>1.439256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5.00</td>\n",
       "      <td>114.000</td>\n",
       "      <td>7.425576</td>\n",
       "      <td>16.285714</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>36.458070</td>\n",
       "      <td>11.128571</td>\n",
       "      <td>2.604286</td>\n",
       "      <td>1.848571</td>\n",
       "      <td>1.022857</td>\n",
       "      <td>0.477579</td>\n",
       "      <td>1.060797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8.00</td>\n",
       "      <td>70.000</td>\n",
       "      <td>7.143328</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>20.323314</td>\n",
       "      <td>10.584314</td>\n",
       "      <td>2.612857</td>\n",
       "      <td>1.641429</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.493919</td>\n",
       "      <td>1.020475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5.50</td>\n",
       "      <td>236.000</td>\n",
       "      <td>38.549656</td>\n",
       "      <td>18.153846</td>\n",
       "      <td>4.307692</td>\n",
       "      <td>39.226537</td>\n",
       "      <td>11.588092</td>\n",
       "      <td>2.743846</td>\n",
       "      <td>1.734615</td>\n",
       "      <td>0.910769</td>\n",
       "      <td>0.481472</td>\n",
       "      <td>2.965358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4.50</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>7.00</td>\n",
       "      <td>255.440</td>\n",
       "      <td>42.890676</td>\n",
       "      <td>11.106087</td>\n",
       "      <td>4.521739</td>\n",
       "      <td>23.395918</td>\n",
       "      <td>11.494264</td>\n",
       "      <td>2.763617</td>\n",
       "      <td>1.710157</td>\n",
       "      <td>0.790261</td>\n",
       "      <td>0.482938</td>\n",
       "      <td>1.864812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4.00</td>\n",
       "      <td>62.000</td>\n",
       "      <td>6.394996</td>\n",
       "      <td>10.333333</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>22.417182</td>\n",
       "      <td>9.922850</td>\n",
       "      <td>2.381667</td>\n",
       "      <td>1.718333</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.467532</td>\n",
       "      <td>1.065833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>9.00</td>\n",
       "      <td>114.000</td>\n",
       "      <td>12.029324</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>20.236434</td>\n",
       "      <td>11.184467</td>\n",
       "      <td>2.765833</td>\n",
       "      <td>1.734167</td>\n",
       "      <td>0.811667</td>\n",
       "      <td>0.481969</td>\n",
       "      <td>1.002444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>6.00</td>\n",
       "      <td>60.000</td>\n",
       "      <td>4.904328</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>20.325237</td>\n",
       "      <td>11.824150</td>\n",
       "      <td>2.926667</td>\n",
       "      <td>1.620000</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.494362</td>\n",
       "      <td>0.817388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>5.50</td>\n",
       "      <td>190.000</td>\n",
       "      <td>26.254656</td>\n",
       "      <td>15.833333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>34.470779</td>\n",
       "      <td>11.217767</td>\n",
       "      <td>2.775000</td>\n",
       "      <td>1.710000</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.479689</td>\n",
       "      <td>2.187888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>6.00</td>\n",
       "      <td>78.000</td>\n",
       "      <td>9.683996</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>34.724218</td>\n",
       "      <td>10.987740</td>\n",
       "      <td>2.828000</td>\n",
       "      <td>1.682000</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.477854</td>\n",
       "      <td>1.936799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4.50</td>\n",
       "      <td>631.660</td>\n",
       "      <td>62.952676</td>\n",
       "      <td>13.855231</td>\n",
       "      <td>4.432770</td>\n",
       "      <td>29.039063</td>\n",
       "      <td>10.765863</td>\n",
       "      <td>2.593148</td>\n",
       "      <td>1.801011</td>\n",
       "      <td>1.028796</td>\n",
       "      <td>0.490143</td>\n",
       "      <td>1.380844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>6.50</td>\n",
       "      <td>215.200</td>\n",
       "      <td>17.930320</td>\n",
       "      <td>11.942286</td>\n",
       "      <td>4.386238</td>\n",
       "      <td>24.667646</td>\n",
       "      <td>10.646815</td>\n",
       "      <td>2.539556</td>\n",
       "      <td>1.807314</td>\n",
       "      <td>0.980999</td>\n",
       "      <td>0.490048</td>\n",
       "      <td>0.995023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>8.00</td>\n",
       "      <td>195.200</td>\n",
       "      <td>22.199984</td>\n",
       "      <td>9.750250</td>\n",
       "      <td>4.747253</td>\n",
       "      <td>20.377006</td>\n",
       "      <td>11.033686</td>\n",
       "      <td>2.772867</td>\n",
       "      <td>1.688701</td>\n",
       "      <td>0.827053</td>\n",
       "      <td>0.486636</td>\n",
       "      <td>1.108890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>7.00</td>\n",
       "      <td>70.000</td>\n",
       "      <td>5.815328</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>20.098303</td>\n",
       "      <td>11.130929</td>\n",
       "      <td>2.611429</td>\n",
       "      <td>1.662857</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.498003</td>\n",
       "      <td>0.830761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3.00</td>\n",
       "      <td>156.000</td>\n",
       "      <td>21.575328</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>61.191020</td>\n",
       "      <td>11.496917</td>\n",
       "      <td>2.953333</td>\n",
       "      <td>1.711667</td>\n",
       "      <td>0.911667</td>\n",
       "      <td>0.472267</td>\n",
       "      <td>3.595888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2.50</td>\n",
       "      <td>268.000</td>\n",
       "      <td>23.576975</td>\n",
       "      <td>33.500000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>78.841037</td>\n",
       "      <td>10.501513</td>\n",
       "      <td>2.527500</td>\n",
       "      <td>1.942500</td>\n",
       "      <td>1.237500</td>\n",
       "      <td>0.459375</td>\n",
       "      <td>2.947122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3.00</td>\n",
       "      <td>246.000</td>\n",
       "      <td>32.926660</td>\n",
       "      <td>30.750000</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>73.793144</td>\n",
       "      <td>11.252038</td>\n",
       "      <td>2.870000</td>\n",
       "      <td>1.726250</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.466171</td>\n",
       "      <td>4.115832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>6.50</td>\n",
       "      <td>96.000</td>\n",
       "      <td>17.215328</td>\n",
       "      <td>13.714286</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>28.850887</td>\n",
       "      <td>11.070300</td>\n",
       "      <td>2.680000</td>\n",
       "      <td>1.754286</td>\n",
       "      <td>0.897143</td>\n",
       "      <td>0.486954</td>\n",
       "      <td>2.459333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>6.60</td>\n",
       "      <td>372.200</td>\n",
       "      <td>34.208316</td>\n",
       "      <td>17.639810</td>\n",
       "      <td>4.748815</td>\n",
       "      <td>38.955617</td>\n",
       "      <td>11.038415</td>\n",
       "      <td>2.675498</td>\n",
       "      <td>1.793128</td>\n",
       "      <td>0.981706</td>\n",
       "      <td>0.485396</td>\n",
       "      <td>1.621247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4.00</td>\n",
       "      <td>138.000</td>\n",
       "      <td>16.676996</td>\n",
       "      <td>27.600000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>65.565418</td>\n",
       "      <td>11.234080</td>\n",
       "      <td>2.826000</td>\n",
       "      <td>1.738000</td>\n",
       "      <td>0.914000</td>\n",
       "      <td>0.467304</td>\n",
       "      <td>3.335399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>6.00</td>\n",
       "      <td>311.000</td>\n",
       "      <td>47.231992</td>\n",
       "      <td>31.100000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>74.718706</td>\n",
       "      <td>11.199400</td>\n",
       "      <td>2.757000</td>\n",
       "      <td>1.742000</td>\n",
       "      <td>0.956000</td>\n",
       "      <td>0.466061</td>\n",
       "      <td>4.723199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>4.00</td>\n",
       "      <td>364.000</td>\n",
       "      <td>38.394324</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>5.692308</td>\n",
       "      <td>65.207514</td>\n",
       "      <td>11.447692</td>\n",
       "      <td>2.845385</td>\n",
       "      <td>1.763077</td>\n",
       "      <td>0.987692</td>\n",
       "      <td>0.462479</td>\n",
       "      <td>2.953410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Hardness  allelectrons_Total  density_Total  allelectrons_Average  \\\n",
       "0       4.50             167.000      23.907992             18.555556   \n",
       "1       4.00              14.000       1.740168              4.666667   \n",
       "2       2.50             102.000       8.511159              4.434783   \n",
       "3       5.50              78.000       8.109328             13.000000   \n",
       "4       6.50             164.000      19.921324             14.909091   \n",
       "5       6.50              58.000       7.650660              6.444444   \n",
       "6       5.75             146.000      16.864992             14.600000   \n",
       "7       6.00             130.000      11.871324             10.000000   \n",
       "8       6.42             249.320      22.345960             13.713971   \n",
       "9       6.35             259.010      23.392140             14.114986   \n",
       "10      5.00              94.000      10.584328             15.666667   \n",
       "11      5.00              68.000       9.107996             13.600000   \n",
       "12      4.50             280.000      24.150564             11.666667   \n",
       "13      5.40             235.000      20.223320             13.055556   \n",
       "14      8.10             226.000      27.432984             11.300000   \n",
       "15      3.00             202.000      22.812640             20.200000   \n",
       "16      3.00               0.000       0.000000              0.000000   \n",
       "17      5.00             225.219      21.553374             17.751951   \n",
       "18      6.00             129.480      12.297928             18.444444   \n",
       "19      6.50             236.180      24.574384             11.809000   \n",
       "20      4.50             604.000      61.226296             13.422222   \n",
       "21      5.00             166.000      25.978324             13.833333   \n",
       "22      5.50             239.200      29.633992             23.872255   \n",
       "23      4.50             605.200      61.364296             13.442914   \n",
       "24      4.70            2024.200     236.459656             48.172299   \n",
       "25      6.50             336.000      37.583308             10.500000   \n",
       "26      6.00             232.000      25.820980              8.923077   \n",
       "27      6.50             116.000       8.076660             14.500000   \n",
       "28      8.50             278.000      26.917984             13.900000   \n",
       "29      4.00             630.200      64.795296             13.998223   \n",
       "30      5.00             114.000       7.425576             16.285714   \n",
       "31      8.00              70.000       7.143328             10.000000   \n",
       "32      5.50             236.000      38.549656             18.153846   \n",
       "33      4.50               0.000       0.000000              0.000000   \n",
       "34      7.00             255.440      42.890676             11.106087   \n",
       "35      4.00              62.000       6.394996             10.333333   \n",
       "36      9.00             114.000      12.029324              9.500000   \n",
       "37      6.00              60.000       4.904328             10.000000   \n",
       "38      5.50             190.000      26.254656             15.833333   \n",
       "39      6.00              78.000       9.683996             15.600000   \n",
       "40      4.50             631.660      62.952676             13.855231   \n",
       "41      6.50             215.200      17.930320             11.942286   \n",
       "42      8.00             195.200      22.199984              9.750250   \n",
       "43      7.00              70.000       5.815328             10.000000   \n",
       "44      3.00             156.000      21.575328             26.000000   \n",
       "45      2.50             268.000      23.576975             33.500000   \n",
       "46      3.00             246.000      32.926660             30.750000   \n",
       "47      6.50              96.000      17.215328             13.714286   \n",
       "48      6.60             372.200      34.208316             17.639810   \n",
       "49      4.00             138.000      16.676996             27.600000   \n",
       "50      6.00             311.000      47.231992             31.100000   \n",
       "51      4.00             364.000      38.394324             28.000000   \n",
       "\n",
       "    val_e_Average  atomicweight_Average  ionenergy_Average  \\\n",
       "0        5.000000             41.609136          11.693844   \n",
       "1        1.333333              8.773227          11.614333   \n",
       "2        3.304348              8.440584          13.176622   \n",
       "3        5.333333             27.448814          11.826400   \n",
       "4        5.090909             32.012361          11.255573   \n",
       "5        4.444444             13.261239          10.930689   \n",
       "6        5.000000             33.739258          11.388810   \n",
       "7        4.307692             20.443453          10.198131   \n",
       "8        4.365237             29.432344          10.598482   \n",
       "9        4.343324             30.446537          10.551961   \n",
       "10       4.666667             33.973910          11.239317   \n",
       "11       4.000000             29.568292          10.600980   \n",
       "12       4.333333             25.810253          10.686196   \n",
       "13       4.388889             27.609017          10.614044   \n",
       "14       4.750000             24.556189          10.999625   \n",
       "15       6.000000             47.315423          15.245810   \n",
       "16       0.000000              0.000000           0.000000   \n",
       "17       4.522267             39.461656          10.850263   \n",
       "18       4.418803             41.217319          10.378423   \n",
       "19       4.600000             25.386557          11.097505   \n",
       "20       4.488889             27.870471          10.883696   \n",
       "21       4.666667             29.159414          11.857833   \n",
       "22       4.295409             56.475997          10.565210   \n",
       "23       4.487783             27.922168          10.881315   \n",
       "24       2.690148            114.951853           7.146097   \n",
       "25       4.687500             22.810328          11.218466   \n",
       "26       4.615385             19.136778          11.127088   \n",
       "27       5.000000             32.387739          11.510863   \n",
       "28       4.650000             29.680679          10.599905   \n",
       "29       4.487783             29.440253          10.879816   \n",
       "30       4.571429             36.458070          11.128571   \n",
       "31       4.571429             20.323314          10.584314   \n",
       "32       4.307692             39.226537          11.588092   \n",
       "33       0.000000              0.000000           0.000000   \n",
       "34       4.521739             23.395918          11.494264   \n",
       "35       4.000000             22.417182           9.922850   \n",
       "36       4.666667             20.236434          11.184467   \n",
       "37       5.333333             20.325237          11.824150   \n",
       "38       4.333333             34.470779          11.217767   \n",
       "39       4.800000             34.724218          10.987740   \n",
       "40       4.432770             29.039063          10.765863   \n",
       "41       4.386238             24.667646          10.646815   \n",
       "42       4.747253             20.377006          11.033686   \n",
       "43       4.571429             20.098303          11.130929   \n",
       "44       4.833333             61.191020          11.496917   \n",
       "45       5.500000             78.841037          10.501513   \n",
       "46       4.875000             73.793144          11.252038   \n",
       "47       4.571429             28.850887          11.070300   \n",
       "48       4.748815             38.955617          11.038415   \n",
       "49       5.200000             65.565418          11.234080   \n",
       "50       4.800000             74.718706          11.199400   \n",
       "51       5.692308             65.207514          11.447692   \n",
       "\n",
       "    el_neg_chi_Average  R_vdw_element_Average  R_cov_element_Average  \\\n",
       "0             2.938889               1.711111               0.884444   \n",
       "1             1.903333               1.310000               0.680000   \n",
       "2             2.672609               1.379130               0.530870   \n",
       "3             2.960000               1.625000               0.813333   \n",
       "4             2.881818               1.640909               0.841818   \n",
       "5             2.700000               1.686667               0.780000   \n",
       "6             2.866000               1.695000               0.786000   \n",
       "7             2.540000               1.820000               0.972308   \n",
       "8             2.525787               1.813894               0.992739   \n",
       "9             2.512578               1.819602               1.001515   \n",
       "10            2.768333               1.745000               0.960000   \n",
       "11            2.580000               1.712000               0.956000   \n",
       "12            2.586250               1.808333               0.926667   \n",
       "13            2.536667               1.810556               0.986667   \n",
       "14            2.763500               1.684000               0.825000   \n",
       "15            3.443000               1.662000               0.836000   \n",
       "16            0.000000               0.000000               0.000000   \n",
       "17            2.635493               1.808829               1.035042   \n",
       "18            2.507578               1.796467               1.052137   \n",
       "19            2.730650               1.740925               0.887445   \n",
       "20            2.621111               1.781333               1.009333   \n",
       "21            2.771667               1.699167               0.834167   \n",
       "22            2.528224               1.824132               1.065629   \n",
       "23            2.620453               1.781604               1.009720   \n",
       "24            1.622865               2.209848               1.615840   \n",
       "25            2.780000               1.726562               0.818125   \n",
       "26            2.760769               1.731923               0.800769   \n",
       "27            2.790000               1.750000               0.898750   \n",
       "28            2.649500               1.720000               0.958000   \n",
       "29            2.620009               1.782048               1.011053   \n",
       "30            2.604286               1.848571               1.022857   \n",
       "31            2.612857               1.641429               0.920000   \n",
       "32            2.743846               1.734615               0.910769   \n",
       "33            0.000000               0.000000               0.000000   \n",
       "34            2.763617               1.710157               0.790261   \n",
       "35            2.381667               1.718333               0.953333   \n",
       "36            2.765833               1.734167               0.811667   \n",
       "37            2.926667               1.620000               0.815000   \n",
       "38            2.775000               1.710000               0.925000   \n",
       "39            2.828000               1.682000               0.836000   \n",
       "40            2.593148               1.801011               1.028796   \n",
       "41            2.539556               1.807314               0.980999   \n",
       "42            2.772867               1.688701               0.827053   \n",
       "43            2.611429               1.662857               0.928571   \n",
       "44            2.953333               1.711667               0.911667   \n",
       "45            2.527500               1.942500               1.237500   \n",
       "46            2.870000               1.726250               0.945000   \n",
       "47            2.680000               1.754286               0.897143   \n",
       "48            2.675498               1.793128               0.981706   \n",
       "49            2.826000               1.738000               0.914000   \n",
       "50            2.757000               1.742000               0.956000   \n",
       "51            2.845385               1.763077               0.987692   \n",
       "\n",
       "    zaratio_Average  density_Average  \n",
       "0          0.477830         2.656444  \n",
       "1          0.825990         0.580056  \n",
       "2          0.713850         0.370050  \n",
       "3          0.488163         1.351555  \n",
       "4          0.483480         1.811029  \n",
       "5          0.479962         0.850073  \n",
       "6          0.478464         1.686499  \n",
       "7          0.489274         0.913179  \n",
       "8          0.487604         1.229151  \n",
       "9          0.486888         1.274776  \n",
       "10         0.481708         1.764055  \n",
       "11         0.474714         1.821599  \n",
       "12         0.479152         1.006274  \n",
       "13         0.488869         1.123518  \n",
       "14         0.485063         1.371649  \n",
       "15         0.460005         2.281264  \n",
       "16         0.000000         0.000000  \n",
       "17         0.477068         1.698855  \n",
       "18         0.483923         1.751842  \n",
       "19         0.483071         1.228719  \n",
       "20         0.490911         1.360584  \n",
       "21         0.485270         2.164860  \n",
       "22         0.465199         2.957484  \n",
       "23         0.490878         1.363045  \n",
       "24         0.434283         5.627312  \n",
       "25         0.482644         1.174478  \n",
       "26         0.482639         0.993115  \n",
       "27         0.481827         1.009583  \n",
       "28         0.486265         1.345899  \n",
       "29         0.490174         1.439256  \n",
       "30         0.477579         1.060797  \n",
       "31         0.493919         1.020475  \n",
       "32         0.481472         2.965358  \n",
       "33         0.000000         0.000000  \n",
       "34         0.482938         1.864812  \n",
       "35         0.467532         1.065833  \n",
       "36         0.481969         1.002444  \n",
       "37         0.494362         0.817388  \n",
       "38         0.479689         2.187888  \n",
       "39         0.477854         1.936799  \n",
       "40         0.490143         1.380844  \n",
       "41         0.490048         0.995023  \n",
       "42         0.486636         1.108890  \n",
       "43         0.498003         0.830761  \n",
       "44         0.472267         3.595888  \n",
       "45         0.459375         2.947122  \n",
       "46         0.466171         4.115832  \n",
       "47         0.486954         2.459333  \n",
       "48         0.485396         1.621247  \n",
       "49         0.467304         3.335399  \n",
       "50         0.466061         4.723199  \n",
       "51         0.462479         2.953410  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Tratando os dados'''\n",
    "\n",
    "# Retirando colunas desinteressantes\n",
    "df_teste = df_teste.drop(['Unnamed: 0', 'Crystal structure', 'Formula'], axis=1)\n",
    "\n",
    "# Trocando o nome de uma coluna para facilitar o codigo\n",
    "df_teste.rename(columns={'Hardness (Mohs)': 'Hardness'}, inplace=True)\n",
    "\n",
    "# Retirando valores nan\n",
    "df_teste = df_teste.dropna()\n",
    "\n",
    "# Ajustando o index\n",
    "df_teste = df_teste.reset_index(drop=True)\n",
    "\n",
    "df_teste # Mostrando nova tabela"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentação dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa é uma etapa crucial para o treinamento e aplicação de rede neural treinada. \n",
    "\n",
    "Em primeiro lugar, foi definido as colunas das features e do target. Tendo sido retiradas 3 colunas dos dataframes devido a relevância nula das características representadas para a definição da dureza do material. Sendo elas:\n",
    "\n",
    "- 'allelectrons_Total'\n",
    "- 'R_vdw_element_Average'\n",
    "- 'zaratio_Average'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo Features\n",
    "FEATURES = [ 'density_Total',  'allelectrons_Average',\n",
    "            'val_e_Average',  'atomicweight_Average',  'ionenergy_Average',\n",
    "            'el_neg_chi_Average',  'R_cov_element_Average',\n",
    "             'density_Average']\n",
    "\n",
    "# Definindo Targets\n",
    "TARGET = 'Hardness'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, foi feita a definição das features e targets para treino e teste(famoso split de dados). Como já foi mencionado diversas vezes, existem dois datasets separados, um para treino e outro para teste. Logo, só foi feita a definição de listas contendo as features e target para treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Separando Features e Targets'''\n",
    "\n",
    "\n",
    "# Separando dados para o treino\n",
    "X_train = df_treino[FEATURES]\n",
    "Y_train = df_treino[TARGET]\n",
    "\n",
    "# Separando dados para teste\n",
    "X_test = df_teste[FEATURES]\n",
    "Y_test = df_teste[TARGET]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vale ressaltar que foi de suma importância a normalização dos dados para o uso das features sem erros, tendo sido utilizada a MinMaxScaler do módulo sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Normalizando os dados'''\n",
    "\n",
    "# Criando um objeto MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Ajuste o scaler aos dados e transforme-os\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o uso dos valores das do treino e teste pela rede neural, é necessário a transformação das listas de cada um em tensores pelo próprio PyTorch. Também, foi realizado o ajuste das dimensões do teste, já que as linhas e colunas estavam invertidas em relação ao treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Transformando em tensores'''\n",
    "\n",
    "# Tensores do treino\n",
    "X_train_scaled_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "\n",
    "# Tensores do teste\n",
    "X_test_scaled_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
    "\n",
    "# Ajustando dimensões dos targets\n",
    "Y_train_tensor_dimensionado = Y_train_tensor.view(-1,1)\n",
    "Y_test_tensor_dimensionado = Y_test_tensor.view(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Montando a Rede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente a montagem da rede neural por meio do PyTorch. Não vamos entrar em detalhes no funcionamento de uma rede neural, para isso recomendamos a leitura das referências utilizadas para o projeto, porém vale ressaltar as características específicas da rede em questão.\n",
    "\n",
    "Foi utilizado o método de dropout e momento com o intuito da melhora da performace da rede neural, além disso foi utilizado SGD para otimizar os parâmetros da rede considerando o momento. Por fim, foi definido o MSE como função de perda e foi definido que a cada 100 épocas é mostrado a perda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Redes Neural'''\n",
    "\n",
    "class NeuralNetworkRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, learning_rate=0.1, dropout_prob=0.3, momentum=0.8):\n",
    "        super(NeuralNetworkRegressor, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size1 = hidden_size1\n",
    "        self.hidden_size2 = hidden_size2\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Primeira camada oculta\n",
    "        self.hidden_layer1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.hidden_activation1 = nn.ReLU()\n",
    "        \n",
    "        # Segunda camada oculta\n",
    "        self.hidden_layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.hidden_activation2 = nn.ReLU()\n",
    "        \n",
    "        # Camada de saída\n",
    "        self.output_layer = nn.Linear(hidden_size2, 1)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "        # Otimizador com Momento\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        \n",
    "        # Função de perda (Mean Squared Error)\n",
    "        self.criterion = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Primeira camada oculta\n",
    "        hidden_output1 = self.hidden_layer1(x)\n",
    "        hidden_output1 = self.hidden_activation1(hidden_output1)\n",
    "        \n",
    "        # Dropout (apenas durante o treinamento)\n",
    "        if self.training:\n",
    "            hidden_output1 = self.dropout(hidden_output1)\n",
    "        \n",
    "        # Segunda camada oculta\n",
    "        hidden_output2 = self.hidden_layer2(hidden_output1)\n",
    "        hidden_output2 = self.hidden_activation2(hidden_output2)\n",
    "        \n",
    "        # Dropout (apenas durante o treinamento)\n",
    "        if self.training:\n",
    "            hidden_output2 = self.dropout(hidden_output2)\n",
    "        \n",
    "        # Camada de saída\n",
    "        output = self.output_layer(hidden_output2)\n",
    "        return output\n",
    "    \n",
    "    def train_model(self, X_train, y_train, epochs, print_perda=False):\n",
    "        for epoch in range(epochs):\n",
    "            # Ativa o modo de treinamento\n",
    "            self.train()\n",
    "            \n",
    "            # Realiza a propagação direta\n",
    "            outputs = self(X_train)\n",
    "            \n",
    "            # Calcula a perda\n",
    "            loss = self.criterion(outputs, y_train)\n",
    "            \n",
    "            # Retropropagação e atualização dos pesos\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            if print_perda:\n",
    "                # Mostra a perda a cada 100 épocas\n",
    "                if (epoch + 1) % 100 == 0:\n",
    "                    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}')\n",
    "            elif type(print_perda) != bool:\n",
    "                print(\"Variavel 'print_perda' com valor fora dos parâmetros\")\n",
    "            \n",
    "                \n",
    "    def predict(self, X, num_samples=10, ):\n",
    "        \"\"\"\n",
    "        Realiza a previsão com base nos dados de entrada fornecidos usando Monte Carlo Dropout.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): Os dados de entrada para prever.\n",
    "            num_samples (int): O número de amostras de dropout a serem usadas (default 10).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A média das previsões feitas com diferentes amostras de dropout.\n",
    "        \"\"\"\n",
    "        # Ativa o modo de avaliação\n",
    "        self.eval()\n",
    "        \n",
    "        # Lista para armazenar as previsões\n",
    "        self.predictions = []\n",
    "        \n",
    "        # Realiza múltiplas previsões com diferentes amostras de dropout\n",
    "        for _ in range(num_samples):\n",
    "            with torch.no_grad():\n",
    "                outputs = self(X)\n",
    "                self.predictions.append(outputs)\n",
    "        \n",
    "        # Calcula a média das previsões\n",
    "        predictions_mean = torch.stack(self.predictions).mean(dim=0)\n",
    "        return predictions_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando a rede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, realizando o treinamento da rede. Para isso, foi utilizado o optuna com o intuito de tetar diversas arquiteturas de rede e selecionar a melhor para aplicação no teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 07:49:25,533] A new study created in memory with name: no-name-8d3d654a-ad1b-416b-b03f-262c510f603b\n",
      "  0%|          | 0/149 [00:00<?, ?it/s][I 2024-05-07 07:49:30,651] Trial 0 finished with value: 1.685296893119812 and parameters: {'hidden_size1': 83, 'hidden_size2': 31, 'learning_rate': 0.009439955183208731, 'dropout_prob': 0.2901135163184969, 'momentum': 0.6636682432250468}. Best is trial 0 with value: 1.685296893119812.\n",
      "  1%|          | 1/149 [00:05<12:35,  5.10s/it][I 2024-05-07 07:49:35,250] Trial 1 finished with value: 2.975376605987549 and parameters: {'hidden_size1': 26, 'hidden_size2': 64, 'learning_rate': 0.0010264434414417735, 'dropout_prob': 0.31479159559686376, 'momentum': 0.3242777118732303}. Best is trial 0 with value: 1.685296893119812.\n",
      "  1%|▏         | 2/149 [00:09<11:46,  4.81s/it][I 2024-05-07 07:49:42,185] Trial 2 finished with value: 2.021578311920166 and parameters: {'hidden_size1': 128, 'hidden_size2': 105, 'learning_rate': 0.003948890754103403, 'dropout_prob': 0.062010324532398964, 'momentum': 0.3364480137467728}. Best is trial 0 with value: 1.685296893119812.\n",
      "  2%|▏         | 3/149 [00:16<14:03,  5.78s/it][I 2024-05-07 07:49:47,779] Trial 3 finished with value: 1.889050841331482 and parameters: {'hidden_size1': 38, 'hidden_size2': 85, 'learning_rate': 0.0001836707129971158, 'dropout_prob': 0.29868141947054494, 'momentum': 0.9747626785454513}. Best is trial 0 with value: 1.685296893119812.\n",
      "  3%|▎         | 4/149 [00:22<13:47,  5.71s/it][I 2024-05-07 07:49:58,884] Trial 4 finished with value: 2.3721628189086914 and parameters: {'hidden_size1': 186, 'hidden_size2': 47, 'learning_rate': 0.0021882973635601348, 'dropout_prob': 0.30134013065811865, 'momentum': 0.2739426576164241}. Best is trial 0 with value: 1.685296893119812.\n",
      "  3%|▎         | 5/149 [00:33<18:22,  7.65s/it][I 2024-05-07 07:50:06,408] Trial 5 finished with value: 2.170720100402832 and parameters: {'hidden_size1': 201, 'hidden_size2': 20, 'learning_rate': 0.08284859032804869, 'dropout_prob': 0.05619849785310782, 'momentum': 0.254743623982311}. Best is trial 0 with value: 1.685296893119812.\n",
      "  4%|▍         | 6/149 [00:40<18:08,  7.61s/it][I 2024-05-07 07:50:13,643] Trial 6 finished with value: 3.171863317489624 and parameters: {'hidden_size1': 92, 'hidden_size2': 51, 'learning_rate': 5.32098899003201e-05, 'dropout_prob': 0.08648609613733305, 'momentum': 0.46891967832217396}. Best is trial 0 with value: 1.685296893119812.\n",
      "  5%|▍         | 7/149 [00:48<17:43,  7.49s/it][I 2024-05-07 07:50:20,348] Trial 7 finished with value: 1.716651201248169 and parameters: {'hidden_size1': 20, 'hidden_size2': 141, 'learning_rate': 0.009313551707468741, 'dropout_prob': 0.4677537869533134, 'momentum': 0.6287751548107143}. Best is trial 0 with value: 1.685296893119812.\n",
      "  5%|▌         | 8/149 [00:54<17:00,  7.24s/it][I 2024-05-07 07:50:29,248] Trial 8 finished with value: 2.566044330596924 and parameters: {'hidden_size1': 80, 'hidden_size2': 208, 'learning_rate': 0.0954950717400573, 'dropout_prob': 0.28670247477383815, 'momentum': 0.8273240189159278}. Best is trial 0 with value: 1.685296893119812.\n",
      "  6%|▌         | 9/149 [01:03<18:06,  7.76s/it][I 2024-05-07 07:50:36,630] Trial 9 finished with value: 3.9880292415618896 and parameters: {'hidden_size1': 18, 'hidden_size2': 195, 'learning_rate': 2.0413295962182546e-05, 'dropout_prob': 0.3978308633078822, 'momentum': 0.7722414660223931}. Best is trial 0 with value: 1.685296893119812.\n",
      "  7%|▋         | 10/149 [01:11<17:42,  7.64s/it][I 2024-05-07 07:50:42,609] Trial 10 finished with value: 1.4692115783691406 and parameters: {'hidden_size1': 50, 'hidden_size2': 23, 'learning_rate': 0.013402355383979856, 'dropout_prob': 0.18944468604506143, 'momentum': 0.026495626409611894}. Best is trial 10 with value: 1.4692115783691406.\n",
      "  7%|▋         | 11/149 [01:17<16:24,  7.13s/it][I 2024-05-07 07:50:48,841] Trial 11 finished with value: 1.498764157295227 and parameters: {'hidden_size1': 48, 'hidden_size2': 23, 'learning_rate': 0.01533317345275496, 'dropout_prob': 0.17889187111132407, 'momentum': 0.059920421903401085}. Best is trial 10 with value: 1.4692115783691406.\n",
      "  8%|▊         | 12/149 [01:23<15:39,  6.86s/it][I 2024-05-07 07:50:55,228] Trial 12 finished with value: 1.4674803018569946 and parameters: {'hidden_size1': 47, 'hidden_size2': 16, 'learning_rate': 0.033989337708765416, 'dropout_prob': 0.15660241700345623, 'momentum': 0.031287578924906925}. Best is trial 12 with value: 1.4674803018569946.\n",
      "  9%|▊         | 13/149 [01:29<15:13,  6.72s/it][I 2024-05-07 07:51:01,060] Trial 13 finished with value: 1.539833664894104 and parameters: {'hidden_size1': 41, 'hidden_size2': 16, 'learning_rate': 0.029923000816827233, 'dropout_prob': 0.16461862491983087, 'momentum': 0.03244618562199042}. Best is trial 12 with value: 1.4674803018569946.\n",
      "  9%|▉         | 14/149 [01:35<14:30,  6.45s/it][I 2024-05-07 07:51:07,170] Trial 14 finished with value: 1.4775646924972534 and parameters: {'hidden_size1': 57, 'hidden_size2': 31, 'learning_rate': 0.03484759067229203, 'dropout_prob': 0.19465087233175626, 'momentum': 0.000995957432158856}. Best is trial 12 with value: 1.4674803018569946.\n",
      " 10%|█         | 15/149 [01:41<14:10,  6.35s/it][I 2024-05-07 07:51:11,968] Trial 15 finished with value: 2.142428398132324 and parameters: {'hidden_size1': 32, 'hidden_size2': 16, 'learning_rate': 0.004518402774599722, 'dropout_prob': 0.11159916586513138, 'momentum': 0.15049357480944978}. Best is trial 12 with value: 1.4674803018569946.\n",
      " 11%|█         | 16/149 [01:46<13:02,  5.88s/it][I 2024-05-07 07:51:17,746] Trial 16 finished with value: 3.1267991065979004 and parameters: {'hidden_size1': 57, 'hidden_size2': 27, 'learning_rate': 0.0005171183678830194, 'dropout_prob': 0.20477071181027412, 'momentum': 0.1389930256772488}. Best is trial 12 with value: 1.4674803018569946.\n",
      " 11%|█▏        | 17/149 [01:52<12:52,  5.85s/it][I 2024-05-07 07:51:23,534] Trial 17 finished with value: 2.177140712738037 and parameters: {'hidden_size1': 27, 'hidden_size2': 38, 'learning_rate': 0.030440949602811816, 'dropout_prob': 0.007299612212889217, 'momentum': 0.1562812475156577}. Best is trial 12 with value: 1.4674803018569946.\n",
      " 12%|█▏        | 18/149 [01:57<12:43,  5.83s/it][I 2024-05-07 07:51:29,244] Trial 18 finished with value: 1.6370103359222412 and parameters: {'hidden_size1': 46, 'hidden_size2': 21, 'learning_rate': 0.012327783330432168, 'dropout_prob': 0.13179078920353857, 'momentum': 0.4395352684291511}. Best is trial 12 with value: 1.4674803018569946.\n",
      " 13%|█▎        | 19/149 [02:03<12:33,  5.79s/it][I 2024-05-07 07:51:34,935] Trial 19 finished with value: 2.545945167541504 and parameters: {'hidden_size1': 64, 'hidden_size2': 26, 'learning_rate': 0.0027291284334757223, 'dropout_prob': 0.24818226642678537, 'momentum': 0.10766275735102926}. Best is trial 12 with value: 1.4674803018569946.\n",
      " 13%|█▎        | 20/149 [02:09<12:23,  5.76s/it][I 2024-05-07 07:51:41,154] Trial 20 finished with value: 2.14715313911438 and parameters: {'hidden_size1': 35, 'hidden_size2': 18, 'learning_rate': 0.04111591286434337, 'dropout_prob': 0.13519564273912546, 'momentum': 0.008944957532174835}. Best is trial 12 with value: 1.4674803018569946.\n",
      " 14%|█▍        | 21/149 [02:15<12:35,  5.90s/it][I 2024-05-07 07:51:48,678] Trial 21 finished with value: 1.46846342086792 and parameters: {'hidden_size1': 61, 'hidden_size2': 33, 'learning_rate': 0.045915909401702655, 'dropout_prob': 0.22966686255808721, 'momentum': 0.025781948926546967}. Best is trial 12 with value: 1.4674803018569946.\n",
      " 15%|█▍        | 22/149 [02:23<13:31,  6.39s/it][I 2024-05-07 07:51:56,559] Trial 22 finished with value: 1.9121850728988647 and parameters: {'hidden_size1': 65, 'hidden_size2': 22, 'learning_rate': 0.09895899822942635, 'dropout_prob': 0.23680946833644378, 'momentum': 0.19810453925615334}. Best is trial 12 with value: 1.4674803018569946.\n",
      " 15%|█▌        | 23/149 [02:31<14:21,  6.84s/it][I 2024-05-07 07:52:03,781] Trial 23 finished with value: 1.650165319442749 and parameters: {'hidden_size1': 49, 'hidden_size2': 35, 'learning_rate': 0.020211189663609186, 'dropout_prob': 0.21859835682026177, 'momentum': 0.10155672883787556}. Best is trial 12 with value: 1.4674803018569946.\n",
      " 16%|█▌        | 24/149 [02:38<14:28,  6.95s/it][I 2024-05-07 07:52:09,450] Trial 24 finished with value: 1.9413809776306152 and parameters: {'hidden_size1': 40, 'hidden_size2': 26, 'learning_rate': 0.007041269195805083, 'dropout_prob': 0.16518316615036557, 'momentum': 0.001876678512226962}. Best is trial 12 with value: 1.4674803018569946.\n",
      " 17%|█▋        | 25/149 [02:43<13:34,  6.57s/it][I 2024-05-07 07:52:15,451] Trial 25 finished with value: 1.9012895822525024 and parameters: {'hidden_size1': 71, 'hidden_size2': 16, 'learning_rate': 0.018357585367473967, 'dropout_prob': 0.23514023680478752, 'momentum': 0.20840387680787767}. Best is trial 12 with value: 1.4674803018569946.\n",
      " 17%|█▋        | 26/149 [02:49<13:06,  6.40s/it][I 2024-05-07 07:52:22,344] Trial 26 finished with value: 2.1231281757354736 and parameters: {'hidden_size1': 100, 'hidden_size2': 21, 'learning_rate': 0.050865937527126065, 'dropout_prob': 0.15511988113865938, 'momentum': 0.09388358924659855}. Best is trial 12 with value: 1.4674803018569946.\n",
      " 18%|█▊        | 27/149 [02:56<13:18,  6.55s/it][I 2024-05-07 07:52:29,321] Trial 27 finished with value: 1.4602850675582886 and parameters: {'hidden_size1': 53, 'hidden_size2': 41, 'learning_rate': 0.05288671854403125, 'dropout_prob': 0.18168670917575253, 'momentum': 0.07392792928167125}. Best is trial 27 with value: 1.4602850675582886.\n",
      " 19%|█▉        | 28/149 [03:03<13:27,  6.68s/it][I 2024-05-07 07:52:35,534] Trial 28 finished with value: 2.374882221221924 and parameters: {'hidden_size1': 56, 'hidden_size2': 39, 'learning_rate': 0.06151252121702744, 'dropout_prob': 0.11233365970064346, 'momentum': 0.0877219108305458}. Best is trial 27 with value: 1.4602850675582886.\n",
      " 19%|█▉        | 29/149 [03:09<13:04,  6.54s/it][I 2024-05-07 07:52:43,228] Trial 29 finished with value: 1.5445377826690674 and parameters: {'hidden_size1': 76, 'hidden_size2': 29, 'learning_rate': 0.04367458670627862, 'dropout_prob': 0.2175663849161706, 'momentum': 0.21196886346708652}. Best is trial 27 with value: 1.4602850675582886.\n",
      " 20%|██        | 30/149 [03:17<13:39,  6.88s/it][I 2024-05-07 07:52:51,128] Trial 30 finished with value: 1.7202327251434326 and parameters: {'hidden_size1': 111, 'hidden_size2': 33, 'learning_rate': 0.008360893191135898, 'dropout_prob': 0.2536987637721061, 'momentum': 0.1579447079172228}. Best is trial 27 with value: 1.4602850675582886.\n",
      " 21%|██        | 31/149 [03:25<14:08,  7.19s/it][I 2024-05-07 07:52:58,079] Trial 31 finished with value: 1.821365475654602 and parameters: {'hidden_size1': 53, 'hidden_size2': 25, 'learning_rate': 0.01875237053206841, 'dropout_prob': 0.18434005983720445, 'momentum': 0.08284760197687799}. Best is trial 27 with value: 1.4602850675582886.\n",
      " 21%|██▏       | 32/149 [03:32<13:52,  7.12s/it][I 2024-05-07 07:53:03,801] Trial 32 finished with value: 2.2059972286224365 and parameters: {'hidden_size1': 44, 'hidden_size2': 19, 'learning_rate': 0.026440238084424958, 'dropout_prob': 0.1887495701871319, 'momentum': 0.05658953729266768}. Best is trial 27 with value: 1.4602850675582886.\n",
      " 22%|██▏       | 33/149 [03:38<12:57,  6.70s/it][I 2024-05-07 07:53:10,956] Trial 33 finished with value: 2.2360174655914307 and parameters: {'hidden_size1': 70, 'hidden_size2': 41, 'learning_rate': 0.056320942675667375, 'dropout_prob': 0.21061526921075804, 'momentum': 0.3128360807587659}. Best is trial 27 with value: 1.4602850675582886.\n",
      " 23%|██▎       | 34/149 [03:45<13:06,  6.84s/it][I 2024-05-07 07:53:16,731] Trial 34 finished with value: 2.153627872467041 and parameters: {'hidden_size1': 31, 'hidden_size2': 33, 'learning_rate': 0.014460259581780318, 'dropout_prob': 0.2723982675992683, 'momentum': 0.005351048582147144}. Best is trial 27 with value: 1.4602850675582886.\n",
      " 23%|██▎       | 35/149 [03:51<12:22,  6.52s/it][I 2024-05-07 07:53:22,637] Trial 35 finished with value: 2.5800106525421143 and parameters: {'hidden_size1': 37, 'hidden_size2': 56, 'learning_rate': 0.05477737481462437, 'dropout_prob': 0.33977624819688995, 'momentum': 0.38670897717887254}. Best is trial 27 with value: 1.4602850675582886.\n",
      " 24%|██▍       | 36/149 [03:57<11:55,  6.33s/it][I 2024-05-07 07:53:29,836] Trial 36 finished with value: 1.3492456674575806 and parameters: {'hidden_size1': 83, 'hidden_size2': 66, 'learning_rate': 0.024996091420308418, 'dropout_prob': 0.14930517755857184, 'momentum': 0.23139262153325707}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 25%|██▍       | 37/149 [04:04<12:18,  6.59s/it][I 2024-05-07 07:53:37,306] Trial 37 finished with value: 1.9208970069885254 and parameters: {'hidden_size1': 85, 'hidden_size2': 69, 'learning_rate': 0.03198765871035458, 'dropout_prob': 0.15481379094135633, 'momentum': 0.27056662831754635}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 26%|██▌       | 38/149 [04:11<12:41,  6.86s/it][I 2024-05-07 07:53:45,019] Trial 38 finished with value: 1.86668062210083 and parameters: {'hidden_size1': 135, 'hidden_size2': 45, 'learning_rate': 0.005968640103019384, 'dropout_prob': 0.07904084833968861, 'momentum': 0.2352272577173901}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 26%|██▌       | 39/149 [04:19<13:02,  7.11s/it][I 2024-05-07 07:53:52,538] Trial 39 finished with value: 1.6945241689682007 and parameters: {'hidden_size1': 63, 'hidden_size2': 65, 'learning_rate': 0.07460333446355899, 'dropout_prob': 0.13668664270616923, 'momentum': 0.3056415628235889}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 27%|██▋       | 40/149 [04:26<13:08,  7.23s/it][I 2024-05-07 07:53:59,653] Trial 40 finished with value: 1.5614066123962402 and parameters: {'hidden_size1': 84, 'hidden_size2': 55, 'learning_rate': 0.009646562455424413, 'dropout_prob': 0.09892296697102954, 'momentum': 0.1827875260003465}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 28%|██▊       | 41/149 [04:34<12:57,  7.20s/it][I 2024-05-07 07:54:06,707] Trial 41 finished with value: 2.016721248626709 and parameters: {'hidden_size1': 50, 'hidden_size2': 45, 'learning_rate': 0.02495364102684535, 'dropout_prob': 0.18168543452773345, 'momentum': 0.04708229531058752}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 28%|██▊       | 42/149 [04:41<12:45,  7.16s/it][I 2024-05-07 07:54:12,687] Trial 42 finished with value: 1.495258092880249 and parameters: {'hidden_size1': 42, 'hidden_size2': 30, 'learning_rate': 0.013389312153976114, 'dropout_prob': 0.20791259227366485, 'momentum': 0.12372469898239524}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 29%|██▉       | 43/149 [04:47<12:01,  6.80s/it][I 2024-05-07 07:54:19,405] Trial 43 finished with value: 1.6395847797393799 and parameters: {'hidden_size1': 58, 'hidden_size2': 77, 'learning_rate': 0.08984958441187209, 'dropout_prob': 0.15020880408130594, 'momentum': 0.05765287564257443}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 30%|██▉       | 44/149 [04:53<11:51,  6.78s/it][I 2024-05-07 07:54:27,587] Trial 44 finished with value: 1.3925726413726807 and parameters: {'hidden_size1': 72, 'hidden_size2': 93, 'learning_rate': 0.04106989138459726, 'dropout_prob': 0.1727017168657403, 'momentum': 0.05185692610907197}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 30%|███       | 45/149 [05:02<12:28,  7.20s/it][I 2024-05-07 07:54:35,482] Trial 45 finished with value: 2.13985013961792 and parameters: {'hidden_size1': 78, 'hidden_size2': 94, 'learning_rate': 0.040695948340013645, 'dropout_prob': 0.12471218883744672, 'momentum': 0.1333459645781067}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 31%|███       | 46/149 [05:09<12:43,  7.41s/it][I 2024-05-07 07:54:44,260] Trial 46 finished with value: 2.320688009262085 and parameters: {'hidden_size1': 93, 'hidden_size2': 110, 'learning_rate': 0.06472270674781666, 'dropout_prob': 0.16453240264524288, 'momentum': 0.07438079968586693}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 32%|███▏      | 47/149 [05:18<13:17,  7.82s/it][I 2024-05-07 07:54:52,458] Trial 47 finished with value: 1.521307110786438 and parameters: {'hidden_size1': 69, 'hidden_size2': 52, 'learning_rate': 0.022265744970491354, 'dropout_prob': 0.262702906382142, 'momentum': 0.19927213809274144}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 32%|███▏      | 48/149 [05:26<13:21,  7.93s/it][I 2024-05-07 07:55:00,213] Trial 48 finished with value: 1.9097332954406738 and parameters: {'hidden_size1': 45, 'hidden_size2': 59, 'learning_rate': 0.04149266119131299, 'dropout_prob': 0.2365477607728242, 'momentum': 0.24878934333213956}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 33%|███▎      | 49/149 [05:34<13:07,  7.88s/it][I 2024-05-07 07:55:06,489] Trial 49 finished with value: 2.068696975708008 and parameters: {'hidden_size1': 61, 'hidden_size2': 49, 'learning_rate': 0.09493722261796322, 'dropout_prob': 0.07854281050567063, 'momentum': 0.1643543553103945}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 34%|███▎      | 50/149 [05:40<12:12,  7.40s/it][I 2024-05-07 07:55:13,597] Trial 50 finished with value: 1.5256340503692627 and parameters: {'hidden_size1': 75, 'hidden_size2': 75, 'learning_rate': 0.0106742774153289, 'dropout_prob': 0.1698012896848326, 'momentum': 0.03662901309403232}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 34%|███▍      | 51/149 [05:48<11:56,  7.31s/it][I 2024-05-07 07:55:19,293] Trial 51 finished with value: 1.506271243095398 and parameters: {'hidden_size1': 53, 'hidden_size2': 19, 'learning_rate': 0.029753563632818378, 'dropout_prob': 0.1924860032836048, 'momentum': 0.03423585578638085}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 35%|███▍      | 52/149 [05:53<11:02,  6.83s/it][I 2024-05-07 07:55:25,132] Trial 52 finished with value: 2.1781725883483887 and parameters: {'hidden_size1': 50, 'hidden_size2': 23, 'learning_rate': 0.017901343065527455, 'dropout_prob': 0.13667322501476353, 'momentum': 0.12694242964917662}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 36%|███▌      | 53/149 [05:59<10:26,  6.53s/it][I 2024-05-07 07:55:31,587] Trial 53 finished with value: 1.4235455989837646 and parameters: {'hidden_size1': 61, 'hidden_size2': 61, 'learning_rate': 0.06719989037147785, 'dropout_prob': 0.1796082525284977, 'momentum': 0.07113387789063444}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 36%|███▌      | 54/149 [06:06<10:18,  6.51s/it][I 2024-05-07 07:55:38,705] Trial 54 finished with value: 2.1970434188842773 and parameters: {'hidden_size1': 65, 'hidden_size2': 62, 'learning_rate': 0.0730231555932968, 'dropout_prob': 0.17298640130568851, 'momentum': 0.10466475268831334}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 37%|███▋      | 55/149 [06:13<10:28,  6.69s/it][I 2024-05-07 07:55:46,152] Trial 55 finished with value: 2.1303021907806396 and parameters: {'hidden_size1': 57, 'hidden_size2': 44, 'learning_rate': 0.04605881556543062, 'dropout_prob': 0.2205576831951447, 'momentum': 0.07041175344593792}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 38%|███▊      | 56/149 [06:20<10:43,  6.92s/it][I 2024-05-07 07:55:55,186] Trial 56 finished with value: 1.7409071922302246 and parameters: {'hidden_size1': 70, 'hidden_size2': 51, 'learning_rate': 0.02654877944095869, 'dropout_prob': 0.14707197753550705, 'momentum': 0.16486532243621013}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 38%|███▊      | 57/149 [06:29<11:34,  7.55s/it][I 2024-05-07 07:56:02,361] Trial 57 finished with value: 2.026301383972168 and parameters: {'hidden_size1': 45, 'hidden_size2': 35, 'learning_rate': 0.06473953082049855, 'dropout_prob': 0.19702775311165602, 'momentum': 0.036604961239446704}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 39%|███▉      | 58/149 [06:36<11:16,  7.44s/it][I 2024-05-07 07:56:08,693] Trial 58 finished with value: 2.370696544647217 and parameters: {'hidden_size1': 64, 'hidden_size2': 68, 'learning_rate': 0.03815755191394084, 'dropout_prob': 0.1209177636545767, 'momentum': 0.11531162302900833}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 40%|███▉      | 59/149 [06:43<10:39,  7.11s/it][I 2024-05-07 07:56:15,264] Trial 59 finished with value: 2.384514808654785 and parameters: {'hidden_size1': 81, 'hidden_size2': 61, 'learning_rate': 0.09857491301502545, 'dropout_prob': 0.10851218990227088, 'momentum': 0.012124690998562976}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 40%|████      | 60/149 [06:49<10:18,  6.95s/it][I 2024-05-07 07:56:21,909] Trial 60 finished with value: 2.020920753479004 and parameters: {'hidden_size1': 41, 'hidden_size2': 39, 'learning_rate': 0.034704560197330536, 'dropout_prob': 0.17421364894906435, 'momentum': 0.07308531382882086}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 41%|████      | 61/149 [06:56<10:03,  6.86s/it][I 2024-05-07 07:56:27,708] Trial 61 finished with value: 1.5523908138275146 and parameters: {'hidden_size1': 53, 'hidden_size2': 17, 'learning_rate': 0.014454105372658064, 'dropout_prob': 0.19607982877748903, 'momentum': 0.0036098353430413926}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 42%|████▏     | 62/149 [07:02<09:28,  6.54s/it][I 2024-05-07 07:56:33,427] Trial 62 finished with value: 1.539612889289856 and parameters: {'hidden_size1': 48, 'hidden_size2': 22, 'learning_rate': 0.05335338064078141, 'dropout_prob': 0.22807007096801737, 'momentum': 0.09499700742920442}. Best is trial 36 with value: 1.3492456674575806.\n",
      " 42%|████▏     | 63/149 [07:07<09:01,  6.29s/it][I 2024-05-07 07:56:39,318] Trial 63 finished with value: 1.3369051218032837 and parameters: {'hidden_size1': 60, 'hidden_size2': 28, 'learning_rate': 0.019952292666081138, 'dropout_prob': 0.20827414115935003, 'momentum': 0.13512555105346177}. Best is trial 63 with value: 1.3369051218032837.\n",
      " 43%|████▎     | 64/149 [07:13<08:44,  6.17s/it][I 2024-05-07 07:56:45,628] Trial 64 finished with value: 1.9356688261032104 and parameters: {'hidden_size1': 62, 'hidden_size2': 29, 'learning_rate': 0.021439298990756354, 'dropout_prob': 0.2097041975400174, 'momentum': 0.14547544735511425}. Best is trial 63 with value: 1.3369051218032837.\n",
      " 44%|████▎     | 65/149 [07:20<08:41,  6.21s/it][I 2024-05-07 07:56:53,940] Trial 65 finished with value: 1.580330491065979 and parameters: {'hidden_size1': 74, 'hidden_size2': 25, 'learning_rate': 0.0700254135308893, 'dropout_prob': 0.15012849301072523, 'momentum': 0.04754699246625817}. Best is trial 63 with value: 1.3369051218032837.\n",
      " 44%|████▍     | 66/149 [07:28<09:27,  6.84s/it][I 2024-05-07 07:57:00,514] Trial 66 finished with value: 1.4532068967819214 and parameters: {'hidden_size1': 71, 'hidden_size2': 28, 'learning_rate': 0.03344596272342349, 'dropout_prob': 0.18126834368741387, 'momentum': 0.12046818401054957}. Best is trial 63 with value: 1.3369051218032837.\n",
      " 45%|████▍     | 67/149 [07:34<09:14,  6.76s/it][I 2024-05-07 07:57:07,425] Trial 67 finished with value: 1.4401752948760986 and parameters: {'hidden_size1': 87, 'hidden_size2': 27, 'learning_rate': 0.030904970133341373, 'dropout_prob': 0.1840866728951727, 'momentum': 0.2270779225780355}. Best is trial 63 with value: 1.3369051218032837.\n",
      " 46%|████▌     | 68/149 [07:41<09:11,  6.81s/it][I 2024-05-07 07:57:14,555] Trial 68 finished with value: 2.1103484630584717 and parameters: {'hidden_size1': 93, 'hidden_size2': 35, 'learning_rate': 0.011334688540977888, 'dropout_prob': 0.1754389179777744, 'momentum': 0.1757726161292307}. Best is trial 63 with value: 1.3369051218032837.\n",
      " 46%|████▋     | 69/149 [07:49<09:12,  6.90s/it][I 2024-05-07 07:57:21,308] Trial 69 finished with value: 1.515139102935791 and parameters: {'hidden_size1': 84, 'hidden_size2': 27, 'learning_rate': 0.01718689367794336, 'dropout_prob': 0.20151611208664677, 'momentum': 0.19998010211714384}. Best is trial 63 with value: 1.3369051218032837.\n",
      " 47%|████▋     | 70/149 [07:55<09:01,  6.86s/it][I 2024-05-07 07:57:28,725] Trial 70 finished with value: 1.5087214708328247 and parameters: {'hidden_size1': 104, 'hidden_size2': 43, 'learning_rate': 0.026091008011909295, 'dropout_prob': 0.18290439478738382, 'momentum': 0.23776556948061683}. Best is trial 63 with value: 1.3369051218032837.\n",
      " 48%|████▊     | 71/149 [08:03<09:08,  7.03s/it][I 2024-05-07 07:57:35,078] Trial 71 finished with value: 1.4493517875671387 and parameters: {'hidden_size1': 70, 'hidden_size2': 20, 'learning_rate': 0.03334380089508208, 'dropout_prob': 0.139546707042728, 'momentum': 0.12646019204088121}. Best is trial 63 with value: 1.3369051218032837.\n",
      " 48%|████▊     | 72/149 [08:09<08:45,  6.82s/it][I 2024-05-07 07:57:41,979] Trial 72 finished with value: 1.4501484632492065 and parameters: {'hidden_size1': 68, 'hidden_size2': 31, 'learning_rate': 0.033140278313837424, 'dropout_prob': 0.1602163327732266, 'momentum': 0.1343277283966673}. Best is trial 63 with value: 1.3369051218032837.\n",
      " 49%|████▉     | 73/149 [08:16<08:40,  6.85s/it][I 2024-05-07 07:57:49,648] Trial 73 finished with value: 1.411500334739685 and parameters: {'hidden_size1': 70, 'hidden_size2': 29, 'learning_rate': 0.031033201648349232, 'dropout_prob': 0.15933675535282138, 'momentum': 0.2814284672550797}. Best is trial 63 with value: 1.3369051218032837.\n",
      " 50%|████▉     | 74/149 [08:24<08:52,  7.09s/it][I 2024-05-07 07:57:57,743] Trial 74 finished with value: 1.279626727104187 and parameters: {'hidden_size1': 78, 'hidden_size2': 31, 'learning_rate': 0.022581031455242605, 'dropout_prob': 0.138646450852048, 'momentum': 0.28299287287570285}. Best is trial 74 with value: 1.279626727104187.\n",
      " 50%|█████     | 75/149 [08:32<09:07,  7.39s/it][I 2024-05-07 07:58:04,189] Trial 75 finished with value: 1.465688705444336 and parameters: {'hidden_size1': 78, 'hidden_size2': 24, 'learning_rate': 0.02075097142482686, 'dropout_prob': 0.1345713885215994, 'momentum': 0.26670256174642126}. Best is trial 74 with value: 1.279626727104187.\n",
      " 51%|█████     | 76/149 [08:38<08:38,  7.11s/it][I 2024-05-07 07:58:10,334] Trial 76 finished with value: 1.625042200088501 and parameters: {'hidden_size1': 88, 'hidden_size2': 20, 'learning_rate': 0.007589648649716566, 'dropout_prob': 0.11848178381124036, 'momentum': 0.30359025322707045}. Best is trial 74 with value: 1.279626727104187.\n",
      " 52%|█████▏    | 77/149 [08:44<08:11,  6.82s/it][I 2024-05-07 07:58:16,857] Trial 77 finished with value: 1.299805998802185 and parameters: {'hidden_size1': 77, 'hidden_size2': 27, 'learning_rate': 0.01662895299647398, 'dropout_prob': 0.13995593494349307, 'momentum': 0.35494670779779686}. Best is trial 74 with value: 1.279626727104187.\n",
      " 52%|█████▏    | 78/149 [08:51<07:57,  6.73s/it][I 2024-05-07 07:58:23,895] Trial 78 finished with value: 1.4063966274261475 and parameters: {'hidden_size1': 81, 'hidden_size2': 26, 'learning_rate': 0.014170644082822053, 'dropout_prob': 0.1576011320161858, 'momentum': 0.37754320189522916}. Best is trial 74 with value: 1.279626727104187.\n",
      " 53%|█████▎    | 79/149 [08:58<07:57,  6.82s/it][I 2024-05-07 07:58:30,106] Trial 79 finished with value: 1.708009958267212 and parameters: {'hidden_size1': 76, 'hidden_size2': 25, 'learning_rate': 0.015691046372005548, 'dropout_prob': 0.10338295642204841, 'momentum': 0.3764212899974946}. Best is trial 74 with value: 1.279626727104187.\n",
      " 54%|█████▎    | 80/149 [09:04<07:38,  6.64s/it][I 2024-05-07 07:58:38,859] Trial 80 finished with value: 1.4260752201080322 and parameters: {'hidden_size1': 80, 'hidden_size2': 31, 'learning_rate': 0.011620612102623344, 'dropout_prob': 0.1586917786860942, 'momentum': 0.346300233716452}. Best is trial 74 with value: 1.279626727104187.\n",
      " 54%|█████▍    | 81/149 [09:13<08:14,  7.28s/it][I 2024-05-07 07:58:45,954] Trial 81 finished with value: 1.4658029079437256 and parameters: {'hidden_size1': 81, 'hidden_size2': 31, 'learning_rate': 0.009971186888658579, 'dropout_prob': 0.15999128459648565, 'momentum': 0.3591694541390298}. Best is trial 74 with value: 1.279626727104187.\n",
      " 55%|█████▌    | 82/149 [09:20<08:03,  7.22s/it][I 2024-05-07 07:58:55,110] Trial 82 finished with value: 1.360447645187378 and parameters: {'hidden_size1': 94, 'hidden_size2': 29, 'learning_rate': 0.012600739745756326, 'dropout_prob': 0.13302367957813468, 'momentum': 0.28093183829782575}. Best is trial 74 with value: 1.279626727104187.\n",
      " 56%|█████▌    | 83/149 [09:29<08:34,  7.80s/it][I 2024-05-07 07:59:01,916] Trial 83 finished with value: 1.4621341228485107 and parameters: {'hidden_size1': 92, 'hidden_size2': 28, 'learning_rate': 0.02163657962235091, 'dropout_prob': 0.13936066206233064, 'momentum': 0.28522448510099274}. Best is trial 74 with value: 1.279626727104187.\n",
      " 56%|█████▋    | 84/149 [09:36<08:07,  7.50s/it][I 2024-05-07 07:59:08,408] Trial 84 finished with value: 1.7517361640930176 and parameters: {'hidden_size1': 101, 'hidden_size2': 24, 'learning_rate': 0.006075731549779467, 'dropout_prob': 0.12145585637860272, 'momentum': 0.32710440923527206}. Best is trial 74 with value: 1.279626727104187.\n",
      " 57%|█████▋    | 85/149 [09:42<07:40,  7.20s/it][I 2024-05-07 07:59:14,008] Trial 85 finished with value: 1.3070359230041504 and parameters: {'hidden_size1': 60, 'hidden_size2': 26, 'learning_rate': 0.015720958085423283, 'dropout_prob': 0.14672204054659738, 'momentum': 0.27509714522399303}. Best is trial 74 with value: 1.279626727104187.\n",
      " 58%|█████▊    | 86/149 [09:48<07:03,  6.72s/it][I 2024-05-07 07:59:22,299] Trial 86 finished with value: 1.9603137969970703 and parameters: {'hidden_size1': 113, 'hidden_size2': 26, 'learning_rate': 0.013441530452791355, 'dropout_prob': 0.09473516527968096, 'momentum': 0.2807029194958412}. Best is trial 74 with value: 1.279626727104187.\n",
      " 58%|█████▊    | 87/149 [09:56<07:25,  7.19s/it][I 2024-05-07 07:59:29,488] Trial 87 finished with value: 2.0420339107513428 and parameters: {'hidden_size1': 74, 'hidden_size2': 29, 'learning_rate': 0.01634457676886415, 'dropout_prob': 0.12786049840235644, 'momentum': 0.2507273434468315}. Best is trial 74 with value: 1.279626727104187.\n",
      " 59%|█████▉    | 88/149 [10:03<07:18,  7.19s/it][I 2024-05-07 07:59:35,569] Trial 88 finished with value: 1.666860580444336 and parameters: {'hidden_size1': 66, 'hidden_size2': 36, 'learning_rate': 0.008548259261295198, 'dropout_prob': 0.1116662521204281, 'momentum': 0.4028960311276304}. Best is trial 74 with value: 1.279626727104187.\n",
      " 60%|█████▉    | 89/149 [10:10<06:51,  6.86s/it][I 2024-05-07 07:59:42,498] Trial 89 finished with value: 1.8145478963851929 and parameters: {'hidden_size1': 89, 'hidden_size2': 33, 'learning_rate': 0.02171787777577536, 'dropout_prob': 0.1440626934793724, 'momentum': 0.3347677955269785}. Best is trial 74 with value: 1.279626727104187.\n",
      " 60%|██████    | 90/149 [10:16<06:45,  6.88s/it][I 2024-05-07 07:59:49,528] Trial 90 finished with value: 1.7457687854766846 and parameters: {'hidden_size1': 95, 'hidden_size2': 27, 'learning_rate': 0.005161202960078253, 'dropout_prob': 0.14767612169154437, 'momentum': 0.2809176864717209}. Best is trial 74 with value: 1.279626727104187.\n",
      " 61%|██████    | 91/149 [10:23<06:41,  6.92s/it][I 2024-05-07 07:59:57,429] Trial 91 finished with value: 1.6857308149337769 and parameters: {'hidden_size1': 61, 'hidden_size2': 37, 'learning_rate': 0.01726247088386088, 'dropout_prob': 0.1613075050422285, 'momentum': 0.306881184400631}. Best is trial 74 with value: 1.279626727104187.\n",
      " 62%|██████▏   | 92/149 [10:31<06:51,  7.22s/it][I 2024-05-07 08:00:05,527] Trial 92 finished with value: 2.0090653896331787 and parameters: {'hidden_size1': 59, 'hidden_size2': 22, 'learning_rate': 0.026397998869478436, 'dropout_prob': 0.1289084656624337, 'momentum': 0.22101828425685902}. Best is trial 74 with value: 1.279626727104187.\n",
      " 62%|██████▏   | 93/149 [10:39<06:59,  7.48s/it][I 2024-05-07 08:00:12,680] Trial 93 finished with value: 2.1629977226257324 and parameters: {'hidden_size1': 74, 'hidden_size2': 33, 'learning_rate': 0.047081661485679036, 'dropout_prob': 0.16574918627654212, 'momentum': 0.33037403078536465}. Best is trial 74 with value: 1.279626727104187.\n",
      " 63%|██████▎   | 94/149 [10:47<06:46,  7.38s/it][I 2024-05-07 08:00:19,479] Trial 94 finished with value: 1.8475191593170166 and parameters: {'hidden_size1': 66, 'hidden_size2': 29, 'learning_rate': 0.012197705519926091, 'dropout_prob': 0.15048673965770612, 'momentum': 0.24637909329501523}. Best is trial 74 with value: 1.279626727104187.\n",
      " 64%|██████▍   | 95/149 [10:53<06:29,  7.21s/it][I 2024-05-07 08:00:26,994] Trial 95 finished with value: 2.1649744510650635 and parameters: {'hidden_size1': 84, 'hidden_size2': 24, 'learning_rate': 0.003550057533237738, 'dropout_prob': 0.16851515550427854, 'momentum': 0.1925477823182859}. Best is trial 74 with value: 1.279626727104187.\n",
      " 64%|██████▍   | 96/149 [11:01<06:26,  7.30s/it][I 2024-05-07 08:00:35,408] Trial 96 finished with value: 1.4319393634796143 and parameters: {'hidden_size1': 55, 'hidden_size2': 90, 'learning_rate': 0.04077458260010786, 'dropout_prob': 0.1951431559255218, 'momentum': 0.21860418442339016}. Best is trial 74 with value: 1.279626727104187.\n",
      " 65%|██████▌   | 97/149 [11:09<06:37,  7.64s/it][I 2024-05-07 08:00:42,306] Trial 97 finished with value: 1.2833360433578491 and parameters: {'hidden_size1': 59, 'hidden_size2': 48, 'learning_rate': 0.024524792695831757, 'dropout_prob': 0.13113318765070955, 'momentum': 0.3559907136948963}. Best is trial 74 with value: 1.279626727104187.\n",
      " 66%|██████▌   | 98/149 [11:16<06:17,  7.41s/it][I 2024-05-07 08:00:48,750] Trial 98 finished with value: 1.67807137966156 and parameters: {'hidden_size1': 66, 'hidden_size2': 40, 'learning_rate': 0.009313726401323566, 'dropout_prob': 0.1151469547010416, 'momentum': 0.2953217208544315}. Best is trial 74 with value: 1.279626727104187.\n",
      " 66%|██████▋   | 99/149 [11:23<05:56,  7.12s/it][I 2024-05-07 08:00:55,670] Trial 99 finished with value: 1.6068047285079956 and parameters: {'hidden_size1': 78, 'hidden_size2': 32, 'learning_rate': 0.026168304672496055, 'dropout_prob': 0.13004061626725422, 'momentum': 0.411723218449818}. Best is trial 74 with value: 1.279626727104187.\n",
      " 67%|██████▋   | 100/149 [11:30<05:46,  7.06s/it][I 2024-05-07 08:01:01,471] Trial 100 finished with value: 1.6408321857452393 and parameters: {'hidden_size1': 72, 'hidden_size2': 30, 'learning_rate': 0.013527853792823119, 'dropout_prob': 0.09277518685515078, 'momentum': 0.356480774686048}. Best is trial 74 with value: 1.279626727104187.\n",
      " 68%|██████▊   | 101/149 [11:35<05:20,  6.68s/it][I 2024-05-07 08:01:07,432] Trial 101 finished with value: 1.6768462657928467 and parameters: {'hidden_size1': 60, 'hidden_size2': 57, 'learning_rate': 0.018341431541360687, 'dropout_prob': 0.1442390487589233, 'momentum': 0.3108970604440453}. Best is trial 74 with value: 1.279626727104187.\n",
      " 68%|██████▊   | 102/149 [11:41<05:03,  6.47s/it][I 2024-05-07 08:01:13,178] Trial 102 finished with value: 1.4480280876159668 and parameters: {'hidden_size1': 56, 'hidden_size2': 48, 'learning_rate': 0.05804361818141797, 'dropout_prob': 0.1505028573274096, 'momentum': 0.26649713518613755}. Best is trial 74 with value: 1.279626727104187.\n",
      " 69%|██████▉   | 103/149 [11:47<04:47,  6.25s/it][I 2024-05-07 08:01:19,278] Trial 103 finished with value: 1.4931766986846924 and parameters: {'hidden_size1': 63, 'hidden_size2': 54, 'learning_rate': 0.022386270718526252, 'dropout_prob': 0.17477835615252504, 'momentum': 0.32589163722203984}. Best is trial 74 with value: 1.279626727104187.\n",
      " 70%|██████▉   | 104/149 [11:53<04:39,  6.21s/it][I 2024-05-07 08:01:24,987] Trial 104 finished with value: 1.6391366720199585 and parameters: {'hidden_size1': 68, 'hidden_size2': 26, 'learning_rate': 0.0482874682574394, 'dropout_prob': 0.18876292127194957, 'momentum': 0.26386329913103607}. Best is trial 74 with value: 1.279626727104187.\n",
      " 70%|███████   | 105/149 [11:59<04:26,  6.05s/it][I 2024-05-07 08:01:30,932] Trial 105 finished with value: 1.4113157987594604 and parameters: {'hidden_size1': 59, 'hidden_size2': 38, 'learning_rate': 0.03772923782653261, 'dropout_prob': 0.12572209906671197, 'momentum': 0.37132574874640634}. Best is trial 74 with value: 1.279626727104187.\n",
      " 71%|███████   | 106/149 [12:05<04:19,  6.02s/it][I 2024-05-07 08:01:36,974] Trial 106 finished with value: 1.4155793190002441 and parameters: {'hidden_size1': 80, 'hidden_size2': 33, 'learning_rate': 0.03848790308638443, 'dropout_prob': 0.10570564254876225, 'momentum': 0.4353301450403189}. Best is trial 74 with value: 1.279626727104187.\n",
      " 72%|███████▏  | 107/149 [12:11<04:13,  6.03s/it][I 2024-05-07 08:01:43,318] Trial 107 finished with value: 2.0704915523529053 and parameters: {'hidden_size1': 74, 'hidden_size2': 37, 'learning_rate': 0.028248350947915182, 'dropout_prob': 0.12372808828705696, 'momentum': 0.3784071299807416}. Best is trial 74 with value: 1.279626727104187.\n",
      " 72%|███████▏  | 108/149 [12:17<04:11,  6.12s/it][I 2024-05-07 08:01:51,899] Trial 108 finished with value: 1.2506225109100342 and parameters: {'hidden_size1': 58, 'hidden_size2': 38, 'learning_rate': 0.016102437562746832, 'dropout_prob': 0.13126073394301102, 'momentum': 0.3450054584888313}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 73%|███████▎  | 109/149 [12:26<04:34,  6.86s/it][I 2024-05-07 08:02:00,508] Trial 109 finished with value: 2.3749403953552246 and parameters: {'hidden_size1': 51, 'hidden_size2': 39, 'learning_rate': 0.015384496940165516, 'dropout_prob': 0.13587556286826896, 'momentum': 0.31036640905897156}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 74%|███████▍  | 110/149 [12:34<04:47,  7.38s/it][I 2024-05-07 08:02:07,042] Trial 110 finished with value: 2.0963900089263916 and parameters: {'hidden_size1': 58, 'hidden_size2': 43, 'learning_rate': 0.010722576143017467, 'dropout_prob': 0.11737014316995859, 'momentum': 0.3545583236459758}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 74%|███████▍  | 111/149 [12:41<04:30,  7.13s/it][I 2024-05-07 08:02:13,478] Trial 111 finished with value: 1.7926336526870728 and parameters: {'hidden_size1': 55, 'hidden_size2': 35, 'learning_rate': 0.019721104722999764, 'dropout_prob': 0.15692293691894482, 'momentum': 0.2968139812332744}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 75%|███████▌  | 112/149 [12:47<04:16,  6.92s/it][I 2024-05-07 08:02:19,712] Trial 112 finished with value: 1.3745859861373901 and parameters: {'hidden_size1': 64, 'hidden_size2': 46, 'learning_rate': 0.024361030412065714, 'dropout_prob': 0.14150113134898923, 'momentum': 0.34147291929182483}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 76%|███████▌  | 113/149 [12:54<04:01,  6.71s/it][I 2024-05-07 08:02:25,812] Trial 113 finished with value: 1.3926504850387573 and parameters: {'hidden_size1': 64, 'hidden_size2': 42, 'learning_rate': 0.025339148393108735, 'dropout_prob': 0.13926105018582663, 'momentum': 0.33773898362513544}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 77%|███████▋  | 114/149 [13:00<03:48,  6.53s/it][I 2024-05-07 08:02:32,007] Trial 114 finished with value: 1.3488940000534058 and parameters: {'hidden_size1': 63, 'hidden_size2': 48, 'learning_rate': 0.024012979035150844, 'dropout_prob': 0.14544076932736713, 'momentum': 0.34059914184153106}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 77%|███████▋  | 115/149 [13:06<03:38,  6.43s/it][I 2024-05-07 08:02:38,554] Trial 115 finished with value: 1.2611820697784424 and parameters: {'hidden_size1': 62, 'hidden_size2': 41, 'learning_rate': 0.023612991847600278, 'dropout_prob': 0.14253297461617226, 'momentum': 0.33588932997061066}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 78%|███████▊  | 116/149 [13:13<03:33,  6.47s/it][I 2024-05-07 08:02:45,051] Trial 116 finished with value: 2.0628602504730225 and parameters: {'hidden_size1': 53, 'hidden_size2': 46, 'learning_rate': 0.01868780787815538, 'dropout_prob': 0.10526536265288104, 'momentum': 0.3961007594260418}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 79%|███████▊  | 117/149 [13:19<03:27,  6.47s/it][I 2024-05-07 08:02:52,431] Trial 117 finished with value: 1.2860581874847412 and parameters: {'hidden_size1': 67, 'hidden_size2': 46, 'learning_rate': 0.02283685642078689, 'dropout_prob': 0.13334686756735864, 'momentum': 0.35376967728641207}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 79%|███████▉  | 118/149 [13:26<03:29,  6.75s/it][I 2024-05-07 08:02:59,182] Trial 118 finished with value: 1.475884199142456 and parameters: {'hidden_size1': 48, 'hidden_size2': 51, 'learning_rate': 0.0122071363370161, 'dropout_prob': 0.08082058152737635, 'momentum': 0.3443133852907237}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 80%|███████▉  | 119/149 [13:33<03:22,  6.75s/it][I 2024-05-07 08:03:06,485] Trial 119 finished with value: 1.791093111038208 and parameters: {'hidden_size1': 63, 'hidden_size2': 47, 'learning_rate': 0.023726368881899988, 'dropout_prob': 0.09702697571703886, 'momentum': 0.3165451303680261}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 81%|████████  | 120/149 [13:40<03:20,  6.91s/it][I 2024-05-07 08:03:13,286] Trial 120 finished with value: 1.4055393934249878 and parameters: {'hidden_size1': 67, 'hidden_size2': 45, 'learning_rate': 0.01591885362971743, 'dropout_prob': 0.1147767198559958, 'momentum': 0.41507120372131223}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 81%|████████  | 121/149 [13:47<03:12,  6.88s/it][I 2024-05-07 08:03:19,996] Trial 121 finished with value: 2.2248587608337402 and parameters: {'hidden_size1': 71, 'hidden_size2': 53, 'learning_rate': 0.028768456182180192, 'dropout_prob': 0.1323781940548341, 'momentum': 0.3678409446369952}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 82%|████████▏ | 122/149 [13:54<03:04,  6.83s/it][I 2024-05-07 08:03:26,952] Trial 122 finished with value: 1.7606180906295776 and parameters: {'hidden_size1': 58, 'hidden_size2': 49, 'learning_rate': 0.020350555189302368, 'dropout_prob': 0.14272047388198486, 'momentum': 0.24864424086860526}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 83%|████████▎ | 123/149 [14:01<02:58,  6.87s/it][I 2024-05-07 08:03:33,736] Trial 123 finished with value: 1.744608759880066 and parameters: {'hidden_size1': 61, 'hidden_size2': 49, 'learning_rate': 0.007397195707716312, 'dropout_prob': 0.17120164255778553, 'momentum': 0.3561251143699881}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 83%|████████▎ | 124/149 [14:08<02:51,  6.84s/it][I 2024-05-07 08:03:40,725] Trial 124 finished with value: 1.3899809122085571 and parameters: {'hidden_size1': 67, 'hidden_size2': 46, 'learning_rate': 0.02299046882933541, 'dropout_prob': 0.14994354692923367, 'momentum': 0.39134012978241906}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 84%|████████▍ | 125/149 [14:15<02:45,  6.89s/it][I 2024-05-07 08:03:47,493] Trial 125 finished with value: 1.4882738590240479 and parameters: {'hidden_size1': 51, 'hidden_size2': 42, 'learning_rate': 0.02319346668739687, 'dropout_prob': 0.14857232269614487, 'momentum': 0.39469379695191276}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 85%|████████▍ | 126/149 [14:21<02:37,  6.85s/it][I 2024-05-07 08:03:54,919] Trial 126 finished with value: 1.910481333732605 and parameters: {'hidden_size1': 66, 'hidden_size2': 41, 'learning_rate': 0.017320397917046624, 'dropout_prob': 0.1252695647050506, 'momentum': 0.3234632181077717}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 85%|████████▌ | 127/149 [14:29<02:34,  7.02s/it][I 2024-05-07 08:04:01,259] Trial 127 finished with value: 1.8061468601226807 and parameters: {'hidden_size1': 56, 'hidden_size2': 45, 'learning_rate': 0.01394193294943917, 'dropout_prob': 0.11004472796348504, 'momentum': 0.2862461447225767}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 86%|████████▌ | 128/149 [14:35<02:23,  6.82s/it][I 2024-05-07 08:04:07,770] Trial 128 finished with value: 1.9746037721633911 and parameters: {'hidden_size1': 76, 'hidden_size2': 58, 'learning_rate': 0.009812451557519183, 'dropout_prob': 0.13279818392541626, 'momentum': 0.4551613951544027}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 87%|████████▋ | 129/149 [14:42<02:14,  6.73s/it][I 2024-05-07 08:04:14,158] Trial 129 finished with value: 1.424216389656067 and parameters: {'hidden_size1': 63, 'hidden_size2': 41, 'learning_rate': 0.03242760123771513, 'dropout_prob': 0.1411699492870189, 'momentum': 0.33742282938040136}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 87%|████████▋ | 130/149 [14:48<02:05,  6.62s/it][I 2024-05-07 08:04:20,848] Trial 130 finished with value: 1.3038547039031982 and parameters: {'hidden_size1': 69, 'hidden_size2': 54, 'learning_rate': 0.019879678407215294, 'dropout_prob': 0.1533039732332724, 'momentum': 0.4270944462298988}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 88%|████████▊ | 131/149 [14:55<01:59,  6.64s/it][I 2024-05-07 08:04:27,791] Trial 131 finished with value: 1.6955419778823853 and parameters: {'hidden_size1': 70, 'hidden_size2': 51, 'learning_rate': 0.019157248880721636, 'dropout_prob': 0.1524874200094482, 'momentum': 0.38907718171056227}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 89%|████████▊ | 132/149 [15:02<01:54,  6.73s/it][I 2024-05-07 08:04:34,995] Trial 132 finished with value: 1.3593546152114868 and parameters: {'hidden_size1': 85, 'hidden_size2': 47, 'learning_rate': 0.02352990644361559, 'dropout_prob': 0.16348708613151403, 'momentum': 0.4148545290535747}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 89%|████████▉ | 133/149 [15:09<01:49,  6.87s/it][I 2024-05-07 08:04:42,101] Trial 133 finished with value: 1.3073177337646484 and parameters: {'hidden_size1': 87, 'hidden_size2': 53, 'learning_rate': 0.029829840142335456, 'dropout_prob': 0.16431985362111606, 'momentum': 0.41702111634938677}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 90%|████████▉ | 134/149 [15:16<01:44,  6.94s/it][I 2024-05-07 08:04:52,231] Trial 134 finished with value: 1.7531737089157104 and parameters: {'hidden_size1': 83, 'hidden_size2': 54, 'learning_rate': 0.02854118721369773, 'dropout_prob': 0.16552461261526902, 'momentum': 0.4891435558985318}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 91%|█████████ | 135/149 [15:26<01:50,  7.90s/it][I 2024-05-07 08:05:00,661] Trial 135 finished with value: 1.2879204750061035 and parameters: {'hidden_size1': 90, 'hidden_size2': 56, 'learning_rate': 0.01213224293267564, 'dropout_prob': 0.12195895878923144, 'momentum': 0.4347317380321862}. Best is trial 108 with value: 1.2506225109100342.\n",
      " 91%|█████████▏| 136/149 [15:35<01:44,  8.06s/it][I 2024-05-07 08:05:12,428] Trial 136 finished with value: 1.2501933574676514 and parameters: {'hidden_size1': 76, 'hidden_size2': 60, 'learning_rate': 0.015104872918858016, 'dropout_prob': 0.12168634714588183, 'momentum': 0.4266541284509957}. Best is trial 136 with value: 1.2501933574676514.\n",
      " 92%|█████████▏| 137/149 [15:46<01:50,  9.17s/it][I 2024-05-07 08:05:23,682] Trial 137 finished with value: 1.7795219421386719 and parameters: {'hidden_size1': 78, 'hidden_size2': 59, 'learning_rate': 0.011069022837857932, 'dropout_prob': 0.11569416031910686, 'momentum': 0.4388280740745928}. Best is trial 136 with value: 1.2501933574676514.\n",
      " 93%|█████████▎| 138/149 [15:58<01:47,  9.80s/it][I 2024-05-07 08:05:33,727] Trial 138 finished with value: 1.4120008945465088 and parameters: {'hidden_size1': 76, 'hidden_size2': 63, 'learning_rate': 0.015401545545144033, 'dropout_prob': 0.08923233491245847, 'momentum': 0.4659067455748912}. Best is trial 136 with value: 1.2501933574676514.\n",
      " 93%|█████████▎| 139/149 [16:08<01:38,  9.87s/it][I 2024-05-07 08:05:42,366] Trial 139 finished with value: 1.5052103996276855 and parameters: {'hidden_size1': 87, 'hidden_size2': 55, 'learning_rate': 0.03462998733340735, 'dropout_prob': 0.12193713636295535, 'momentum': 0.42312467795030784}. Best is trial 136 with value: 1.2501933574676514.\n",
      " 94%|█████████▍| 140/149 [16:16<01:25,  9.50s/it][I 2024-05-07 08:05:56,551] Trial 140 finished with value: 1.393479585647583 and parameters: {'hidden_size1': 74, 'hidden_size2': 67, 'learning_rate': 0.020174200744584182, 'dropout_prob': 0.10267307757281283, 'momentum': 0.4298495238157956}. Best is trial 136 with value: 1.2501933574676514.\n",
      " 95%|█████████▍| 141/149 [16:31<01:27, 10.91s/it][I 2024-05-07 08:06:12,593] Trial 141 finished with value: 1.7873291969299316 and parameters: {'hidden_size1': 87, 'hidden_size2': 52, 'learning_rate': 0.01670565361532417, 'dropout_prob': 0.16197033320393858, 'momentum': 0.4207387625462255}. Best is trial 136 with value: 1.2501933574676514.\n",
      " 95%|█████████▌| 142/149 [16:47<01:27, 12.44s/it][I 2024-05-07 08:06:30,666] Trial 142 finished with value: 1.7514517307281494 and parameters: {'hidden_size1': 90, 'hidden_size2': 56, 'learning_rate': 0.028916139051698327, 'dropout_prob': 0.17623654879870543, 'momentum': 0.45650485268413044}. Best is trial 136 with value: 1.2501933574676514.\n",
      " 96%|█████████▌| 143/149 [17:05<01:24, 14.14s/it][I 2024-05-07 08:06:37,387] Trial 143 finished with value: 1.62743079662323 and parameters: {'hidden_size1': 83, 'hidden_size2': 60, 'learning_rate': 0.014524971588366539, 'dropout_prob': 0.1328603715521648, 'momentum': 0.4828867580053129}. Best is trial 136 with value: 1.2501933574676514.\n",
      " 97%|█████████▋| 144/149 [17:11<00:59, 11.91s/it][I 2024-05-07 08:06:45,209] Trial 144 finished with value: 1.4344589710235596 and parameters: {'hidden_size1': 97, 'hidden_size2': 48, 'learning_rate': 0.0196141532821766, 'dropout_prob': 0.2023646337876445, 'momentum': 0.39950058645401626}. Best is trial 136 with value: 1.2501933574676514.\n",
      " 97%|█████████▋| 145/149 [17:19<00:42, 10.68s/it][I 2024-05-07 08:06:52,999] Trial 145 finished with value: 1.5914809703826904 and parameters: {'hidden_size1': 80, 'hidden_size2': 64, 'learning_rate': 0.00867329458448734, 'dropout_prob': 0.14863430172608583, 'momentum': 0.3770520561369405}. Best is trial 136 with value: 1.2501933574676514.\n",
      " 98%|█████████▊| 146/149 [17:27<00:29,  9.82s/it][I 2024-05-07 08:07:00,647] Trial 146 finished with value: 1.4545117616653442 and parameters: {'hidden_size1': 72, 'hidden_size2': 49, 'learning_rate': 0.03948052148854077, 'dropout_prob': 0.1678860521005317, 'momentum': 0.5135455137554693}. Best is trial 136 with value: 1.2501933574676514.\n",
      " 99%|█████████▊| 147/149 [17:35<00:18,  9.16s/it][I 2024-05-07 08:07:06,699] Trial 147 finished with value: 1.4214308261871338 and parameters: {'hidden_size1': 60, 'hidden_size2': 56, 'learning_rate': 0.02492542165421609, 'dropout_prob': 0.12337375119358214, 'momentum': 0.36218846073138733}. Best is trial 136 with value: 1.2501933574676514.\n",
      " 99%|█████████▉| 148/149 [17:41<00:08,  8.23s/it][I 2024-05-07 08:07:13,780] Trial 148 finished with value: 1.5124928951263428 and parameters: {'hidden_size1': 90, 'hidden_size2': 43, 'learning_rate': 0.011887126107071596, 'dropout_prob': 0.18974913271176796, 'momentum': 0.4155353361432059}. Best is trial 136 with value: 1.2501933574676514.\n",
      "100%|██████████| 149/149 [17:48<00:00,  7.17s/it]\n"
     ]
    }
   ],
   "source": [
    "'''Procurando melhor arquitetura'''\n",
    "\n",
    "# Definindo parametro de entrada da rede\n",
    "input_size = 8\n",
    "epochs = 2000\n",
    "\n",
    "# Definindo o número de arquiteturas a serem testadas\n",
    "num_arch = 149\n",
    "\n",
    "# Função objetivo para otimização do Optuna\n",
    "def objective(trial):\n",
    "    # Definindo os hiperparâmetros a serem otimizados\n",
    "    hidden_size1 = trial.suggest_int('hidden_size1', 16, 256, log=True)\n",
    "    hidden_size2 = trial.suggest_int('hidden_size2', 16, 256, log=True)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    dropout_prob = trial.suggest_float('dropout_prob', 0.0, 0.5)\n",
    "    momentum = trial.suggest_float('momentum', 0.0, 1.0)\n",
    "\n",
    "    \n",
    "    # Criando uma instância do modelo com os hiperparâmetros sugeridos\n",
    "    model = NeuralNetworkRegressor(input_size, hidden_size1, hidden_size2, learning_rate, dropout_prob, momentum)\n",
    "    \n",
    "    # Treinando o modelo\n",
    "    model.train_model(X_train_scaled_tensor, Y_train_tensor_dimensionado, epochs)\n",
    "    \n",
    "    # Fazendo previsões no conjunto de validação\n",
    "    predictions = model.predict(X_test_scaled_tensor)\n",
    "\n",
    "    # Calcula o MSE no conjunto de validação\n",
    "    mse = mean_squared_error(Y_test_tensor_dimensionado, predictions)\n",
    "    \n",
    "    return mse\n",
    "\n",
    "\n",
    "# Executando a otimização do Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "# Cria uma  barra de progresso com um total igual ao número de arquiteturas a serem testadas\n",
    "with tqdm(total=num_arch) as pbar:\n",
    "    # Define da função de atualização da barra de progresso\n",
    "    def update_progress_bar(study, trial):\n",
    "        pbar.update(1)  # Atualiza a barra de progresso em uma unidade a cada chamada\n",
    "    \n",
    "    # Executa a otimização com Optuna\n",
    "    study.optimize(objective, n_trials=num_arch, callbacks=[update_progress_bar])\n",
    "\n",
    "# Obtendo os melhores hiperparâmetros encontrados\n",
    "best_params = study.best_params\n",
    "\n",
    "# Criando uma instância do modelo com os melhores hiperparâmetros\n",
    "best_model = NeuralNetworkRegressor(input_size, **best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabendo os melhores parâmetros de arquitetura de rede, foi treinado um modelo usando a melhor arquitetura encontrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Treinando a melhor rede'''\n",
    "\n",
    "# Treinando o modelo com todos os dados de treinamento\n",
    "best_model.train_model(X_train_scaled_tensor, Y_train_tensor_dimensionado, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando a rede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, temos a aplicação do modelo treinado e o print da predição da dureza dos 52 materiais do dataset de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "tensor([[5.0596],\n",
      "        [2.7855],\n",
      "        [2.7416],\n",
      "        [4.8133],\n",
      "        [5.2083],\n",
      "        [5.5771],\n",
      "        [4.6462],\n",
      "        [5.5043],\n",
      "        [4.8153],\n",
      "        [4.7382],\n",
      "        [5.3406],\n",
      "        [5.3156],\n",
      "        [5.1157],\n",
      "        [4.9016],\n",
      "        [5.4202],\n",
      "        [2.8745],\n",
      "        [2.6683],\n",
      "        [4.4810],\n",
      "        [3.9297],\n",
      "        [5.3295],\n",
      "        [4.7730],\n",
      "        [5.1192],\n",
      "        [3.4685],\n",
      "        [4.7694],\n",
      "        [2.6034],\n",
      "        [4.8759],\n",
      "        [5.1218],\n",
      "        [4.7779],\n",
      "        [5.2196],\n",
      "        [4.7002],\n",
      "        [4.3211],\n",
      "        [5.7566],\n",
      "        [5.2445],\n",
      "        [2.6683],\n",
      "        [4.8716],\n",
      "        [5.2478],\n",
      "        [5.3915],\n",
      "        [5.0292],\n",
      "        [5.3336],\n",
      "        [5.3943],\n",
      "        [4.6236],\n",
      "        [5.0145],\n",
      "        [5.5769],\n",
      "        [5.3625],\n",
      "        [4.4338],\n",
      "        [2.7568],\n",
      "        [3.7013],\n",
      "        [5.9876],\n",
      "        [4.5633],\n",
      "        [3.7628],\n",
      "        [3.8096],\n",
      "        [3.2655]])\n"
     ]
    }
   ],
   "source": [
    "'''Testando a rede'''\n",
    "\n",
    "# Fazendo previsões finais com o melhor modelo\n",
    "final_predictions = best_model.predict(X_test_scaled_tensor)\n",
    "print(\"Predictions:\")\n",
    "print(final_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo dos erros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, foi calculado os erros das pedições do modelo, foi escolhido mostrar os erros em 4 mensurações:\n",
    "\n",
    "- MSE -> É a média dos quadrados das diferenças entre os valores reais  e os valores previstos pelo modelo \n",
    "\n",
    "- MAE -> É a média dos valores absolutos das diferenças entre os valores reais e os valores previstos pelo modelo \n",
    "\n",
    "- RMSE -> É a raiz do MSE\n",
    "\n",
    "- R2 -> É o quadrado do coeficiente de correlação de Pearson entre os valores reais e os valores previstos pelo modelo \n",
    "\n",
    "Para melhor visualização, há o plot de um gráfico representando os resíduos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2.0079331398010254\n",
      "MAE: 1.1327770948410034\n",
      "RMSE: 1.417015552520752\n",
      "R²: 0.11786390820560844\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Calculando erros'''\n",
    "\n",
    "# Calculando as métricas\n",
    "mse = mean_squared_error(Y_test_tensor_dimensionado, final_predictions)\n",
    "mae = mean_absolute_error(Y_test_tensor_dimensionado, final_predictions)\n",
    "rmse = root_mean_squared_error(Y_test_tensor_dimensionado, final_predictions)\n",
    "r2 = r_squared(Y_test_tensor_dimensionado, final_predictions)\n",
    "\n",
    "# Exibindo os resultados\n",
    "print(f'MSE: {mse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'R²: {r2}')\n",
    "\n",
    "\n",
    "# Plotando o gráfico de resíduos\n",
    "plot_residuals(Y_test_tensor_dimensionado, final_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir dos resultados da dureza dos minerais preditos pela rede neural, é possível observar um erro muito baixo em todos as 4 métricas utilizadas, mesmo com as limitações de uma rede neural MLP com 2 camadas ocultas. Dessa forma, mostrando que a utilização de redes neurais MLP para predição de dureza de minerais é extremamente eficaz dado as propriedades fundamentais dos minerais. Portanto, pode-se afirmar que os resultados do trabalho foram extremamente satisfatórios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências\n",
    "## Datasets\n",
    "[1] Dumlao, J. (2024). Prediction of Mohs Hardness with Machine Learning. Kaggle. Retirado de https://www.kaggle.com/datasets/jocelyndumlao/prediction-of-mohs-hardness-with-machine-learning/data\n",
    "\n",
    "[2] Garnett, J. C. (2022). Dataset for Mohs Hardness Prediction. Mendeley Data. DOI: 10.17632/jm79zfps6b.1. retirado de https://data.mendeley.com/datasets/jm79zfps6b/1\n",
    "\n",
    "## Literatura \n",
    "[1] TABOR, David. Mohs's hardness scale-a physical interpretation. Proceedings of the Physical Society. Section B, v. 67, n. 3, p. 249, 1954.\n",
    "\n",
    "[2] Cassar, Daniel(2024). ATP-303 NN 1.1 - Introdução.pdf. Retirado do repositório da matéria Redes Neurais e Algoritimos Genéticos da ILUM - Escola de Ciência.\n",
    "\n",
    "[3] Cassar, Daniel(2024). ATP-303 NN 1.2 - Redes neurais.pdf. Retirado do repositório da matéria Redes Neurais e Algoritimos Genéticos da ILUM - Escola de Ciência.\n",
    "\n",
    "[4] Cassar, Daniel(2024). ATP-303 NN 1.3 - Regra da cadeia.pdf. Retirado do repositório da matéria Redes Neurais e Algoritimos Genéticos da ILUM - Escola de Ciência.\n",
    "\n",
    "[5] Cassar, Daniel(2024). ATP-303 NN 1.4 - Backpropagation.pdf. Retirado do repositório da matéria Redes Neurais e Algoritimos Genéticos da ILUM - Escola de Ciência.\n",
    "\n",
    "[6] Cassar, Daniel(2024). ATP-303 NN 3.2 - Notebook Autograd.ipynb. Retirado do repositório da matéria Redes Neurais e Algoritimos Genéticos da ILUM - Escola de Ciência.\n",
    "\n",
    "[7] Cassar, Daniel(2024). ATP-303 NN 4.2 - Notebook MLP.ipynb. Retirado do repositório da matéria Redes Neurais e Algoritimos Genéticos da ILUM - Escola de Ciência.\n",
    "\n",
    "[8] Cassar, Daniel(2024). ATP-303 NN 5.2 - Notebook PyTorch.ipynb. Retirado do repositório da matéria Redes Neurais e Algoritimos Genéticos da ILUM - Escola de Ciência.\n",
    "\n",
    "## Bibliotecas\n",
    "[1] PyTorch. (2024). PyTorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration. Retirado de https://pytorch.org/.\n",
    "\n",
    "[2] PyTorch. (2024). torch.nn - PyTorch v1.10.1 documentation. Retirado de https://pytorch.org/docs/1.10.1/nn.html.\n",
    "\n",
    "[3] PyTorch. (2024). torch.optim - PyTorch v1.10.1 documentation. Retirado de https://pytorch.org/docs/1.10.1/optim.html.\n",
    "\n",
    "[4] Pandas. (2010). pandas documentation. Retirado de https://pandas.pydata.org/docs/.\n",
    "\n",
    "[5] Scikit-learn. (2011). scikit-learn preprocessing module documentation. Retirado de https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing.\n",
    "\n",
    "[6] Scikit-learn. (2011). scikit-learn metrics module documentation. Retirado de https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.\n",
    "\n",
    "[7] Matplotlib. (2007). matplotlib.pyplot documentation. Retirado de https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html.\n",
    "\n",
    "[8] NumPy. (2020). NumPy v1.22.0 documentation. Retirado de https://numpy.org/doc/stable/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dark_Kernel",
   "language": "python",
   "name": "dark_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "25a19fbe0a9132dfb9279d48d161753c6352f8f9478c2e74383d340069b907c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
